<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Answering questions with data</title>
  <meta name="description" content="An introductory statistics textbook for psychology students">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Answering questions with data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introductory statistics textbook for psychology students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Answering questions with data" />
  
  <meta name="twitter:description" content="An introductory statistics textbook for psychology students" />
  

<meta name="author" content="Matthew J. C. Crump">
<meta name="author" content="Anjali Krishnan">
<meta name="author" content="Stephen Volz">
<meta name="author" content="Alla Chavarga">
<meta name="author" content="Jeffrey Suzuki">


<meta name="date" content="2018-07-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="inferenceForNumericalDataANOVA.html">
<link rel="next" href="multipleRegressionAndANOVA.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/javascript">
mattcrump=1;
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="tufte.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#important-notes"><i class="fa fa-check"></i><b>0.1</b> Important notes</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="why-statistics.html"><a href="why-statistics.html"><i class="fa fa-check"></i><b>1</b> Why Statistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="why-statistics.html"><a href="why-statistics.html#on-the-psychology-of-statistics"><i class="fa fa-check"></i><b>1.1</b> On the psychology of statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="why-statistics.html"><a href="why-statistics.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1.1</b> The curse of belief bias</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="why-statistics.html"><a href="why-statistics.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="why-statistics.html"><a href="why-statistics.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="why-statistics.html"><a href="why-statistics.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="why-statistics.html"><a href="why-statistics.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
<li class="chapter" data-level="1.6" data-path="why-statistics.html"><a href="why-statistics.html#a-brief-introduction-to-research-designchstudydesign"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research design[ch:studydesign]</a><ul>
<li class="chapter" data-level="1.6.1" data-path="why-statistics.html"><a href="why-statistics.html#introduction-to-psychological-measurementsecmeasurement"><i class="fa fa-check"></i><b>1.6.1</b> Introduction to psychological measurement [sec:measurement]</a></li>
<li class="chapter" data-level="1.6.2" data-path="why-statistics.html"><a href="why-statistics.html#scales-of-measurementsecscales"><i class="fa fa-check"></i><b>1.6.2</b> Scales of measurement[sec:scales]</a></li>
<li class="chapter" data-level="1.6.3" data-path="why-statistics.html"><a href="why-statistics.html#assessing-the-reliability-of-a-measurementsecreliability"><i class="fa fa-check"></i><b>1.6.3</b> Assessing the reliability of a measurement [sec:reliability]</a></li>
<li class="chapter" data-level="1.6.4" data-path="why-statistics.html"><a href="why-statistics.html#the-role-of-variables-predictors-and-outcomes-secivdv"><i class="fa fa-check"></i><b>1.6.4</b> The “role” of variables: predictors and outcomes [sec:ivdv]</a></li>
<li class="chapter" data-level="1.6.5" data-path="why-statistics.html"><a href="why-statistics.html#experimental-and-non-experimental-researchsecresearchdesigns"><i class="fa fa-check"></i><b>1.6.5</b> Experimental and non-experimental research [sec:researchdesigns]</a></li>
<li class="chapter" data-level="1.6.6" data-path="why-statistics.html"><a href="why-statistics.html#assessing-the-validity-of-a-studysecvalidity"><i class="fa fa-check"></i><b>1.6.6</b> Assessing the validity of a study [sec:validity]</a></li>
<li class="chapter" data-level="1.6.7" data-path="why-statistics.html"><a href="why-statistics.html#confounds-artifacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>1.6.7</b> Confounds, artifacts and other threats to validity</a></li>
<li class="chapter" data-level="1.6.8" data-path="why-statistics.html"><a href="why-statistics.html#summary"><i class="fa fa-check"></i><b>1.6.8</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DescribingData.html"><a href="DescribingData.html"><i class="fa fa-check"></i><b>2</b> Describing Data</a><ul>
<li class="chapter" data-level="2.1" data-path="DescribingData.html"><a href="DescribingData.html#this-is-what-too-many-numbers-looks-like"><i class="fa fa-check"></i><b>2.1</b> This is what too many numbers looks like</a></li>
<li class="chapter" data-level="2.2" data-path="DescribingData.html"><a href="DescribingData.html#look-at-the-data"><i class="fa fa-check"></i><b>2.2</b> Look at the data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DescribingData.html"><a href="DescribingData.html#stop-plotting-time-o-o-oh-u-can-plot-this"><i class="fa fa-check"></i><b>2.2.1</b> Stop, plotting time (o o oh) U can plot this</a></li>
<li class="chapter" data-level="2.2.2" data-path="DescribingData.html"><a href="DescribingData.html#histograms"><i class="fa fa-check"></i><b>2.2.2</b> Histograms</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DescribingData.html"><a href="DescribingData.html#important-ideas-distribution-central-tendency-and-variance"><i class="fa fa-check"></i><b>2.3</b> Important Ideas: Distribution, Central Tendency, and Variance</a></li>
<li class="chapter" data-level="2.4" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-central-tendency-sameness"><i class="fa fa-check"></i><b>2.4</b> Measures of Central Tendency (Sameness)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="DescribingData.html"><a href="DescribingData.html#from-many-numbers-to-one"><i class="fa fa-check"></i><b>2.4.1</b> From many numbers to one</a></li>
<li class="chapter" data-level="2.4.2" data-path="DescribingData.html"><a href="DescribingData.html#mode"><i class="fa fa-check"></i><b>2.4.2</b> Mode</a></li>
<li class="chapter" data-level="2.4.3" data-path="DescribingData.html"><a href="DescribingData.html#median"><i class="fa fa-check"></i><b>2.4.3</b> Median</a></li>
<li class="chapter" data-level="2.4.4" data-path="DescribingData.html"><a href="DescribingData.html#mean"><i class="fa fa-check"></i><b>2.4.4</b> Mean</a></li>
<li class="chapter" data-level="2.4.5" data-path="DescribingData.html"><a href="DescribingData.html#what-does-the-mean-mean"><i class="fa fa-check"></i><b>2.4.5</b> What does the mean mean?</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-variation-differentness"><i class="fa fa-check"></i><b>2.5</b> Measures of Variation (Differentness)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="DescribingData.html"><a href="DescribingData.html#the-range"><i class="fa fa-check"></i><b>2.5.1</b> The Range</a></li>
<li class="chapter" data-level="2.5.2" data-path="DescribingData.html"><a href="DescribingData.html#the-difference-scores"><i class="fa fa-check"></i><b>2.5.2</b> The Difference Scores</a></li>
<li class="chapter" data-level="2.5.3" data-path="DescribingData.html"><a href="DescribingData.html#the-variance"><i class="fa fa-check"></i><b>2.5.3</b> The Variance</a></li>
<li class="chapter" data-level="2.5.4" data-path="DescribingData.html"><a href="DescribingData.html#the-standard-deviation"><i class="fa fa-check"></i><b>2.5.4</b> The Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="DescribingData.html"><a href="DescribingData.html#using-descriptive-statistics-with-data"><i class="fa fa-check"></i><b>2.6</b> Using Descriptive Statistics with data</a></li>
<li class="chapter" data-level="2.7" data-path="DescribingData.html"><a href="DescribingData.html#rolling-your-own-descriptive-statistics"><i class="fa fa-check"></i><b>2.7</b> Rolling your own descriptive statistics</a><ul>
<li class="chapter" data-level="2.7.1" data-path="DescribingData.html"><a href="DescribingData.html#absolute-deviations"><i class="fa fa-check"></i><b>2.7.1</b> Absolute deviations</a></li>
<li class="chapter" data-level="2.7.2" data-path="DescribingData.html"><a href="DescribingData.html#other-sign-inverting-operations"><i class="fa fa-check"></i><b>2.7.2</b> Other sign-inverting operations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Correlation.html"><a href="Correlation.html"><i class="fa fa-check"></i><b>3</b> Correlation</a><ul>
<li class="chapter" data-level="3.1" data-path="Correlation.html"><a href="Correlation.html#if-something-caused-something-else-to-change-what-would-that-look-like"><i class="fa fa-check"></i><b>3.1</b> If something caused something else to change, what would that look like?</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Correlation.html"><a href="Correlation.html#charlie-and-the-chocolate-factory"><i class="fa fa-check"></i><b>3.1.1</b> Charlie and the Chocolate factory</a></li>
<li class="chapter" data-level="3.1.2" data-path="Correlation.html"><a href="Correlation.html#scatterplots"><i class="fa fa-check"></i><b>3.1.2</b> Scatterplots</a></li>
<li class="chapter" data-level="3.1.3" data-path="Correlation.html"><a href="Correlation.html#positive-negative-and-no-correlation"><i class="fa fa-check"></i><b>3.1.3</b> Positive, Negative, and No-Correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Correlation.html"><a href="Correlation.html#pearsons-r"><i class="fa fa-check"></i><b>3.2</b> Pearson’s r</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Correlation.html"><a href="Correlation.html#the-idea-of-co-variance"><i class="fa fa-check"></i><b>3.2.1</b> The idea of co-variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Correlation.html"><a href="Correlation.html#turning-the-numbers-into-a-measure-of-co-variance"><i class="fa fa-check"></i><b>3.3</b> Turning the numbers into a measure of co-variance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Correlation.html"><a href="Correlation.html#co-variance-the-measure"><i class="fa fa-check"></i><b>3.3.1</b> Co-variance, the measure</a></li>
<li class="chapter" data-level="3.3.2" data-path="Correlation.html"><a href="Correlation.html#pearsons-r-we-there-yet"><i class="fa fa-check"></i><b>3.3.2</b> Pearson’s r we there yet</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Correlation.html"><a href="Correlation.html#examples-with-data"><i class="fa fa-check"></i><b>3.4</b> Examples with Data</a></li>
<li class="chapter" data-level="3.5" data-path="Correlation.html"><a href="Correlation.html#interpreting-correlations"><i class="fa fa-check"></i><b>3.5</b> Interpreting Correlations</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Correlation.html"><a href="Correlation.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>3.5.1</b> Correlation does not equal causation</a></li>
<li class="chapter" data-level="3.5.2" data-path="Correlation.html"><a href="Correlation.html#correlation-and-random-chance"><i class="fa fa-check"></i><b>3.5.2</b> Correlation and Random chance</a></li>
<li class="chapter" data-level="3.5.3" data-path="Correlation.html"><a href="Correlation.html#some-more-movies"><i class="fa fa-check"></i><b>3.5.3</b> Some more movies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="FoundationForInference.html"><a href="FoundationForInference.html"><i class="fa fa-check"></i><b>4</b> Foundation for inference</a><ul>
<li class="chapter" data-level="4.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#brief-review-of-experiments"><i class="fa fa-check"></i><b>4.1</b> Brief review of Experiments</a></li>
<li class="chapter" data-level="4.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#the-data-came-from-a-distribution"><i class="fa fa-check"></i><b>4.2</b> The data came from a distribution</a><ul>
<li class="chapter" data-level="4.2.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#uniform-distribution"><i class="fa fa-check"></i><b>4.2.1</b> Uniform distribution</a></li>
<li class="chapter" data-level="4.2.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#not-all-samples-are-the-same-they-are-usually-quite-different"><i class="fa fa-check"></i><b>4.2.2</b> Not all samples are the same, they are usually quite different</a></li>
<li class="chapter" data-level="4.2.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#large-samples-are-more-like-the-distribution-they-came-from"><i class="fa fa-check"></i><b>4.2.3</b> Large samples are more like the distribution they came from</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#playing-with-distributions"><i class="fa fa-check"></i><b>4.3</b> Playing with distributions</a></li>
<li class="chapter" data-level="4.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#is-there-a-difference"><i class="fa fa-check"></i><b>4.4</b> Is there a difference?</a><ul>
<li class="chapter" data-level="4.4.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#chance-can-produce-differences"><i class="fa fa-check"></i><b>4.4.1</b> Chance can produce differences</a></li>
<li class="chapter" data-level="4.4.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#differences-due-to-chance-can-be-simulated"><i class="fa fa-check"></i><b>4.4.2</b> Differences due to chance can be simulated</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="FoundationForInference.html"><a href="FoundationForInference.html#chance-makes-some-differences-more-likely-than-others"><i class="fa fa-check"></i><b>4.5</b> Chance makes some differences more likely than others</a></li>
<li class="chapter" data-level="4.6" data-path="FoundationForInference.html"><a href="FoundationForInference.html#sampling-distribution-of-the-sample-means"><i class="fa fa-check"></i><b>4.6</b> Sampling distribution of the sample means</a></li>
<li class="chapter" data-level="4.7" data-path="FoundationForInference.html"><a href="FoundationForInference.html#seeing-the-pieces"><i class="fa fa-check"></i><b>4.7</b> Seeing the pieces</a></li>
<li class="chapter" data-level="4.8" data-path="FoundationForInference.html"><a href="FoundationForInference.html#caseStudyGenderDiscrimination"><i class="fa fa-check"></i><b>4.8</b> Randomization case study: gender discrimination</a><ul>
<li class="chapter" data-level="4.8.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#variabilityWithinData"><i class="fa fa-check"></i><b>4.8.1</b> Variability within data</a></li>
<li class="chapter" data-level="4.8.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#simulatingTheStudy"><i class="fa fa-check"></i><b>4.8.2</b> Simulating the study</a></li>
<li class="chapter" data-level="4.8.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#checking-for-independence"><i class="fa fa-check"></i><b>4.8.3</b> Checking for independence</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="FoundationForInference.html"><a href="FoundationForInference.html#caseStudyOpportunityCost"><i class="fa fa-check"></i><b>4.9</b> Randomization case study: opportunity cost</a><ul>
<li class="chapter" data-level="4.9.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#exploring-the-data-set-before-the-analysis"><i class="fa fa-check"></i><b>4.9.1</b> Exploring the data set before the analysis</a></li>
<li class="chapter" data-level="4.9.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#results-from-chance-alone"><i class="fa fa-check"></i><b>4.9.2</b> Results from chance alone</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="FoundationForInference.html"><a href="FoundationForInference.html#HypothesisTesting"><i class="fa fa-check"></i><b>4.10</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="4.10.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#hypothesis-testing-in-the-us-court-system"><i class="fa fa-check"></i><b>4.10.1</b> Hypothesis testing in the US court system</a></li>
<li class="chapter" data-level="4.10.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#p-value-and-statistical-significance"><i class="fa fa-check"></i><b>4.10.2</b> p-value and statistical significance</a></li>
<li class="chapter" data-level="4.10.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#decision-errors"><i class="fa fa-check"></i><b>4.10.3</b> Decision errors</a></li>
<li class="chapter" data-level="4.10.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#significanceLevel"><i class="fa fa-check"></i><b>4.10.4</b> Choosing a significance level</a></li>
<li class="chapter" data-level="4.10.5" data-path="FoundationForInference.html"><a href="FoundationForInference.html#IntroducingTwoSidedHypotheses"><i class="fa fa-check"></i><b>4.10.5</b> Introducing two-sided hypotheses</a></li>
<li class="chapter" data-level="4.10.6" data-path="FoundationForInference.html"><a href="FoundationForInference.html#InflatingType1ErrorRate"><i class="fa fa-check"></i><b>4.10.6</b> Controlling the Type 1 Error rate</a></li>
<li class="chapter" data-level="4.10.7" data-path="FoundationForInference.html"><a href="FoundationForInference.html#how-to-use-a-hypothesis-test"><i class="fa fa-check"></i><b>4.10.7</b> How to use a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="4.11" data-path="FoundationForInference.html"><a href="FoundationForInference.html#SimulationCaseStudies"><i class="fa fa-check"></i><b>4.11</b> Simulation case studies</a><ul>
<li class="chapter" data-level="4.11.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#medical-consultant"><i class="fa fa-check"></i><b>4.11.1</b> Medical consultant</a></li>
<li class="chapter" data-level="4.11.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#tappers-and-listeners"><i class="fa fa-check"></i><b>4.11.2</b> Tappers and listeners</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="FoundationForInference.html"><a href="FoundationForInference.html#CLTsection"><i class="fa fa-check"></i><b>4.12</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="4.12.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#null-distribution-from-the-case-studies"><i class="fa fa-check"></i><b>4.12.1</b> Null distribution from the case studies</a></li>
<li class="chapter" data-level="4.12.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#examples-of-future-settings-we-will-consider"><i class="fa fa-check"></i><b>4.12.2</b> Examples of future settings we will consider</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normalDist"><i class="fa fa-check"></i><b>4.13</b> Normal distribution</a><ul>
<li class="chapter" data-level="4.13.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#NormalDistributionModelSubsection"><i class="fa fa-check"></i><b>4.13.1</b> Normal distribution model</a></li>
<li class="chapter" data-level="4.13.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#standardizing-with-z-scores"><i class="fa fa-check"></i><b>4.13.2</b> Standardizing with Z scores</a></li>
<li class="chapter" data-level="4.13.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-probability-table"><i class="fa fa-check"></i><b>4.13.3</b> Normal probability table</a></li>
<li class="chapter" data-level="4.13.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-probability-examples"><i class="fa fa-check"></i><b>4.13.4</b> Normal probability examples</a></li>
<li class="chapter" data-level="4.13.5" data-path="FoundationForInference.html"><a href="FoundationForInference.html#rule"><i class="fa fa-check"></i><b>4.13.5</b> 68-95-99.7 rule</a></li>
<li class="chapter" data-level="4.13.6" data-path="FoundationForInference.html"><a href="FoundationForInference.html#assessingNormal"><i class="fa fa-check"></i><b>4.13.6</b> Evaluating the normal approximation</a></li>
</ul></li>
<li class="chapter" data-level="4.14" data-path="FoundationForInference.html"><a href="FoundationForInference.html#ApplyingTheNormalModel"><i class="fa fa-check"></i><b>4.14</b> Applying the normal model</a><ul>
<li class="chapter" data-level="4.14.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#standard-error"><i class="fa fa-check"></i><b>4.14.1</b> Standard error</a></li>
<li class="chapter" data-level="4.14.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-model-application-opportunity-cost"><i class="fa fa-check"></i><b>4.14.2</b> Normal model application: opportunity cost</a></li>
<li class="chapter" data-level="4.14.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-model-application-medical-consultant"><i class="fa fa-check"></i><b>4.14.3</b> Normal model application: medical consultant</a></li>
<li class="chapter" data-level="4.14.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#conditions-for-applying-the-normal-model"><i class="fa fa-check"></i><b>4.14.4</b> Conditions for applying the normal model</a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="FoundationForInference.html"><a href="FoundationForInference.html#ConfidenceIntervals"><i class="fa fa-check"></i><b>4.15</b> Confidence intervals</a><ul>
<li class="chapter" data-level="4.15.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#capturing-the-population-parameter"><i class="fa fa-check"></i><b>4.15.1</b> Capturing the population parameter</a></li>
<li class="chapter" data-level="4.15.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#constructing-a-95-confidence-interval"><i class="fa fa-check"></i><b>4.15.2</b> Constructing a 95% confidence interval</a></li>
<li class="chapter" data-level="4.15.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#changingTheConfidenceLevelSection"><i class="fa fa-check"></i><b>4.15.3</b> Changing the confidence level</a></li>
<li class="chapter" data-level="4.15.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#interpretingCIs"><i class="fa fa-check"></i><b>4.15.4</b> Interpreting confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html"><i class="fa fa-check"></i><b>5</b> Inference for numerical data: t-Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#oneSampleMeansWithTDistribution"><i class="fa fa-check"></i><b>5.1</b> One-sample means with the <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="5.1.1" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#two-examples-using-the-normal-distribution"><i class="fa fa-check"></i><b>5.1.1</b> Two examples using the normal distribution</a></li>
<li class="chapter" data-level="5.1.2" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#introducingTheTDistribution"><i class="fa fa-check"></i><b>5.1.2</b> Introducing the <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#tDistSolutionToSEProblem"><i class="fa fa-check"></i><b>5.1.3</b> Applying the <span class="math inline">\(t\)</span> distribution to the single-mean situation</a></li>
<li class="chapter" data-level="5.1.4" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#oneSampleTConfidenceIntervals"><i class="fa fa-check"></i><b>5.1.4</b> One sample <span class="math inline">\(t\)</span> confidence intervals</a></li>
<li class="chapter" data-level="5.1.5" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#oneSampleTTests"><i class="fa fa-check"></i><b>5.1.5</b> One sample <span class="math inline">\(t\)</span> tests</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#pairedData"><i class="fa fa-check"></i><b>5.2</b> Paired data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#paired-observations"><i class="fa fa-check"></i><b>5.2.1</b> Paired observations</a></li>
<li class="chapter" data-level="5.2.2" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#inference-for-paired-data"><i class="fa fa-check"></i><b>5.2.2</b> Inference for paired data</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#differenceOfTwoMeans"><i class="fa fa-check"></i><b>5.3</b> Difference of two means</a><ul>
<li class="chapter" data-level="5.3.1" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#confidence-interval-for-a-differences-of-means"><i class="fa fa-check"></i><b>5.3.1</b> Confidence interval for a differences of means</a></li>
<li class="chapter" data-level="5.3.2" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#hypothesis-tests-based-on-a-difference-in-means"><i class="fa fa-check"></i><b>5.3.2</b> Hypothesis tests based on a difference in means</a></li>
<li class="chapter" data-level="5.3.3" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#case-study-two-versions-of-a-course-exam"><i class="fa fa-check"></i><b>5.3.3</b> Case study: two versions of a course exam</a></li>
<li class="chapter" data-level="5.3.4" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#summary-for-inference-using-the-t-distribution"><i class="fa fa-check"></i><b>5.3.4</b> Summary for inference using the <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="5.3.5" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#pooledStandardDeviations"><i class="fa fa-check"></i><b>5.3.5</b> Pooled standard deviation estimate (special topic)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html"><i class="fa fa-check"></i><b>6</b> Inference for numerical data: ANOVA</a><ul>
<li class="chapter" data-level="6.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#anovaAndRegrWithCategoricalVariables"><i class="fa fa-check"></i><b>6.1</b> Comparing many means with ANOVA</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#is-batting-performance-related-to-player-position-in-mlb"><i class="fa fa-check"></i><b>6.1.1</b> Is batting performance related to player position in MLB?</a></li>
<li class="chapter" data-level="6.1.2" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#analysis-of-variance-anova-and-the-f-test"><i class="fa fa-check"></i><b>6.1.2</b> Analysis of variance (ANOVA) and the F test</a></li>
<li class="chapter" data-level="6.1.3" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#reading-an-anova-table-from-software"><i class="fa fa-check"></i><b>6.1.3</b> Reading an ANOVA table from software</a></li>
<li class="chapter" data-level="6.1.4" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#graphical-diagnostics-for-an-anova-analysis"><i class="fa fa-check"></i><b>6.1.4</b> Graphical diagnostics for an ANOVA analysis</a></li>
<li class="chapter" data-level="6.1.5" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#multipleComparisonsAndControllingTheType1ErrorRate"><i class="fa fa-check"></i><b>6.1.5</b> Multiple comparisons and controlling Type 1 Error rate</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#bootstrapping-to-study-the-standard-deviation"><i class="fa fa-check"></i><b>6.2</b> Bootstrapping to study the standard deviation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#bootstrap-samples-and-distributions"><i class="fa fa-check"></i><b>6.2.1</b> Bootstrap samples and distributions</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#inference-using-the-bootstrap"><i class="fa fa-check"></i><b>6.2.2</b> Inference using the bootstrap</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#frequently-asked-questions"><i class="fa fa-check"></i><b>6.2.3</b> Frequently asked questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html"><i class="fa fa-check"></i><b>7</b> Introduction to linear regression</a><ul>
<li class="chapter" data-level="7.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#lineFittingResidualsCorrelation"><i class="fa fa-check"></i><b>7.1</b> Line fitting, residuals, and correlation</a><ul>
<li class="chapter" data-level="7.1.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#beginning-with-straight-lines"><i class="fa fa-check"></i><b>7.1.1</b> Beginning with straight lines</a></li>
<li class="chapter" data-level="7.1.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#fitting-a-line-by-eye"><i class="fa fa-check"></i><b>7.1.2</b> Fitting a line by eye</a></li>
<li class="chapter" data-level="7.1.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#residuals"><i class="fa fa-check"></i><b>7.1.3</b> Residuals</a></li>
<li class="chapter" data-level="7.1.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#describing-linear-relationships-with-correlation"><i class="fa fa-check"></i><b>7.1.4</b> Describing linear relationships with correlation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#fittingALineByLSR"><i class="fa fa-check"></i><b>7.2</b> Fitting a line by least squares regression</a><ul>
<li class="chapter" data-level="7.2.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#an-objective-measure-for-finding-the-best-line"><i class="fa fa-check"></i><b>7.2.1</b> An objective measure for finding the best line</a></li>
<li class="chapter" data-level="7.2.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#findingTheLeastSquaresLineSection"><i class="fa fa-check"></i><b>7.2.2</b> Finding the least squares line</a></li>
<li class="chapter" data-level="7.2.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#interpreting-regression-line-parameter-estimates"><i class="fa fa-check"></i><b>7.2.3</b> Interpreting regression line parameter estimates</a></li>
<li class="chapter" data-level="7.2.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#extrapolation-is-treacherous"><i class="fa fa-check"></i><b>7.2.4</b> Extrapolation is treacherous</a></li>
<li class="chapter" data-level="7.2.5" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#using-r2-to-describe-the-strength-of-a-fit"><i class="fa fa-check"></i><b>7.2.5</b> Using <span class="math inline">\(R^2\)</span> to describe the strength of a fit</a></li>
<li class="chapter" data-level="7.2.6" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#categoricalPredictorsWithTwoLevels"><i class="fa fa-check"></i><b>7.2.6</b> Categorical predictors with two levels</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#typesOfOutliersInLinearRegression"><i class="fa fa-check"></i><b>7.3</b> Types of outliers in linear regression</a></li>
<li class="chapter" data-level="7.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#inferenceForLinearRegression"><i class="fa fa-check"></i><b>7.4</b> Inference for linear regression</a><ul>
<li class="chapter" data-level="7.4.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#conditions-for-the-least-squares-line"><i class="fa fa-check"></i><b>7.4.1</b> Conditions for the least squares line</a></li>
<li class="chapter" data-level="7.4.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#midterm-elections-and-unemployment"><i class="fa fa-check"></i><b>7.4.2</b> Midterm elections and unemployment</a></li>
<li class="chapter" data-level="7.4.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#testStatisticForTheSlope"><i class="fa fa-check"></i><b>7.4.3</b> Understanding regression output from software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html"><i class="fa fa-check"></i><b>8</b> Multiple and logistic regression</a><ul>
<li class="chapter" data-level="8.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#introductionToMultipleRegression"><i class="fa fa-check"></i><b>8.1</b> Introduction to multiple regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#twoSingleVariableModelsForMarioKartData"><i class="fa fa-check"></i><b>8.1.1</b> A single-variable model for the Mario Kart data</a></li>
<li class="chapter" data-level="8.1.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#includingAndAssessingManyVariablesInAModel"><i class="fa fa-check"></i><b>8.1.2</b> Including and assessing many variables in a model</a></li>
<li class="chapter" data-level="8.1.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#adjusted-r2-as-a-better-estimate-of-explained-variance"><i class="fa fa-check"></i><b>8.1.3</b> Adjusted <span class="math inline">\(R^2\)</span> as a better estimate of explained variance</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelSelection"><i class="fa fa-check"></i><b>8.2</b> Model selection</a><ul>
<li class="chapter" data-level="8.2.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#identifying-variables-in-the-model-that-may-not-be-helpful"><i class="fa fa-check"></i><b>8.2.1</b> Identifying variables in the model that may not be helpful</a></li>
<li class="chapter" data-level="8.2.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#two-model-selection-strategies"><i class="fa fa-check"></i><b>8.2.2</b> Two model selection strategies</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#multipleRegressionModelAssumptions"><i class="fa fa-check"></i><b>8.3</b> Checking model assumptions using graphs</a></li>
<li class="chapter" data-level="8.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#logisticRegression"><i class="fa fa-check"></i><b>8.4</b> Logistic regression</a><ul>
<li class="chapter" data-level="8.4.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#email-data"><i class="fa fa-check"></i><b>8.4.1</b> Email data</a></li>
<li class="chapter" data-level="8.4.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelingTheProbabilityOfAnEvent"><i class="fa fa-check"></i><b>8.4.2</b> Modeling the probability of an event</a></li>
<li class="chapter" data-level="8.4.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#practical-decisions-in-the-email-application"><i class="fa fa-check"></i><b>8.4.3</b> Practical decisions in the email application</a></li>
<li class="chapter" data-level="8.4.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#diagnostics-for-the-email-classifier"><i class="fa fa-check"></i><b>8.4.4</b> Diagnostics for the email classifier</a></li>
<li class="chapter" data-level="8.4.5" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#improvingTheSetOfVariablesForASpamFilter"><i class="fa fa-check"></i><b>8.4.5</b> Improving the set of variables for a spam filter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html"><i class="fa fa-check"></i><b>9</b> Inference for categorical data</a><ul>
<li class="chapter" data-level="9.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#singleProportion"><i class="fa fa-check"></i><b>9.1</b> Inference for a single proportion</a><ul>
<li class="chapter" data-level="9.1.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#when-the-sample-proportion-is-nearly-normal"><i class="fa fa-check"></i><b>9.1.1</b> When the sample proportion is nearly normal</a></li>
<li class="chapter" data-level="9.1.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#confIntForPropSection"><i class="fa fa-check"></i><b>9.1.2</b> Confidence intervals for a proportion</a></li>
<li class="chapter" data-level="9.1.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#htForPropSection"><i class="fa fa-check"></i><b>9.1.3</b> Hypothesis testing for a proportion</a></li>
<li class="chapter" data-level="9.1.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#choosing-a-sample-size-when-estimating-a-proportion"><i class="fa fa-check"></i><b>9.1.4</b> Choosing a sample size when estimating a proportion</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#differenceOfTwoProportions"><i class="fa fa-check"></i><b>9.2</b> Difference of two proportions</a><ul>
<li class="chapter" data-level="9.2.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#SampleDistributionOfTheDiffOfTwoProportions"><i class="fa fa-check"></i><b>9.2.1</b> Sample distribution of the difference of two proportions</a></li>
<li class="chapter" data-level="9.2.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#intervals-and-tests-for-p_1--p_2"><i class="fa fa-check"></i><b>9.2.2</b> Intervals and tests for <span class="math inline">\(p_1 -p_2\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pooledHTForProportionsSection"><i class="fa fa-check"></i><b>9.2.3</b> Hypothesis testing when <span class="math inline">\(H_0: p_1=p_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#oneWayChiSquare"><i class="fa fa-check"></i><b>9.3</b> Testing for goodness of fit using chi-square (special topic)</a><ul>
<li class="chapter" data-level="9.3.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#creating-a-test-statistic-for-one-way-tables"><i class="fa fa-check"></i><b>9.3.1</b> Creating a test statistic for one-way tables</a></li>
<li class="chapter" data-level="9.3.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#chiSquareTestStatistic"><i class="fa fa-check"></i><b>9.3.2</b> The chi-square test statistic</a></li>
<li class="chapter" data-level="9.3.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-distribution-and-finding-areas"><i class="fa fa-check"></i><b>9.3.3</b> The chi-square distribution and finding areas</a></li>
<li class="chapter" data-level="9.3.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pValueForAChiSquareTest"><i class="fa fa-check"></i><b>9.3.4</b> Finding a p-value for a chi-square distribution</a></li>
<li class="chapter" data-level="9.3.5" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#evaluating-goodness-of-fit-for-a-distribution"><i class="fa fa-check"></i><b>9.3.5</b> Evaluating goodness of fit for a distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#twoWayTablesAndChiSquare"><i class="fa fa-check"></i><b>9.4</b> Testing for independence in two-way tables (special topic)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#expected-counts-in-two-way-tables"><i class="fa fa-check"></i><b>9.4.1</b> Expected counts in two-way tables</a></li>
<li class="chapter" data-level="9.4.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-test-for-two-way-tables"><i class="fa fa-check"></i><b>9.4.2</b> The chi-square test for two-way tables</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Answering questions with data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linRegrForTwoVar" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Introduction to linear regression</h1>
<p>Linear regression is a very powerful statistical technique. Many people have some familiarity with regression just from reading the news, where graphs with straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables.</p>
<p>Figure [perfLinearModel] shows two variables whose relationship can be modeled perfectly with a straight line. The equation for the line is</p>
<p><span class="math display">\[\begin{aligned}
y = 5 + 57.49x\end{aligned}\]</span></p>
<p>Imagine what a perfect linear relationship would mean: you would know the exact value of <span class="math inline">\(y\)</span> just by knowing the value of <span class="math inline">\(x\)</span>. This is unrealistic in almost any natural process. For example, if we took family income <span class="math inline">\(x\)</span>, this value would provide some useful information about how much financial support <span class="math inline">\(y\)</span> a college may offer a prospective student. However, there would still be variability in financial support, even when comparing students whose families have similar financial backgrounds.</p>
<div class="figure">
<img src="05/figures/perfLinearModel/perfLinearModel" alt="Requests from twelve separate buyers were simultaneously placed with a trading company to purchase Target Corporation stock (ticker TGT, April 26th, 2012), and the total cost of the shares were reported. Because the cost is computed using a linear formula, the linear fit is perfect." />
<p class="caption">Requests from twelve separate buyers were simultaneously placed with a trading company to purchase Target Corporation stock (ticker <code>TGT</code>, April 26th, 2012), and the total cost of the shares were reported. Because the cost is computed using a linear formula, the linear fit is perfect.</p>
</div>
<p>[perfLinearModel]</p>
<p>Linear regression assumes that the relationship between two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, can be modeled by a straight line:</p>
<p><span class="math display">\[\begin{aligned}
y = \beta_0 + \beta_1x
\label{genLinModelWNoErrorTerm}\end{aligned}\]</span></p>
<p>[</p>
<p><span class="math inline">\(\beta_0, \beta_1\)</span></p>
<p><br />
Linear model<br />
parameters]</p>
<p><span class="math inline">\(\beta_0, \beta_1\)</span></p>
<p><br />
Linear model<br />
parameters</p>
<p>where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> represent two model parameters (<span class="math inline">\(\beta\)</span> is the Greek letter <em>beta</em>). These parameters are estimated using data, and we write their point estimates as <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. When we use <span class="math inline">\(x\)</span> to predict <span class="math inline">\(y\)</span>, we usually call <span class="math inline">\(x\)</span> the explanatory or <strong>predictor</strong> variable, and we call <span class="math inline">\(y\)</span> the response.</p>
<p>It is rare for all of the data to fall on a straight line, as seen in the three scatterplots in Figure [imperfLinearModel]. In each case, the data fall around a straight line, even if none of the observations fall exactly on the line. The first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The second plot shows an upward trend that, while evident, is not as strong as the first. The last plot shows a very weak downward trend in the data, so slight we can hardly notice it. In each of these examples, we will have some uncertainty regarding our estimates of the model parameters, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. For instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less? As we move forward in this chapter, we will learn different criteria for line-fitting, and we will also learn about the uncertainty associated with estimates of model parameters.</p>
<div class="figure">
<img src="05/figures/imperfLinearModel/imperfLinearModel" alt="Three data sets where a linear model may be useful even though the data do not all fall exactly on the line." />
<p class="caption">Three data sets where a linear model may be useful even though the data do not all fall exactly on the line.</p>
</div>
<p>[imperfLinearModel]</p>
<p>We will also see examples in this chapter where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful. One such case is shown in Figure [notGoodAtAllForALinearModel] where there is a very strong relationship between the variables even though the trend is not linear. We will discuss nonlinear trends in this chapter, but the details of fitting nonlinear models are saved for a later course.</p>
<div class="figure">
<img src="05/figures/notGoodAtAllForALinearModel/notGoodAtAllForALinearModel" alt="A linear model is not useful in this nonlinear case. These data are from an introductory physics experiment." />
<p class="caption">A linear model is not useful in this nonlinear case. These data are from an introductory physics experiment.</p>
</div>
<p>[notGoodAtAllForALinearModel]</p>
<div id="lineFittingResidualsCorrelation" class="section level2">
<h2><span class="header-section-number">7.1</span> Line fitting, residuals, and correlation</h2>
<p>It is helpful to think deeply about the line fitting process. In this section, we examine criteria for identifying a linear model and introduce a new statistic, <em>correlation</em>.</p>
<div id="beginning-with-straight-lines" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Beginning with straight lines</h3>
<p>Scatterplots were introduced in Chapter [introductionToData] as a graphical technique to present two numerical variables simultaneously. Such plots permit the relationship between the variables to be examined with ease. Figure [scattHeadLTotalL] shows a scatterplot for the head length and total length of 104 brushtail possums from Australia. Each point represents a single possum from the data.</p>
<div class="figure">
<img src="05/figures/scattHeadLTotalL/scattHeadLTotalL" alt="A scatterplot showing head length against total length for 104 brushtail possums. A point representing a possum with head length 94.1mm and total length 89cm is highlighted." />
<p class="caption">A scatterplot showing head length against total length for 104 brushtail possums. A point representing a possum with head length 94.1mm and total length 89cm is highlighted.</p>
</div>
<p>[scattHeadLTotalL]</p>
<div class="figure">
<img src="05/figures/possumPic/possumPic" alt="The common brushtail possum of Australia. Photo by wollombi on Flickr" />
<p class="caption">The common brushtail possum of Australia. Photo by wollombi on Flickr</p>
</div>
<p>[possumPic]</p>
<p>The head and total length variables are associated. Possums with an above average total length also tend to have above average head lengths. While the relationship is not perfectly linear, it could be helpful to partially explain the connection between these variables with a straight line.</p>
<div class="figure">
<img src="05/figures/scattHeadLTotalLTube/scattHeadLTotalLTube" alt="The figure on the left shows head length versus total length, and reveals that many of the points could be captured by a straight band. On the right, we see that a curved band is more appropriate in the scatterplot for weight and mpgCity from the cars data set." />
<p class="caption">The figure on the left shows head length versus total length, and reveals that many of the points could be captured by a straight band. On the right, we see that a curved band is more appropriate in the scatterplot for <strong>weight</strong> and <strong>mpgCity</strong> from the <strong>cars</strong> data set.</p>
</div>
<p>[scattHeadLTotalLTube]</p>
<p>Straight lines should only be used when the data appear to have a linear relationship, such as the case shown in the left panel of Figure [scattHeadLTotalLTube]. The right panel of Figure [scattHeadLTotalLTube] shows a case where a curved line would be more useful in understanding the relationship between the two variables.</p>
<p><span>Watch out for curved trends</span></p>
<p>We only consider models based on straight lines in this chapter. If data show a nonlinear trend, like that in the right panel of Figure [scattHeadLTotalLTube], more advanced techniques should be used.</p>
</div>
<div id="fitting-a-line-by-eye" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Fitting a line by eye</h3>
<p>We want to describe the relationship between the head length and total length variables in the possum data set using a line. In this example, we will use the total length as the predictor variable, <span class="math inline">\(x\)</span>, to predict a possum’s head length, <span class="math inline">\(y\)</span>. We could fit the linear relationship by eye, as in Figure [scattHeadLTotalLLine]. The equation for this line is</p>
<p><span class="math display">\[\begin{aligned}
\hat{y} = 41 + 0.59x
\label{headLLinModTotalL}\end{aligned}\]</span></p>
<p>We can use this line to discuss properties of possums. For instance, the equation predicts a possum with a total length of 80 cm will have a head length of</p>
<p><span class="math display">\[\begin{aligned}
\hat{y} &amp;= 41 + 0.59\times 80 \\
    &amp;= 88.2 % mm\end{aligned}\]</span></p>
<p>A “hat” on <span class="math inline">\(y\)</span> is used to signify that this is an estimate. This estimate may be viewed as an average: the equation predicts that possums with a total length of 80 cm will have an average head length of 88.2 mm. Absent further information about an 80 cm possum, the prediction for head length that uses the average is a reasonable estimate.</p>
<div class="figure">
<img src="05/figures/scattHeadLTotalLLine/scattHeadLTotalLLine" alt="A reasonable linear model was fit to represent the relationship between head length and total length." />
<p class="caption">A reasonable linear model was fit to represent the relationship between head length and total length.</p>
</div>
<p>[scattHeadLTotalLLine]</p>
</div>
<div id="residuals" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Residuals</h3>
<p>are the leftover variation in the data after accounting for the model fit:</p>
<p><span class="math display">\[\begin{aligned}
\text{Data} = \text{Fit} + \text{Residual}\end{aligned}\]</span></p>
<p>Each observation will have a residual. If an observation is above the regression line, then its residual, the vertical distance from the observation to the line, is positive. Observations below the line have negative residuals. One goal in picking the right linear model is for these residuals to be as small as possible.</p>
<p>Three observations are noted specially in Figure [scattHeadLTotalLLine]. The observation marked by an “<span class="math inline">\(\times\)</span>” has a small, negative residual of about -1; the observation marked by “<span class="math inline">\(+\)</span>” has a large residual of about +7; and the observation marked by “<span class="math inline">\(\triangle\)</span>” has a moderate residual of about -4. The size of a residual is usually discussed in terms of its absolute value. For example, the residual for “<span class="math inline">\(\triangle\)</span>” is larger than that of “<span class="math inline">\(\times\)</span>” because <span class="math inline">\(|-4|\)</span> is larger than <span class="math inline">\(|-1|\)</span>.</p>
<p>The residual of the <span class="math inline">\(i^{th}\)</span> observation <span class="math inline">\((x_i, y_i)\)</span> is the difference of the observed response (<span class="math inline">\(y_i\)</span>) and the response we would predict based on the model fit (<span class="math inline">\(\hat{y}_i\)</span>):</p>
<p><span class="math display">\[\begin{aligned}
e_i = y_i - \hat{y}_i\end{aligned}\]</span></p>
<p>We typically identify <span class="math inline">\(\hat{y}_i\)</span> by plugging <span class="math inline">\(x_i\)</span> into the model.</p>
<p><span>The linear fit shown in Figure [scattHeadLTotalLLine] is given as <span class="math inline">\(\hat{y} = 41 + 0.59x\)</span>. Based on this line, formally compute the residual of the observation <span class="math inline">\((77.0, 85.3)\)</span>. This observation is denoted by “<span class="math inline">\(\times\)</span>” on the plot. Check it against the earlier visual estimate, -1.</span> We first compute the predicted value of point “<span class="math inline">\(\times\)</span>” based on the model:</p>
<p><span class="math display">\[\begin{aligned}
\hat{y}_{\times} = 41+0.59x_{\times} = 41+0.59\times 77.0 = 86.4\end{aligned}\]</span></p>
<p>Next we compute the difference of the actual head length and the predicted head length:</p>
<p><span class="math display">\[\begin{aligned}
e_{\times} = y_{\times} - \hat{y}_{\times} = 85.3 -  86.4 = -1.1\end{aligned}\]</span></p>
<p>This is very close to the visual estimate of -1.</p>
<p>If a model underestimates an observation, will the residual be positive or negative? What about if it overestimates the observation?<a href="#fn80" class="footnoteRef" id="fnref80"><sup>80</sup></a></p>
<p>Compute the residuals for the observations <span class="math inline">\((85.0, 98.6)\)</span> (“<span class="math inline">\(+\)</span>” in the figure) and <span class="math inline">\((95.5, 94.0)\)</span> (“<span class="math inline">\(\triangle\)</span>”) using the linear relationship <span class="math inline">\(\hat{y} = 41 + 0.59x\)</span>.<a href="#fn81" class="footnoteRef" id="fnref81"><sup>81</sup></a></p>
<p>Residuals are helpful in evaluating how well a linear model fits a data set. We often display them in a <strong>residual plot</strong> such as the one shown in Figure [scattHeadLTotalLResidualPlot] for the regression line in Figure [scattHeadLTotalLLine]. The residuals are plotted at their original horizontal locations but with the vertical coordinate as the residual. For instance, the point <span class="math inline">\((85.0,98.6)_{+}\)</span> had a residual of 7.45, so in the residual plot it is placed at <span class="math inline">\((85.0, 7.45)\)</span>. Creating a residual plot is sort of like tipping the scatterplot over so the regression line is horizontal.</p>
<div class="figure">
<img src="05/figures/scattHeadLTotalLResidualPlot/scattHeadLTotalLResidualPlot" alt="Residual plot for the model in Figure [scattHeadLTotalLLine]." />
<p class="caption">Residual plot for the model in Figure [scattHeadLTotalLLine].</p>
</div>
<p>[scattHeadLTotalLResidualPlot]</p>
<p><span>One purpose of residual plots is to identify characteristics or patterns still apparent in data after fitting a model. Figure [sampleLinesAndResPlots] shows three scatterplots with linear models in the first row and residual plots in the second row. Can you identify any patterns remaining in the residuals?</span></p>
<div class="figure">
<img src="05/figures/sampleLinesAndResPlots/sampleLinesAndResPlots" alt="Sample data with their best fitting lines (top row) and their corresponding residual plots (bottom row)." />
<p class="caption">Sample data with their best fitting lines (top row) and their corresponding residual plots (bottom row).</p>
</div>
<p>[sampleLinesAndResPlots]</p>
<p>In the first data set (first column), the residuals show no obvious patterns. The residuals appear to be scattered randomly around the dashed line that represents 0.</p>
<p>The second data set shows a pattern in the residuals. There is some curvature in the scatterplot, which is more obvious in the residual plot. We should not use a straight line to model these data. Instead, a more advanced technique should be used.</p>
<p>The last plot shows very little upwards trend, and the residuals also show no obvious patterns. It is reasonable to try to fit a linear model to the data. However, it is unclear whether there is statistically significant evidence that the slope parameter is different from zero. The point estimate of the slope parameter, labeled <span class="math inline">\(b_1\)</span>, is not zero, but we might wonder if this could just be due to chance. We will address this sort of scenario in Section [inferenceForLinearRegression].</p>
</div>
<div id="describing-linear-relationships-with-correlation" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Describing linear relationships with correlation</h3>
<p><span> , which always takes values between -1 and 1, describes the strength of the linear relationship between two variables. We denote the correlation by <span class="math inline">\(R\)</span>.</span></p>
<p>[</p>
<p><span class="math inline">\(R\)</span><br />
correlation]</p>
<p><span class="math inline">\(R\)</span><br />
correlation</p>
<p>We can compute the correlation using a formula, just as we did with the sample mean and standard deviation. However, this formula is rather complex,<a href="#fn82" class="footnoteRef" id="fnref82"><sup>82</sup></a> so we generally perform the calculations on a computer or calculator. Figure [posNegCorPlots] shows eight plots and their corresponding correlations. Only when the relationship is perfectly linear is the correlation either -1 or 1. If the relationship is strong and positive, the correlation will be near +1. If it is strong and negative, it will be near -1. If there is no apparent linear relationship between the variables, then the correlation will be near zero.</p>
<div class="figure">
<img src="05/figures/posNegCorPlots/posNegCorPlots" alt="Sample scatterplots and their correlations. The first row shows variables with a positive relationship, represented by the trend up and to the right. The second row shows variables with a negative trend, where a large value in one variable is associated with a low value in the other." />
<p class="caption">Sample scatterplots and their correlations. The first row shows variables with a positive relationship, represented by the trend up and to the right. The second row shows variables with a negative trend, where a large value in one variable is associated with a low value in the other.</p>
</div>
<p>[posNegCorPlots]</p>
<p>The correlation is intended to quantify the strength of a linear trend. Nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship; see three such examples in Figure [corForNonLinearPlots].</p>
<div class="figure">
<img src="05/figures/posNegCorPlots/corForNonLinearPlots" alt="Sample scatterplots and their correlations. In each case, there is a strong relationship between the variables. However, the correlation is not very strong, and the relationship is not linear." />
<p class="caption">Sample scatterplots and their correlations. In each case, there is a strong relationship between the variables. However, the correlation is not very strong, and the relationship is not linear.</p>
</div>
<p>[corForNonLinearPlots]</p>
<p>It appears no straight line would fit any of the datasets represented in Figure [corForNonLinearPlots]. Instead, try drawing nonlinear curves on each plot. Once you create a curve for each, describe what is important in your fit.<a href="#fn83" class="footnoteRef" id="fnref83"><sup>83</sup></a></p>
</div>
</div>
<div id="fittingALineByLSR" class="section level2">
<h2><span class="header-section-number">7.2</span> Fitting a line by least squares regression</h2>
<p>Fitting linear models by eye is open to criticism since it is based on an individual preference. In this section, we use <em>least squares regression</em> as a more rigorous approach.</p>
<p>This section considers family income and gift aid data from a random sample of fifty students in the 2011 freshman class of Elmhurst College in Illinois.<a href="#fn84" class="footnoteRef" id="fnref84"><sup>84</sup></a> Gift aid is financial aid that does not need to be paid back, as opposed to a loan. A scatterplot of the data is shown in Figure [elmhurstScatterW2Lines] along with two linear fits. The lines follow a negative trend in the data; students who have higher family incomes tended to have lower gift aid from the university.</p>
<div class="figure">
<img src="05/figures/elmhurstPlots/elmhurstScatterW2Lines" alt="Gift aid and family income for a random sample of 50 freshman students from Elmhurst College. Two lines are fit to the data, the solid line being the least squares line." />
<p class="caption">Gift aid and family income for a random sample of 50 freshman students from Elmhurst College. Two lines are fit to the data, the solid line being the <em>least squares line</em>.</p>
</div>
<p>[elmhurstScatterW2Lines]</p>
<p>Is the correlation positive or negative in Figure [elmhurstScatterW2Lines]?<a href="#fn85" class="footnoteRef" id="fnref85"><sup>85</sup></a></p>
<div id="an-objective-measure-for-finding-the-best-line" class="section level3">
<h3><span class="header-section-number">7.2.1</span> An objective measure for finding the best line</h3>
<p>We begin by thinking about what we mean by “best”. Mathematically, we want a line that has small residuals. Perhaps our criterion could minimize the sum of the residual magnitudes:</p>
<p><span class="math display">\[\begin{aligned}
|e_1| + |e_2| + \dots + |e_n|
\label{sumOfAbsoluteValueOfResiduals}\end{aligned}\]</span></p>
<p>which we could accomplish with a computer program. The resulting dashed line shown in Figure [elmhurstScatterW2Lines] demonstrates this fit can be quite reasonable. However, a more common practice is to choose the line that minimizes the sum of the squared residuals:</p>
<p><span class="math display">\[\begin{aligned}
e_{1}^2 + e_{2}^2 + \dots + e_{n}^2
\label{sumOfSquaresForResiduals}\end{aligned}\]</span></p>
<p>The line that minimizes this <strong>least squares criterion</strong> is represented as the solid line in Figure [elmhurstScatterW2Lines]. This is commonly called the <strong>least squares line</strong>. The following are three possible reasons to choose Criterion  over Criterion :</p>
<ol style="list-style-type: decimal">
<li><p>It is the most commonly used method.</p></li>
<li><p>Computing the line based on Criterion  is much easier by hand and in most statistical software.</p></li>
<li><p>In many applications, a residual twice as large as another residual is more than twice as bad. For example, being off by 4 is usually more than twice as bad as being off by 2. Squaring the residuals accounts for this discrepancy.</p></li>
</ol>
<p>The first two reasons are largely for tradition and convenience; the last reason explains why Criterion  is typically most helpful.<a href="#fn86" class="footnoteRef" id="fnref86"><sup>86</sup></a></p>
</div>
<div id="findingTheLeastSquaresLineSection" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Finding the least squares line</h3>
<p>For the Elmhurst data, we could write the equation of the least squares regression line as</p>
<p><span class="math display">\[\begin{aligned}
\widehat{aid} = \beta_0 + \beta_{1}\times family\_\hspace{0.3mm}income\end{aligned}\]</span></p>
<p>Here the equation is set up to predict gift aid based on a student’s family income, which would be useful to students considering Elmhurst. These two values, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, are the <em>parameters</em> of the regression line.</p>
<p>As in Chapters 4-6, the parameters are estimated using observed data. In practice, this estimation is done using a computer in the same way that other estimates, like a sample mean, can be estimated using a computer or calculator. However, we can also find the parameter estimates by applying two properties of the least squares line:</p>
<ul>
<li><p>The slope of the least squares line can be estimated by</p>
<p><span class="math display">\[\begin{aligned}
b_1 = \frac{s_y}{s_x} R
\label{slopeOfLSRLine}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(R\)</span> is the correlation between the two variables, and <span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the sample standard deviations of the explanatory variable and response, respectively.</p></li>
<li><p>If <span class="math inline">\(\bar{x}\)</span> is the mean of the horizontal variable (from the data) and <span class="math inline">\(\bar{y}\)</span> is the mean of the vertical variable, then the point <span class="math inline">\((\bar{x}, \bar{y})\)</span> is on the least squares line.</p></li>
</ul>
<p>[</p>
<p><span class="math inline">\(b_0, b_1\)</span></p>
<p><br />
Sample<br />
estimates<br />
of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>]</p>
<p><span class="math inline">\(b_0, b_1\)</span></p>
<p><br />
Sample<br />
estimates<br />
of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span></p>
<p>We use <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> to represent the point estimates of the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Table [summaryStatsOfSATGPAData] shows the sample means for the family income and gift aid as $101,800 and $19,940, respectively. Plot the point <span class="math inline">\((101.8, 19.94)\)</span> on Figure  to verify it falls on the least squares line (the solid line).<a href="#fn87" class="footnoteRef" id="fnref87"><sup>87</sup></a></p>
<p><span>l rr</span></p>
<p>&amp; &amp;<br />
&amp;   family income, in $1000s (“<span class="math inline">\(x\)</span>”) &amp;   gift aid, in $1000s (“<span class="math inline">\(y\)</span>”)<br />
&amp; &amp;<br />
mean &amp; <span class="math inline">\(\bar{x} = 101.8\)</span> &amp; <span class="math inline">\(\bar{y} = 19.94\)</span><br />
sd &amp; <span class="math inline">\(s_x = 63.2\)</span> &amp; <span class="math inline">\(s_y = 5.46\)</span></p>
<p><br />
 &amp;<br />
&amp;<br />
[summaryStatsOfSATGPAData]</p>
<p>[findingTheSlopeOfTheLSRLineForIncomeAndAid] Using the summary statistics in Table [summaryStatsOfSATGPAData], compute the slope for the regression line of gift aid against family income.<a href="#fn88" class="footnoteRef" id="fnref88"><sup>88</sup></a></p>
<p>You might recall the <strong>point-slope</strong> form of a line from math class (another common form is <em>slope-intercept</em>). Given the slope of a line and a point on the line, <span class="math inline">\((x_0, y_0)\)</span>, the equation for the line can be written as</p>
<p><span class="math display">\[\begin{aligned}
y - y_0 = slope\times (x - x_0)
\label{pointSlopeFormForALine}\end{aligned}\]</span></p>
<p>A common exercise to become more familiar with foundations of least squares regression is to use basic summary statistics and point-slope form to produce the least squares line.</p>
<p>To identify the least squares line from summary statistics:</p>
<ul>
<li><p>Estimate the slope parameter, <span class="math inline">\(\beta_1\)</span>, by calculating <span class="math inline">\(b_1\)</span> using Equation .</p></li>
<li><p>Noting that the point <span class="math inline">\((\bar{x}, \bar{y})\)</span> is on the least squares line, use <span class="math inline">\(x_0=\bar{x}\)</span> and <span class="math inline">\(y_0=\bar{y}\)</span> along with the slope <span class="math inline">\(b_1\)</span> in the point-slope equation: <span class="math display">\[y - \bar{y} = b_1 (x - \bar{x})\]</span></p></li>
<li><p>Simplify the equation.</p></li>
</ul>
<p><span>Using the point <span class="math inline">\((101.8, 19.94)\)</span> from the sample means and the slope estimate <span class="math inline">\(b_1 = -0.0431\)</span> from Guided Practice [findingTheSlopeOfTheLSRLineForIncomeAndAid], find the least-squares line for predicting aid based on family income.</span> [exampleToFindLSRLineOfElmhurstData] Apply the point-slope equation using <span class="math inline">\((101.8, 19.94)\)</span> and the slope <span class="math inline">\(b_1 = -0.0431\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
y - y_0     &amp;= b_1 (x - x_0) \\
y - 19.94  &amp;= -0.0431(x - 101.8)\end{aligned}\]</span></p>
<p>Expanding the right side and then adding 19.94 to each side, the equation simplifies: <span class="math display">\[\widehat{aid} = 24.3 - 0.0431 \times family\_\hspace{0.3mm}income\]</span> Here we have replaced <span class="math inline">\(y\)</span> with <span class="math inline">\(\widehat{aid}\)</span> and <span class="math inline">\(x\)</span> with <span class="math inline">\(family\_\hspace{0.3mm}income\)</span> to put the equation in context.</p>
<p>We mentioned earlier that a computer is usually used to compute the least squares line. A summary table based on computer output is shown in Table [rOutputForIncomeAidLSRLine] for the Elmhurst data. The first column of numbers provides estimates for <span class="math inline">\({b}_0\)</span> and <span class="math inline">\({b}_1\)</span>, respectively. Compare these to the result from Example [exampleToFindLSRLineOfElmhurstData].</p>
<p><span>l rrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 24.3193 &amp; 1.2915 &amp; 18.83 &amp; 0.0000<br />
family_</p>
<p>income &amp; -0.0431 &amp; 0.0108 &amp; -3.98 &amp; 0.0002<br />
[rOutputForIncomeAidLSRLine]</p>
<p><span>Examine the second, third, and fourth columns in Table [rOutputForIncomeAidLSRLine]. Can you guess what they represent?</span> We’ll describe the meaning of the columns using the second row, which corresponds to <span class="math inline">\(\beta_1\)</span>. The first column provides the point estimate for <span class="math inline">\(\beta_1\)</span>, as we calculated in an earlier example: -0.0431. The second column is a standard error for this point estimate: 0.0108. The third column is a <span class="math inline">\(t\)</span> test statistic for the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span>: <span class="math inline">\(T=-3.98\)</span>. The last column is the p-value for the <span class="math inline">\(t\)</span> test statistic for the null hypothesis <span class="math inline">\(\beta_1=0\)</span> and a two-sided alternative hypothesis: 0.0002. We will get into more of these details in Section [inferenceForLinearRegression].</p>
<p><span>Suppose a high school senior is considering Elmhurst College. Can she simply use the linear equation that we have estimated to calculate her financial aid from the university?</span> She may use it as an estimate, though some qualifiers on this approach are important. First, the data all come from one freshman class, and the way aid is determined by the university may change from year to year. Second, the equation will provide an imperfect estimate. While the linear equation is good at capturing the trend in the data, no individual student’s aid will be perfectly predicted.</p>
</div>
<div id="interpreting-regression-line-parameter-estimates" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Interpreting regression line parameter estimates</h3>
<p>Interpreting parameters in a regression model is often one of the most important steps in the analysis.</p>
<p><span>The slope and intercept estimates for the Elmhurst data are -0.0431 and 24.3. What do these numbers really mean?</span> Interpreting the slope parameter is helpful in almost any application. For each additional $1,000 of family income, we would expect a student to receive a net difference of <span class="math inline">\(\$\text{1,000}\times (-0.0431) = -\$43.10\)</span> in aid on average, i.e. $43.10 <em>less</em>. Note that a higher family income corresponds to less aid because the coefficient of family income is negative in the model. We must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational. That is, increasing a student’s family income may not cause the student’s aid to drop. (It would be reasonable to contact the college and ask if the relationship is causal, i.e. if Elmhurst College’s aid decisions are partially based on students’ family income.)</p>
<p>The estimated intercept <span class="math inline">\(b_0=24.3\)</span> (in $1000s) describes the average aid if a student’s family had no income. The meaning of the intercept is relevant to this application since the family income for some students at Elmhurst is $0. In other applications, the intercept may have little or no practical value if there are no observations where <span class="math inline">\(x\)</span> is near zero.</p>
<p><span> The slope describes the estimated difference in the <span class="math inline">\(y\)</span> variable if the explanatory variable <span class="math inline">\(x\)</span> for a case happened to be one unit larger. The intercept describes the average outcome of <span class="math inline">\(y\)</span> if <span class="math inline">\(x=0\)</span> <em>and</em> the linear model is valid all the way to <span class="math inline">\(x=0\)</span>, which in many applications is not the case.</span></p>
</div>
<div id="extrapolation-is-treacherous" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Extrapolation is treacherous</h3>
<p><em>When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February <span class="math inline">\(6^{th}\)</span> it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.</em></p>
<p>Stephen Colbert</p>
<p>April 6th, 2010<a href="#fn89" class="footnoteRef" id="fnref89"><sup>89</sup></a><br />
Linear models can be used to approximate the relationship between two variables. However, these models have real limitations. Linear regression is simply a modeling framework. The truth is almost always much more complex than our simple line. For example, we do not know how the data outside of our limited window will behave.</p>
<p><span>Use the model <span class="math inline">\(\widehat{aid} = 24.3 - 0.0431\times family\_\hspace{0.3mm}income\)</span> to estimate the aid of another freshman student whose family had income of $1 million.</span> Recall that the units of family income are in $1000s, so we want to calculate the aid for <span class="math inline">\(family\_\hspace{0.3mm}income = 1000\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
24.3 - 0.0431\times family\_\hspace{0.3mm}income  = 24.3 - 0.0431\times 1000 = -18.8\end{aligned}\]</span></p>
<p>The model predicts this student will have -$18,800 in aid (!). Elmhurst College cannot (or at least does not) require any students to pay extra on top of tuition to attend.</p>
<p>Applying a model estimate to values outside of the realm of the original data is called <strong>extrapolation</strong>. Generally, a linear model is only an approximation of the real relationship between two variables. If we extrapolate, we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been explored.</p>
</div>
<div id="using-r2-to-describe-the-strength-of-a-fit" class="section level3">
<h3><span class="header-section-number">7.2.5</span> Using <span class="math inline">\(R^2\)</span> to describe the strength of a fit</h3>
<p>We evaluated the strength of the linear relationship between two variables earlier using the correlation, <span class="math inline">\(R\)</span>. However, it is more common to explain the strength of a linear fit using <span class="math inline">\(R^2\)</span>, called . If provided with a linear model, we might like to describe how closely the data cluster around the linear fit.</p>
<div class="figure">
<img src="05/figures/elmhurstPlots/elmhurstScatterWLSROnly" alt="Gift aid and family income for a random sample of 50 freshman students from Elmhurst College, shown with the least squares regression line." />
<p class="caption">Gift aid and family income for a random sample of 50 freshman students from Elmhurst College, shown with the least squares regression line.</p>
</div>
<p>[elmhurstScatterWLSROnly]</p>
<p>The <span class="math inline">\(R^2\)</span> of a linear model describes the amount of variation in the response that is explained by the least squares line. For example, consider the Elmhurst data, shown in Figure [elmhurstScatterWLSROnly]. The variance of the response variable, aid received, is <span class="math inline">\(s_{aid}^2=29.8\)</span>. However, if we apply our least squares line, then this model reduces our uncertainty in predicting aid using a student’s family income. The variability in the residuals describes how much variation remains after using the model: <span class="math inline">\(s_{_{RES}}^2 = 22.4\)</span>. In short, there was a reduction of <span class="math display">\[\frac{s_{aid}^2 - s_{_{RES}}^2}{s_{_{aid}}^2}
    = \frac{29.8 - 22.4}{29.8} = \frac{7.5}{29.8}
    = 0.25\]</span> or about 25% in the data’s variation by using information about family income for predicting aid using a linear model. This corresponds exactly to the R-squared value:</p>
<p><span class="math display">\[\begin{aligned}
R &amp;= -0.499 &amp;R^2 &amp;= 0.25\end{aligned}\]</span></p>
<p>If a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?<a href="#fn90" class="footnoteRef" id="fnref90"><sup>90</sup></a></p>
</div>
<div id="categoricalPredictorsWithTwoLevels" class="section level3">
<h3><span class="header-section-number">7.2.6</span> Categorical predictors with two levels</h3>
<p>Categorical variables are also useful in predicting outcomes. Here we consider a categorical predictor with two levels (recall that a <em>level</em> is the same as a <em>category</em>). We’ll consider Ebay auctions for a video game, <em>Mario Kart</em> for the Nintendo Wii, where both the total price of the auction and the condition of the game were recorded.<a href="#fn91" class="footnoteRef" id="fnref91"><sup>91</sup></a> Here we want to predict total price based on game condition, which takes values and . A plot of the auction data is shown in Figure [marioKartNewUsed].</p>
<div class="figure">
<img src="05/figures/marioKartNewUsed/marioKartNewUsed" alt="Total auction prices for the video game Mario Kart, divided into used (x=0) and new (x=1) condition games. The least squares regression line is also shown." />
<p class="caption">Total auction prices for the video game <em>Mario Kart</em>, divided into used (<span class="math inline">\(x=0\)</span>) and new (<span class="math inline">\(x=1\)</span>) condition games. The least squares regression line is also shown.</p>
</div>
<p>[marioKartNewUsed]</p>
<p>To incorporate the game condition variable into a regression equation, we must convert the categories into a numerical form. We will do so using an <strong>indicator variable</strong> called , which takes value 1 when the game is new and 0 when the game is used. Using this indicator variable, the linear model may be written as</p>
<p><span class="math display">\[\begin{aligned}
\widehat{price} = \beta_0 + \beta_1 \times \text{\textbf{cond\_\hspace{0.3mm}new}}\end{aligned}\]</span></p>
<p>The fitted model is summarized in Table [marioKartNewUsedRegrSummary], and the model with its parameter estimates is given as</p>
<p><span class="math display">\[\begin{aligned}
\widehat{price} = 42.87 + 10.90 \times \text{\textbf{cond\_\hspace{0.3mm}new}}\end{aligned}\]</span></p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 42.87 &amp; 0.81 &amp; 52.67 &amp; 0.0000<br />
cond_</p>
<p>new &amp; 10.90 &amp; 1.26 &amp; 8.66 &amp; 0.0000<br />
[marioKartNewUsedRegrSummary]</p>
<p><span>Interpret the two parameters estimated in the model for the price of <em>Mario Kart</em> in eBay auctions.</span> The intercept is the estimated price when takes value 0, i.e. when the game is in used condition. That is, the average selling price of a used version of the game is $42.87.</p>
<p>The slope indicates that, on average, new games sell for about $10.90 more than used games.</p>
<p><span> The estimated intercept is the value of the response variable for the first category (i.e. the category corresponding to an indicator value of 0). The estimated slope is the average change in the response variable between the two categories.</span></p>
</div>
</div>
<div id="typesOfOutliersInLinearRegression" class="section level2">
<h2><span class="header-section-number">7.3</span> Types of outliers in linear regression</h2>
<p>In this section, we identify criteria for determining which outliers are important and influential.</p>
<p>Outliers in regression are observations that fall far from the “cloud” of points. These points are especially important because they can have a strong influence on the least squares line.</p>
<p><span>There are six plots shown in Figure [outlierPlots] along with the least squares line and residual plots. For each scatterplot and residual plot pair, identify any obvious outliers and note how they influence the least squares line. Recall that an outlier is any point that doesn’t appear to belong with the vast majority of the other points.</span>[outlierPlotsExample]</p>
<ul>
<li><p>There is one outlier far from the other points, though it only appears to slightly influence the line.</p></li>
<li><p>There is one outlier on the right, though it is quite close to the least squares line, which suggests it wasn’t very influential.</p></li>
<li><p>There is one point far away from the cloud, and this outlier appears to pull the least squares line up on the right; examine how the line around the primary cloud doesn’t appear to fit very well.</p></li>
<li><p>There is a primary cloud and then a small secondary cloud of four outliers. The secondary cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere. There might be an interesting explanation for the dual clouds, which is something that could be investigated.</p></li>
<li><p>There is no obvious trend in the main cloud of points and the outlier on the right appears to largely control the slope of the least squares line.</p></li>
<li><p>There is one outlier far from the cloud, however, it falls quite close to the least squares line and does not appear to be very influential.</p></li>
</ul>
<div class="figure">
<img src="05/figures/outlierPlots/outlierPlots" alt="Six plots, each with a least squares line and residual plot. All data sets have at least one outlier." />
<p class="caption">Six plots, each with a least squares line and residual plot. All data sets have at least one outlier.</p>
</div>
<p>[outlierPlots]</p>
<p>Examine the residual plots in Figure [outlierPlots]. You will probably find that there is some trend in the main clouds of (3) and (4). In these cases, the outliers influenced the slope of the least squares lines. In (5), data with no clear trend were assigned a line with a large trend simply due to one outlier (!).</p>
<p><span> Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with <strong>high leverage</strong>.</span></p>
<p>Points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. If one of these high leverage points does appear to actually invoke its influence on the slope of the line – as in cases (3), (4), and (5) of Example [outlierPlotsExample] – then we call it an <strong>influential point</strong>. Usually we can say a point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line.</p>
<p>It is tempting to remove outliers. Don’t do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings – the “outliers” – they would soon go bankrupt by making poorly thought-out investments.</p>
<p><span>Don’t ignore outliers when fitting a final model</span> <span>If there are outliers in the data, they should not be removed or ignored without a good reason. Whatever final model is fit to the data would not be very helpful if it ignores the most exceptional cases.</span></p>
<p><span>Outliers for a categorical predictor with two levels</span><span> Be cautious about using a categorical predictor when one of the levels has very few observations. When this happens, those few observations become influential points.</span></p>
</div>
<div id="inferenceForLinearRegression" class="section level2">
<h2><span class="header-section-number">7.4</span> Inference for linear regression</h2>
<p>In this section we discuss uncertainty in the estimates of the slope and y-intercept for a regression line. Just as we identified standard errors for point estimates in previous chapters, we first discuss standard errors for these new estimates. However, in the case of regression, we will identify standard errors using statistical software.</p>
<div id="conditions-for-the-least-squares-line" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Conditions for the least squares line</h3>
<p>When performing inference on a least squares line, we generally require the following:</p>
<dl>
<dt>Linearity.</dt>
<dd><p>The data should show a linear trend. If there is a nonlinear trend (e.g. left panel of Figure [whatCanGoWrongWithLinearModel]), an advanced regression method from another book or later course should be applied.</p>
</dd>
<dt>Nearly normal residuals.</dt>
<dd><p>Generally the residuals must be nearly normal. When this condition is found to be unreasonable, it is usually because of outliers or concerns about influential points, which we will discuss in greater depth in Section [typesOfOutliersInLinearRegression]. An example of non-normal residuals is shown in the second panel of Figure [whatCanGoWrongWithLinearModel].</p>
</dd>
<dt>Constant variability.</dt>
<dd><p>The variability of points around the least squares line remains roughly constant. An example of non-constant variability is shown in the third panel of Figure [whatCanGoWrongWithLinearModel].</p>
</dd>
<dt>Independent observations.</dt>
<dd><p>Be cautious about applying regression to data collected sequentially in what is called a <strong>time series</strong>. Such data may have an underlying structure that should be considered in a model and analysis. An example of a time series where independence is violated is shown in the fourth panel of Figure [whatCanGoWrongWithLinearModel].</p>
</dd>
</dl>
<p>For additional information on checking regression conditions, see Section [multipleRegressionModelAssumptions].</p>
<div class="figure">
<img src="05/figures/whatCanGoWrongWithLinearModel/whatCanGoWrongWithLinearModel" alt="Four examples showing when the methods in this chapter are insufficient to apply to the data. In the left panel, a straight line does not fit the data. In the second panel, there are outliers; two points on the left are relatively distant from the rest of the data, and one of these points is very far away from the line. In the third panel, the variability of the data around the line increases with larger values of x. In the last panel, a time series data set is shown, where successive observations are highly correlated." />
<p class="caption">Four examples showing when the methods in this chapter are insufficient to apply to the data. In the left panel, a straight line does not fit the data. In the second panel, there are outliers; two points on the left are relatively distant from the rest of the data, and one of these points is very far away from the line. In the third panel, the variability of the data around the line increases with larger values of <span class="math inline">\(x\)</span>. In the last panel, a time series data set is shown, where successive observations are highly correlated.</p>
</div>
<p>[whatCanGoWrongWithLinearModel]</p>
<p><span>Should we have concerns about applying inference to the Elmhurst data in Figure [elmhurstScatterLSRLine<sub>c</sub>onditions]?</span> The trend appears to be linear, the data fall around the line with no obvious outliers, the variance is roughly constant. These are also not time series observations. It would be reasonable to analyze the model using inference.</p>
<div class="figure">
<img src="05/figures/elmhurstPlots/elmhurstScatterLSRLine_conditions" alt="Gift aid and family income for a random sample of 50 freshman students from Elmhurst College. Two lines are fit to the data, the solid line being the least squares line." />
<p class="caption">Gift aid and family income for a random sample of 50 freshman students from Elmhurst College. Two lines are fit to the data, the solid line being the <em>least squares line</em>.</p>
</div>
<p>[elmhurstScatterLSRLine<sub>c</sub>onditions]</p>
</div>
<div id="midterm-elections-and-unemployment" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Midterm elections and unemployment</h3>
<p>Elections for members of the United States House of Representatives occur every two years, coinciding every four years with the U.S. Presidential election. The set of House elections occurring during the middle of a Presidential term are called . In America’s two-party system, one political theory suggests the higher the unemployment rate, the worse the President’s party will do in the midterm elections.</p>
<p>To assess the validity of this claim, we can compile historical data and look for a connection. We consider every midterm election from 1898 to 2010, with the exception of those elections during the Great Depression. Figure [unemploymentAndChangeInHouse] shows these data and the least-squares regression line:</p>
<p><span class="math display">\[\begin{aligned}
&amp;\text{\% change in House seats for President&#39;s party}  \\
&amp;\qquad\qquad= -6.71 - 1.00\times \text{(unemployment rate)}\end{aligned}\]</span></p>
<p>We consider the percent change in the number of seats of the President’s party (e.g. percent change in the number of seats for Democrats in 2010) against the unemployment rate.</p>
<p>Examining the data, there are no clear deviations from linearity, the constant variance condition, or in the normality of residuals (though we don’t examine a normal probability plot here). While the data are collected sequentially, a separate analysis was used to check for any apparent correlation between successive observations; no such correlation was found.</p>
<div class="figure">
<img src="05/figures/unemploymentAndChangeInHouse/unemploymentAndChangeInHouse" alt="The percent change in House seats for the President’s party in each election from 1898 to 2010 plotted against the unemployment rate. The two points for the Great Depression have been removed, and a least squares regression line has been fit to the data." />
<p class="caption">The percent change in House seats for the President’s party in each election from 1898 to 2010 plotted against the unemployment rate. The two points for the Great Depression have been removed, and a least squares regression line has been fit to the data.</p>
</div>
<p>[unemploymentAndChangeInHouse]</p>
<p>The data for the Great Depression (1934 and 1938) were removed because the unemployment rate was 21% and 18%, respectively. Do you agree that they should be removed for this investigation? Why or why not?<a href="#fn92" class="footnoteRef" id="fnref92"><sup>92</sup></a></p>
<p>There is a negative slope in the line shown in Figure [unemploymentAndChangeInHouse]. However, this slope (and the y-intercept) are only estimates of the parameter values. We might wonder, is this convincing evidence that the “true” linear model has a negative slope? That is, do the data provide strong evidence that the political theory is accurate? We can frame this investigation into a two-sided statistical hypothesis test. We use a two-sided test since a statistically significant result in either direction would be interesting.</p>
<ul>
<li><p><span class="math inline">\(\beta_1 = 0\)</span>. The true linear model has slope zero.</p></li>
<li><p><span class="math inline">\(\beta_1 \neq 0\)</span>. The true linear model has a slope different than zero. The higher the unemployment, the greater the loss for the President’s party in the House of Representatives, or vice-versa.</p></li>
</ul>
<p>We would reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_A\)</span> if the data provide strong evidence that the true slope parameter is less than zero. To assess the hypotheses, we identify a standard error for the estimate, compute an appropriate test statistic, and identify the p-value.</p>
</div>
<div id="testStatisticForTheSlope" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Understanding regression output from software</h3>
<p>Just like other point estimates we have seen before, we can compute a standard error and test statistic for <span class="math inline">\(b_1\)</span>. We will generally label the test statistic using a <span class="math inline">\(T\)</span>, since it follows the <span class="math inline">\(t\)</span> distribution.</p>
<p>We will rely on statistical software to compute the standard error and leave the explanation of how this standard error is determined to a second or third statistics course. Table [midtermElectionUnemploymentRRegressionOutput] shows software output for the least squares regression line in Figure [unemploymentAndChangeInHouse]. The row labeled <em>unemp</em> represents the information for the slope, which is the coefficient of the unemployment variable.</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; -6.7142 &amp; 5.4567 &amp; -1.23 &amp; 0.2300<br />
unemp &amp; -1.0010 &amp; 0.8717 &amp; -1.15 &amp; 0.2617<br />
<br />
[midtermElectionUnemploymentRRegressionOutput]</p>
<p><span>What do the first and second columns of Table [midtermElectionUnemploymentRRegressionOutput] represent?</span> The entries in the first column represent the least squares estimates, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, and the values in the second column correspond to the standard errors of each estimate.</p>
<p>We previously used a <span class="math inline">\(t\)</span> test statistic for hypothesis testing in the context of numerical data. Regression is very similar. In the hypotheses we consider, the null value for the slope is 0, so we can compute the test statistic using the T (or Z) score formula:</p>
<p><span class="math display">\[\begin{aligned}
T = \frac{\text{estimate} - \text{null value}}{\text{SE}} = \frac{-1.0010 - 0}{0.8717} = -1.15\end{aligned}\]</span></p>
<p>We can look for the two-tailed p-value – shown in Figure [oneSidedTailForMidtermUnemploymentHT] – using the probability table for the <span class="math inline">\(t\)</span> distribution in Appendix [tDistributionTable] on page .</p>
<div class="figure">
<img src="05/figures/pValueMidtermUnemploymentHT/pValueMidtermUnemploymentHT" alt="The distribution shown here is the sampling distribution for b_1, if the null hypothesis was true. The shaded tail represents the p-value for the hypothesis test evaluating whether there is convincing evidence that higher unemployment corresponds to a greater loss of House seats for the President’s party during a midterm election." />
<p class="caption">The distribution shown here is the sampling distribution for <span class="math inline">\(b_1\)</span>, if the null hypothesis was true. The shaded tail represents the p-value for the hypothesis test evaluating whether there is convincing evidence that higher unemployment corresponds to a greater loss of House seats for the President’s party during a midterm election.</p>
</div>
<p>[oneSidedTailForMidtermUnemploymentHT]</p>
<p><span>Table [midtermElectionUnemploymentRRegressionOutput] offers the degrees of freedom for the test statistic <span class="math inline">\(T\)</span>: <span class="math inline">\(df=25\)</span>. Identify the p-value for the hypothesis test.</span> Looking in the 25 degrees of freedom row in Appendix [tDistributionTable], we see that the absolute value of the test statistic is smaller than any value listed, which means the tail area and therefore also the p-value is larger than 0.200 (two tails!). Because the p-value is so large, we fail to reject the null hypothesis. That is, the data do not provide convincing evidence that unemployment is a good predictor of how well a president’s party will do in the midterm elections for the House of Representatives.</p>
<p>We could have identified the <span class="math inline">\(t\)</span> test statistic from the software output in Table [midtermElectionUnemploymentRRegressionOutput], shown in the second row (unemp) and third column (t value). The entry in the second row and last column in Table [midtermElectionUnemploymentRRegressionOutput] represents the p-value for the two-sided hypothesis test where the null value is zero.</p>
<p><span> We usually rely on statistical software to identify point estimates and standard errors for parameters of a regression line. After verifying conditions hold for fitting a line, we can use the methods learned in Section [oneSampleMeansWithTDistribution] for the <span class="math inline">\(t\)</span> distribution to create confidence intervals for regression parameters or to evaluate hypothesis tests.</span></p>
<p><span>Don’t carelessly use the p-value from regression output</span><span>The last column in regression output often lists p-values for one particular hypothesis: a two-sided test where the null value is zero. If a hypothesis test should be one-sided or a comparison is being made to a value other than zero, be cautious about using the software output to obtain the p-value.</span></p>
<p><span>Examine Figure , which relates the Elmhurst College aid and student family income. How sure are you that the slope is statistically significantly different from zero? That is, do you think a formal hypothesis test would reject the claim that the true slope of the line should be zero?</span> [overallAidIncomeInformalAssessmentOfRegressionLineSlope] While the relationship between the variables is not perfect, there is an evident decreasing trend in the data. This suggests the hypothesis test will reject the null claim that the slope is zero.</p>
<p>Table [rOutputForIncomeAidLSRLineInInferenceSection] shows statistical software output from fitting the least squares regression line shown in Figure [elmhurstScatterWLSROnly]. Use this output to formally evaluate the following hypotheses. <span class="math inline">\(H_0\)</span>: The true coefficient for family income is zero. <span class="math inline">\(H_A\)</span>: The true coefficient for family income is not zero.<a href="#fn93" class="footnoteRef" id="fnref93"><sup>93</sup></a></p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 24.3193 &amp; 1.2915 &amp; 18.83 &amp; 0.0000<br />
family_</p>
<p>income &amp; -0.0431 &amp; 0.0108 &amp; -3.98 &amp; 0.0002<br />
<br />
[rOutputForIncomeAidLSRLineInInferenceSection]</p>
<p><span> If conditions for fitting the regression line do not hold, then the methods presented here should not be applied. The standard error or distribution assumption of the point estimate – assumed to be normal when applying the <span class="math inline">\(t\)</span> test statistic – may not be valid.</span></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="80">
<li id="fn80"><p>If a model underestimates an observation, then the model estimate is below the actual. The residual, which is the actual observation value minus the model estimate, must then be positive. The opposite is true when the model overestimates the observation: the residual is negative.<a href="linRegrForTwoVar.html#fnref80">↩</a></p></li>
<li id="fn81"><p>(<span class="math inline">\(+\)</span>) First compute the predicted value based on the model: <span class="math display">\[\hat{y}_{+} = 41+0.59x_{+} = 41+0.59\times 85.0 = 91.15\]</span> Then the residual is given by <span class="math display">\[e_{+} = y_{+} - \hat{y}_{+} = 98.6-91.15=7.45\]</span>This was close to the earlier estimate of 7.</p>
<p>(<span class="math inline">\(\triangle\)</span>) <span class="math inline">\(\hat{y}_{\triangle} = 41+0.59x_{\triangle} = 97.3\)</span>. <span class="math inline">\(e_{\triangle} = y_{\triangle} - \hat{y}_{\triangle} = -3.3\)</span>, close to the estimate of -4.<a href="linRegrForTwoVar.html#fnref81">↩</a></p></li>
<li id="fn82"><p>Formally, we can compute the correlation for observations <span class="math inline">\((x_1, y_1)\)</span>, <span class="math inline">\((x_2, y_2)\)</span>, …, <span class="math inline">\((x_n, y_n)\)</span> using the formula</p>
<p><span class="math display">\[\begin{aligned}
R = \frac{1}{n-1}\sum_{i=1}^{n} \frac{x_i-\bar{x}}{s_x}\frac{y_i-\bar{y}}{s_y}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\bar{y}\)</span>, <span class="math inline">\(s_x\)</span>, and <span class="math inline">\(s_y\)</span> are the sample means and standard deviations for each variable.<a href="linRegrForTwoVar.html#fnref82">↩</a></p></li>
<li id="fn83"><p>We’ll leave it to you to draw the lines. In general, the lines you draw should be close to most points and reflect overall trends in the data.<a href="linRegrForTwoVar.html#fnref83">↩</a></p></li>
<li id="fn84"><p>These data were sampled from a table of data for all freshman from the 2011 class at Elmhurst College that accompanied an article titled <em>What Students Really Pay to Go to College</em> published online by <em>The Chronicle of Higher Education</em>: <a href="http://chronicle.com/article/What-Students-Really-Pay-to-Go/131435">chronicle.com/article/What-Students-Really-Pay-to-Go/131435</a><a href="linRegrForTwoVar.html#fnref84">↩</a></p></li>
<li id="fn85"><p>Larger family incomes are associated with lower amounts of aid, so the correlation will be negative. Using a computer, the correlation can be computed: -0.499.<a href="linRegrForTwoVar.html#fnref85">↩</a></p></li>
<li id="fn86"><p>There are applications where Criterion  may be more useful, and there are plenty of other criteria we might consider. However, this book only applies the least squares criterion.<a href="linRegrForTwoVar.html#fnref86">↩</a></p></li>
<li id="fn87"><p>If you need help finding this location, draw a straight line up from the x-value of 100 (or thereabout). Then draw a horizontal line at 20 (or thereabout). These lines should intersect on the least squares line.<a href="linRegrForTwoVar.html#fnref87">↩</a></p></li>
<li id="fn88"><p>Apply Equation  with the summary statistics from Table [summaryStatsOfSATGPAData] to compute the slope:</p>
<p><span class="math display">\[\begin{aligned}
b_1 = \frac{s_y}{s_x} R = \frac{5.46}{63.2}(-0.499) = -0.0431\end{aligned}\]</span><a href="linRegrForTwoVar.html#fnref88">↩</a></p></li>
<li id="fn90"><p>About <span class="math inline">\(R^2 = (-0.97)^2 = 0.94\)</span> or 94% of the variation is explained by the linear model.<a href="linRegrForTwoVar.html#fnref90">↩</a></p></li>
<li id="fn91"><p>These data were collected in Fall 2009 and may be found at <a href="http://www.openintro.org">openintro.org</a>.<a href="linRegrForTwoVar.html#fnref91">↩</a></p></li>
<li id="fn92"><p>We will provide two considerations. Each of these points would have very high leverage on any least-squares regression line, and years with such high unemployment may not help us understand what would happen in other years where the unemployment is only modestly high. On the other hand, these are exceptional cases, and we would be discarding important information if we exclude them from a final analysis.<a href="linRegrForTwoVar.html#fnref92">↩</a></p></li>
<li id="fn93"><p>We look in the second row corresponding to the family income variable. We see the point estimate of the slope of the line is -0.0431, the standard error of this estimate is 0.0108, and the <span class="math inline">\(t\)</span> test statistic is -3.98. The p-value corresponds exactly to the two-sided test we are interested in: 0.0002. The p-value is so small that we reject the null hypothesis and conclude that family income and financial aid at Elmhurst College for freshman entering in the year 2011 are negatively correlated and the true slope parameter is indeed less than 0, just as we believed in Example [overallAidIncomeInformalAssessmentOfRegressionLineSlope].<a href="linRegrForTwoVar.html#fnref93">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferenceForNumericalDataANOVA.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multipleRegressionAndANOVA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/statistics/blob/master/07-Linear_Regression.Rmd",
"text": "Edit"
},
"download": ["statistics.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
