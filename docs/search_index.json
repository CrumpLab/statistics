[
["inferenceForNumericalDatattest.html", "Chapter 6 t-Tests 6.1 Check your confidence in your mean 6.2 One-sample t-test", " Chapter 6 t-Tests One day, many moons ago, William Sealy Gosset got a job working for Guinness Breweries. They make that really dark and famous Irish beer called Guinness. What happens next went something like this (total fabrication, but mostly on point). Guinness wanted all of its beers to the best beers. No mistakes, No bad beers. They wanted to improve their quality control, so that wherever you pour a Guiness, anywhere in the world, it always comes out just fantastic, let’s say 5 stars out of 5 every time. They want to be the best. Guiness had some beer tasters, who were super-experts. Everytime they tasted a Guiness from the factor that wasn’t 5 out of 5, they knew right away. But, Guiness had a big problem. They would make a keg of beer, and they would want to know if every single pint that would come out would be a 5 out of 5. So, the beer tasters drank pint after pint out of the keg, until it was gone. Some kegs were all 5 out of 5s. Some weren’t, Guiness needed to fix that. But, the biggest problem was that, after the testing, there was no beer left to sell, the testers drank it all (remember I’m kind of making this part up to illustrate a point, they probably still had beer to sell). Guiness had a sampling and population problem. They wanted to know that the entire population of the beers they made were all 5 out of 5 stars. But, if they sampled the entire population, they would drink all of their beer, and wouldn’t have any left to sell. Enter William Sealy Gosset. Gosset figured out the solution to the problem. He asked questions like this: How many samples do I really need to take to know the whole population is 5 out of 5? What’s the fewest amount of samples I need to take to know the above, that would mean Guiness could test fewer beers for quality, and sell more beers for profit, and making the product testing time shorter. Gosset solved those questions, and he invented something called the Student’s t-test. Gosset was working for Guinness, and could be fired for releasing trade-secrets that he invented (the t-test). But, Gosset published the work anyways, under a pseudonym. He called himself Student, hence Student’s t-test. Now you know the rest of the story. It turns out this was a very nice thing for Gosset to have done. t-tests are used all the time, and they are useful, that’s why they are used. In this chapter we learn how they work. You’ll be surprised to learn that what we’ve already talked about, (the Crump Test, and the Randomization Test), are both very very similar to the t-test. So, in general, you have already been thinking about the things you need to think about to understand t-tests. You’re probably wondering what is this \\(t\\), what does \\(t\\) mean? We will tell you. Before we tell what it means, we first tell you about one more idea. 6.1 Check your confidence in your mean We’ve talked about getting a sample of data. We know we can find the mean, we know can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data. You might be thinking about these things, like mean and standard deviation, as very different things. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things. What if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about how confident you should be that the number represents all of the numbers in the sample. It could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data. How can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean? We can do this using a ratio: \\(\\frac{mean}{\\text{standard deviation}}\\) Think about what happens here. We are dividing a number by a number. Look at what happens: \\(\\frac{number}{\\text{same number}} = 1\\) \\(\\frac{number}{\\text{smaller number}} = \\text{big number}\\) compared to: \\(\\frac{number}{\\text{bigger number}} = \\text{smaller number}\\) Imagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula? $ = 50 $ Imgaine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula? $ = 0.5 $ Notice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case. What did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpet this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio becomes different kinds of “statistics”, and the ratios will look like this in general: \\(\\text{name of statistic} = \\frac{\\text{measure of what we know}}{\\text{measure of what we don&#39;t know}}\\) or, to say it using different words: \\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\) In fact, this is the general formula for the t-test. Big surprise! 6.2 One-sample t-test Now we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test. Commonly, the one-sample t-test is used to estimate the chances that your sample came from a particular popualtion. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean. Straight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Offically, the uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember, from the chapter on descriptive statistics, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific: 6.2.1 Formulas for one-sample t-test \\(\\text{name of statistic} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\) \\(\\text{t} = \\frac{\\text{measure of effect}}{\\text{measure of error}}\\) \\(\\text{t} = \\frac{\\text{Mean difference}}{\\text{standard error}}\\) \\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}}\\) \\(\\text{t} = \\frac{\\text{Sample Mean - Population Mean}}{\\text{Sample Standard Error}}\\) \\(\\text{Estimated Standard Error} = \\text{Standard Error of Sample} = \\frac{s}{\\sqrt{N}}\\) Where, s is the sample standard deviation. Some of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it. 6.2.2 What does t represent? \\(t\\) gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations. \\(t\\) is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). What kinds of numbers should we expect to find for these \\(ts\\)? Well, wouldn’t that depend on a lot of things, how could we know that? Let’s start small, and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would t be? It would be zero, because we first subtract sample mean from the population mean, 5-5=0. So, t can be zero, when there is no difference. Let’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can t be here? It will be a positive number, because 6-5= +1. But, will t be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then t could be 1, because 1/1 = 1. If the sample standard error is smaller than 1, what happens to \\(t\\)? It get’s bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, t would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As t get’s bigger we could be more confident in the mean difference we are measuring. Can \\(t\\) be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then \\(t\\) will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the \\(t\\) become negative. What if our sample mean was 4. Well, then \\(t\\) will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.). So, that is some intutions about what the kinds of values t can take. \\(t\\) can be positive or negative, and big or small. Fine then, what can we do with this? 6.2.3 Calculating t from data Let’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz 5 questions on it. There’s a 50% chance of getting each answer correct. Every student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). Want we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at the table: ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union students scores mean Difference_from_Mean Squared_Deviations 1 50 61 -11 121 2 70 61 9 81 3 60 61 -1 1 4 40 61 -21 441 5 80 61 19 361 6 30 61 -31 961 7 90 61 29 841 8 60 61 -1 1 9 70 61 9 81 10 60 61 -1 1 Sums 610 610 0 2890 Means 61 61 0 289 sd 17.92 SEM 5.67 t 1.94003527336861 You can see the column column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation. Remember the sample standard deviation is the square root of the sample variance, or: \\(\\text{sample standard deviation} = \\sqrt{\\frac{\\sum_{i}^{n}({x_{i}-\\bar{x})^2}}{N-1}}\\) \\(\\text{sd} = \\sqrt{\\frac{2890}{10-1}} = 17.92\\) The standard error of the mean, is the standard deviation divided by the square root of N $ = = = 5.67 $ \\(t\\) is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean. \\(\\text{t} = \\frac{\\bar{X}-u}{S_{\\bar{X}}} = \\frac{\\bar{X}-u}{SEM} = \\frac{61-50}{5.67} = 1.94\\) And, that is you how calculate \\(t\\), by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate \\(t\\) using software, so it will just spit out \\(t\\). For example in R, all you have to do is this: t.test(scores, mu=50) ## ## One Sample t-test ## ## data: scores ## t = 1.9412, df = 9, p-value = 0.08415 ## alternative hypothesis: true mean is not equal to 50 ## 95 percent confidence interval: ## 48.18111 73.81889 ## sample estimates: ## mean of x ## 61 6.2.4 How does \\(t\\) behave? "]
]
