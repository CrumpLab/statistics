[
["gifs.html", "Chapter 11 GIFs 11.1 Correlation GIFs 11.2 Sampling distributions 11.3 Statistical Inference", " Chapter 11 GIFs This is the place where I put the stats gifs as I make them. The gifs can downloaded from this page, or they can be downloaded from this folder on the github repo for this book https://github.com/CrumpLab/statistics/tree/master/gifs. Please feel free to use them however you wish. The source code for compiling the gifs in R is shown alonside each gif. The animiations are made possible by the gganimate package. This is a work in progress, subject to change and addition library(ggplot2) library(gganimate) library(dplyr) 11.1 Correlation GIFs Note regression lines and confidence bands can be added using geom_smooth(method=lm, se=T) 11.1.1 N=10, both variables drawn from a uniform distribution all_df&lt;-data.frame() for(sim in 1:10){ North_pole &lt;- runif(10,1,10) South_pole &lt;- runif(10,1,10) t_df&lt;-data.frame(simulation=rep(sim,10), North_pole, South_pole) all_df&lt;-rbind(all_df,t_df) } ggplot(all_df,aes(x=North_pole,y=South_pole))+ geom_point()+ geom_smooth(method=lm, se=FALSE)+ theme_classic()+ transition_states( simulation, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.1.2 Correlation between random deviates from uniform distribution across four sample sizes N= 10,50,100,1000 All values sampled from a uniform distribution all_df&lt;-data.frame() for(sim in 1:10){ for(n in c(10,50,100,1000)){ North_pole &lt;- runif(n,1,10) South_pole &lt;- runif(n,1,10) t_df&lt;-data.frame(nsize=rep(n,n), simulation=rep(sim,n), North_pole, South_pole) all_df&lt;-rbind(all_df,t_df) } } ggplot(all_df,aes(x=North_pole,y=South_pole))+ geom_point()+ geom_smooth(method=lm, se=FALSE)+ theme_classic()+ facet_wrap(~nsize)+ transition_states( simulation, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.1.3 Correlation between random deviates from normal distribution across four sample sizes N= 10,50,100,1000 All values sampled from the same normal distribution (mean=0, sd=1) all_df&lt;-data.frame() for(sim in 1:10){ for(n in c(10,50,100,1000)){ North_pole &lt;- rnorm(n,0,1) South_pole &lt;- rnorm(n,0,1) t_df&lt;-data.frame(nsize=rep(n,n), simulation=rep(sim,n), North_pole, South_pole) all_df&lt;-rbind(all_df,t_df) } } ggplot(all_df,aes(x=North_pole,y=South_pole))+ geom_point()+ geom_smooth(method=lm, se=FALSE)+ theme_classic()+ facet_wrap(~nsize)+ transition_states( simulation, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.1.4 Type I errors, sampling random deviates from normal distribution with regression lines These scatterplots only show what would be type I errors (assuming alpha=.05). The X and Y values were both sampled from the same normal distribution (mean = 0, sd=1). 1000 simulations were conducted for each sample size (10,50,100,1000). For each, the animiation shows 10 scatterplots where the observed “correlation” would have passed a significance test. According to definition, these correlations only arise from random normal deviates 5% of the time, but when they do arise for small sample sizes, they look fairly convincing. all_df&lt;-data.frame() for(n in c(10,50,100,1000)){ count_sims&lt;-0 for(sim in 1:1000){ North_pole &lt;- rnorm(n,0,1) South_pole &lt;- rnorm(n,0,1) if(cor.test(North_pole,South_pole)$p.value&lt;.05){ count_sims&lt;-count_sims+1 t_df&lt;-data.frame(nsize=rep(n,n), simulation=rep(count_sims,n), North_pole, South_pole) all_df&lt;-rbind(all_df,t_df) if(count_sims==10){ break } } } } ggplot(all_df,aes(x=North_pole,y=South_pole))+ geom_point()+ geom_smooth(method=lm, se=TRUE)+ theme_classic()+ facet_wrap(~nsize)+ transition_states( simulation, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.1.5 Cell-size and correlation This simulation illustrates how the behavior of correlating two random normal samples as a function of cell-size. The sample-size is always set at N=10. For each panel, the simulation uses an increasing cell-size to estimate the mean for X and Y. When cell-size is 1, 10 X and Y values are drawn from the same normal (u=0, sd=1). When cell-size is 5, for each X,Y score in the plot, 5 samples were drawn from the same normal, and then the mean of the samples is plotted. The effect of cell-size shrinks the dot cloud, as both X and Y scores provide better estimates of the population mean = 0. Cell-size has no effect on the behavior of r, which swings around because sample-size N is small. These are all random, so there is always a 5% type I error rate (alpha =.05). get_sampling_means&lt;-function(m,sd,cell_size,s_size){ save_means&lt;-length(s_size) for(i in 1:s_size){ save_means[i]&lt;-mean(rnorm(cell_size,m,sd)) } return(save_means) } all_df&lt;-data.frame() for(n in c(1,5,10,100)){ count_sims&lt;-0 for(sim in 1:10){ North_pole &lt;- get_sampling_means(0,1,n,10) South_pole &lt;- get_sampling_means(0,1,n,10) count_sims&lt;-count_sims+1 t_df&lt;-data.frame(nsize=rep(n,10), simulation=rep(count_sims,10), North_pole, South_pole) all_df&lt;-rbind(all_df,t_df) } } ggplot(all_df,aes(x=North_pole,y=South_pole))+ geom_point()+ geom_smooth(method=lm, se=TRUE)+ theme_classic()+ facet_wrap(~nsize)+ ggtitle(&quot;Random scatterplots, N=10, Cell-size = 1,5,10,100&quot;)+ transition_states( simulation, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.1.6 Regression We look at how the residuals (error from points to line) behave as the regression lines moves above and below it’s true value. The total error associated with all of the red lines is represents by the grey area. This total error is smallest (minimized) when the black line overlaps with the blue regression line (the best fit line). The total error expands as the black line moves away from the regression. That’s why the regression line is the least wrong (best fit) line to skewer the data (according to least squares definition) d &lt;- mtcars fit &lt;- lm(mpg ~ hp, data = d) d$predicted &lt;- predict(fit) # Save the predicted values d$residuals &lt;- residuals(fit) # Save the residual values coefs&lt;-coef(lm(mpg ~ hp, data = mtcars)) coefs[1] coefs[2] x&lt;-d$hp move_line&lt;-c(seq(-6,6,.5),seq(6,-6,-.5)) total_error&lt;-length(length(move_line)) cnt&lt;-0 for(i in move_line){ cnt&lt;-cnt+1 predicted_y &lt;- coefs[2]*x + coefs[1]+i error_y &lt;- (predicted_y-d$mpg)^2 total_error[cnt]&lt;-sqrt(sum(error_y)/32) } move_line_sims&lt;-rep(move_line,each=32) total_error_sims&lt;-rep(total_error,each=32) sims&lt;-rep(1:50,each=32) d&lt;-d %&gt;% slice(rep(row_number(), 50)) d&lt;-cbind(d,sims,move_line_sims,total_error_sims) anim&lt;-ggplot(d, aes(x = hp, y = mpg, frame=sims)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightblue&quot;) + geom_abline(intercept = 30.09886+move_line_sims, slope = -0.06822828)+ lims(x = c(0,400), y = c(-10,40))+ geom_segment(aes(xend = hp, yend = predicted+move_line_sims, color=&quot;red&quot;), alpha = .5) + geom_point() + geom_ribbon(aes(ymin = predicted+move_line_sims - total_error_sims, ymax = predicted+move_line_sims + total_error_sims), fill = &quot;lightgrey&quot;, alpha=.2)+ theme_classic()+ theme(legend.position=&quot;none&quot;)+ xlab(&quot;X&quot;)+ylab(&quot;Y&quot;)+ transition_manual(frames=sims)+ enter_fade() + exit_fade()+ ease_aes(&#39;sine-in-out&#39;) animate(anim,fps=5) 11.2 Sampling distributions 11.2.1 Sampling from a uniform distribution Animation shows histograms for N=20, sampled from a uniform distribution, along with mean (red line). Uniform distribution in this case is integer values from 1 to 10. a&lt;-round(runif(20*10,1,10)) df&lt;-data.frame(a,sample=rep(1:10,each=20)) df2&lt;-aggregate(a~sample,df,mean) df&lt;-cbind(df,mean_loc=rep(df2$a,each=20)) library(gganimate) ggplot(df,aes(x=a, group=sample,frame=sample)) + geom_histogram() + geom_vline(aes(xintercept=mean_loc,frame = sample),color=&quot;red&quot;)+ scale_x_continuous(breaks=seq(1,10,1))+ theme_classic()+ transition_states( sample, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.2.2 Sampling distribution of the mean, Normal population distribution and sample histograms This animiation illustrates the relationship between a distribution (population), samples from the distribution, and the sampling distribution of the sample means, all as a function of n Normal distribution in red. Individual sample histograms in grey. Vertical red line is mean of individual sample. Histograms for sampling distribution of the sample mean in blue. Vertical blue line is mean of the sampling distribution of the sample mean. Note: for purposes of the animation (and because it was easier to do this way), the histograms for the sampling distribution of the sample means have different sizes. When sample-size = 10, the histogram shows 10 sample means. When sample size=100, the histogram shows 100 sample means. I could have simulated many more sample means (say 10000) for each, but then the histograms for the sample means would be static. The y-axis is very rough. The heights of the histrograms and distributions were scaled to be in the same range for the animation. get_sampling_means&lt;-function(m,sd,s_size){ save_means&lt;-length(s_size) for(i in 1:s_size){ save_means[i]&lt;-mean(rnorm(s_size,m,sd)) } return(save_means) } all_df&lt;-data.frame() for(sims in 1:10){ for(n in c(10,50,100,1000)){ sample&lt;-rnorm(n,0,1) sample_means&lt;-get_sampling_means(0,1,n) t_df&lt;-data.frame(sims=rep(sims,n), sample, sample_means, sample_size=rep(n,n), sample_mean=rep(mean(sample),n), sampling_mean=rep(mean(sample_means),n) ) all_df&lt;-rbind(all_df,t_df) } } ggplot(all_df, aes(x=sample))+ geom_histogram(aes(y=(..density..)/max(..density..)^.8),color=&quot;white&quot;,fill=&quot;grey&quot;)+ geom_histogram(aes(x=sample_means,y=(..density..)/max(..density..)),fill=&quot;blue&quot;,color=&quot;white&quot;,alpha=.5)+ stat_function(fun = dnorm, args = list(mean = 0, sd = 1), lwd = .75, col = &#39;red&#39;)+ geom_vline(aes(xintercept=sample_mean,frame=sims),color=&quot;red&quot;)+ geom_vline(aes(xintercept=sampling_mean,frame=sims),color=&quot;blue&quot;)+ facet_wrap(~sample_size)+xlim(-3,3)+ theme_classic()+ggtitle(&quot;Population (red), Samples (grey), \\n and Sampling distribution of the mean (blue)&quot;)+ylab(&quot;Rough likelihoods&quot;)+ xlab(&quot;value&quot;)+ transition_states( sims, transition_length = 2, state_length = 1 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) 11.3 Statistical Inference 11.3.1 Randomization Test This is an attempt at visualizing a randomization test. Samples are taken under two conditions of the IV (A and B). At the beginning of the animation, the original scores in the first condition are shown as green dots on the left, and the original scores in the second condition are the red dots on the right. The means for each group are the purple dots. During the randomization, the original scores are shuffled randomly between the two conditions. After each shuffle, two new means are computed and displayed as the yellow dots. This occurs either for all permutations, or for a large random sample of them. The animation shows the original scores being shuffled around across the randomizations (the colored dots switch their original condition, appearing from side to side). For intuitive inference, one might look at the range of motion of the yellow dots. This is how the mean difference between group 1 and group 2 behaves under randomization. It’s what chance can do. If the difference between the purple dots is well outside the range of motion of the yellow dots, then the mean difference observed in the beginning is not likely produced by chance. study&lt;-round(runif(10,80,100)) no_study&lt;-round(runif(10,40,90)) study_df&lt;-data.frame(student=seq(1:10),study,no_study) mean_original&lt;-data.frame(IV=c(&quot;studied&quot;,&quot;didnt_study&quot;), means=c(mean(study),mean(no_study))) t_df&lt;-data.frame(sims=rep(1,20), IV=rep(c(&quot;studied&quot;,&quot;didnt_study&quot;),each=10), values=c(study,no_study), rand_order=rep(c(0,1),each=10)) raw_df&lt;-t_df for(i in 2:10){ new_index&lt;-sample(1:20) t_df$values&lt;-t_df$values[new_index] t_df$rand_order&lt;-t_df$rand_order[new_index] t_df$sims&lt;-rep(i,20) raw_df&lt;-rbind(raw_df,t_df) } raw_df$rand_order&lt;-as.factor(raw_df$rand_order) rand_df&lt;-aggregate(values~sims*IV,raw_df,mean) names(rand_df)&lt;-c(&quot;sims&quot;,&quot;IV&quot;,&quot;means&quot;) a&lt;-ggplot(raw_df,aes(x=IV,y=values,color=rand_order,size=3))+ geom_point(stat=&quot;identity&quot;,alpha=.5)+ geom_point(data=mean_original,aes(x=IV,y=means),stat=&quot;identity&quot;,shape=21,size=6,color=&quot;black&quot;,fill=&quot;mediumorchid2&quot;)+ geom_point(data=rand_df,aes(x=IV,y=means),stat=&quot;identity&quot;,shape=21,size=6,color=&quot;black&quot;,fill=&quot;gold&quot;)+ theme_classic(base_size = 15)+ coord_cartesian(ylim=c(40, 100))+ theme(legend.position=&quot;none&quot;) + ggtitle(&quot;Randomization test: Original Means (purple), \\n Randomized means (yellow) \\n Original scores (red,greenish)&quot;)+ transition_states( sims, transition_length = 1, state_length = 2 )+enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) animate(a,nframes=100,fps=5) "]
]
