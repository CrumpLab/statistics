<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Answering questions with data</title>
  <meta name="description" content="An introductory statistics textbook for psychology students">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Answering questions with data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introductory statistics textbook for psychology students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Answering questions with data" />
  
  <meta name="twitter:description" content="An introductory statistics textbook for psychology students" />
  

<meta name="author" content="Author List, TBD">
<meta name="author" content="Current Contributions, Matthew J. C. Crump">
<meta name="author" content="Adapted work so far from Navarro, D., Diaz, Barr, &amp; Cetinkaya-Rundel">
<meta name="author" content="In Draft subject to change, we will get all attributions and licenses done correctly">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="foundations-for-inference.html">
<link rel="next" href="inferenceForNumericalDataANOVA.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79429674-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-79429674-3');
</script>


<script type="text/javascript">
mattcrump=1;
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="tufte.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#important-notes"><i class="fa fa-check"></i><b>0.1</b> Important notes</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="why-statistics.html"><a href="why-statistics.html"><i class="fa fa-check"></i><b>1</b> Why Statistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="why-statistics.html"><a href="why-statistics.html#on-the-psychology-of-statistics"><i class="fa fa-check"></i><b>1.1</b> On the psychology of statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="why-statistics.html"><a href="why-statistics.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1.1</b> The curse of belief bias</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="why-statistics.html"><a href="why-statistics.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="why-statistics.html"><a href="why-statistics.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="why-statistics.html"><a href="why-statistics.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="why-statistics.html"><a href="why-statistics.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
<li class="chapter" data-level="1.6" data-path="why-statistics.html"><a href="why-statistics.html#a-brief-introduction-to-research-designchstudydesign"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research design[ch:studydesign]</a><ul>
<li class="chapter" data-level="1.6.1" data-path="why-statistics.html"><a href="why-statistics.html#introduction-to-psychological-measurementsecmeasurement"><i class="fa fa-check"></i><b>1.6.1</b> Introduction to psychological measurement [sec:measurement]</a></li>
<li class="chapter" data-level="1.6.2" data-path="why-statistics.html"><a href="why-statistics.html#scales-of-measurementsecscales"><i class="fa fa-check"></i><b>1.6.2</b> Scales of measurement[sec:scales]</a></li>
<li class="chapter" data-level="1.6.3" data-path="why-statistics.html"><a href="why-statistics.html#assessing-the-reliability-of-a-measurementsecreliability"><i class="fa fa-check"></i><b>1.6.3</b> Assessing the reliability of a measurement [sec:reliability]</a></li>
<li class="chapter" data-level="1.6.4" data-path="why-statistics.html"><a href="why-statistics.html#the-role-of-variables-predictors-and-outcomes-secivdv"><i class="fa fa-check"></i><b>1.6.4</b> The “role” of variables: predictors and outcomes [sec:ivdv]</a></li>
<li class="chapter" data-level="1.6.5" data-path="why-statistics.html"><a href="why-statistics.html#experimental-and-non-experimental-researchsecresearchdesigns"><i class="fa fa-check"></i><b>1.6.5</b> Experimental and non-experimental research [sec:researchdesigns]</a></li>
<li class="chapter" data-level="1.6.6" data-path="why-statistics.html"><a href="why-statistics.html#assessing-the-validity-of-a-studysecvalidity"><i class="fa fa-check"></i><b>1.6.6</b> Assessing the validity of a study [sec:validity]</a></li>
<li class="chapter" data-level="1.6.7" data-path="why-statistics.html"><a href="why-statistics.html#confounds-artifacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>1.6.7</b> Confounds, artifacts and other threats to validity</a></li>
<li class="chapter" data-level="1.6.8" data-path="why-statistics.html"><a href="why-statistics.html#summary"><i class="fa fa-check"></i><b>1.6.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="why-statistics.html"><a href="why-statistics.html#basicExampleOfStentsAndStrokes"><i class="fa fa-check"></i><b>1.7</b> Case study: using stents to prevent strokes</a></li>
<li class="chapter" data-level="1.8" data-path="why-statistics.html"><a href="why-statistics.html#dataBasics"><i class="fa fa-check"></i><b>1.8</b> Data basics</a><ul>
<li class="chapter" data-level="1.8.1" data-path="why-statistics.html"><a href="why-statistics.html#observations-variables-and-data-matrices"><i class="fa fa-check"></i><b>1.8.1</b> Observations, variables, and data matrices</a></li>
<li class="chapter" data-level="1.8.2" data-path="why-statistics.html"><a href="why-statistics.html#variableTypes"><i class="fa fa-check"></i><b>1.8.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.8.3" data-path="why-statistics.html"><a href="why-statistics.html#variableRelations"><i class="fa fa-check"></i><b>1.8.3</b> Relationships between variables</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="why-statistics.html"><a href="why-statistics.html#overviewOfDataCollectionPrinciples"><i class="fa fa-check"></i><b>1.9</b> Overview of data collection principles</a><ul>
<li class="chapter" data-level="1.9.1" data-path="why-statistics.html"><a href="why-statistics.html#populationsAndSamples"><i class="fa fa-check"></i><b>1.9.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.9.2" data-path="why-statistics.html"><a href="why-statistics.html#anecdotalEvidenceSubsection"><i class="fa fa-check"></i><b>1.9.2</b> Anecdotal evidence</a></li>
<li class="chapter" data-level="1.9.3" data-path="why-statistics.html"><a href="why-statistics.html#sampling-from-a-population"><i class="fa fa-check"></i><b>1.9.3</b> Sampling from a population</a></li>
<li class="chapter" data-level="1.9.4" data-path="why-statistics.html"><a href="why-statistics.html#explanatoryAndResponse"><i class="fa fa-check"></i><b>1.9.4</b> Explanatory and response variables</a></li>
<li class="chapter" data-level="1.9.5" data-path="why-statistics.html"><a href="why-statistics.html#introducing-observational-studies-and-experiments"><i class="fa fa-check"></i><b>1.9.5</b> Introducing observational studies and experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="why-statistics.html"><a href="why-statistics.html#observational-studies-and-sampling-strategies"><i class="fa fa-check"></i><b>1.10</b> Observational studies and sampling strategies</a><ul>
<li class="chapter" data-level="1.10.1" data-path="why-statistics.html"><a href="why-statistics.html#observational-studies"><i class="fa fa-check"></i><b>1.10.1</b> Observational studies</a></li>
<li class="chapter" data-level="1.10.2" data-path="why-statistics.html"><a href="why-statistics.html#threeSamplingMethods"><i class="fa fa-check"></i><b>1.10.2</b> Three sampling methods (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="why-statistics.html"><a href="why-statistics.html#experimentsSection"><i class="fa fa-check"></i><b>1.11</b> Experiments</a><ul>
<li class="chapter" data-level="1.11.1" data-path="why-statistics.html"><a href="why-statistics.html#experimentalDesignPrinciples"><i class="fa fa-check"></i><b>1.11.1</b> Principles of experimental design</a></li>
<li class="chapter" data-level="1.11.2" data-path="why-statistics.html"><a href="why-statistics.html#biasInHumanExperiments"><i class="fa fa-check"></i><b>1.11.2</b> Reducing bias in human experiments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DescribingData.html"><a href="DescribingData.html"><i class="fa fa-check"></i><b>2</b> Describing Data</a><ul>
<li class="chapter" data-level="2.1" data-path="DescribingData.html"><a href="DescribingData.html#this-is-what-too-many-numbers-looks-like"><i class="fa fa-check"></i><b>2.1</b> This is what too many numbers looks like</a></li>
<li class="chapter" data-level="2.2" data-path="DescribingData.html"><a href="DescribingData.html#look-at-the-data"><i class="fa fa-check"></i><b>2.2</b> Look at the data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DescribingData.html"><a href="DescribingData.html#stop-plotting-time-o-o-oh-u-can-plot-this"><i class="fa fa-check"></i><b>2.2.1</b> Stop, plotting time (o o oh) U can plot this</a></li>
<li class="chapter" data-level="2.2.2" data-path="DescribingData.html"><a href="DescribingData.html#histograms"><i class="fa fa-check"></i><b>2.2.2</b> Histograms</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DescribingData.html"><a href="DescribingData.html#important-ideas-distribution-central-tendency-and-variance"><i class="fa fa-check"></i><b>2.3</b> Important Ideas: Distribution, Central Tendency, and Variance</a></li>
<li class="chapter" data-level="2.4" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-central-tendency-sameness"><i class="fa fa-check"></i><b>2.4</b> Measures of Central Tendency (Sameness)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="DescribingData.html"><a href="DescribingData.html#from-many-numbers-to-one"><i class="fa fa-check"></i><b>2.4.1</b> From many numbers to one</a></li>
<li class="chapter" data-level="2.4.2" data-path="DescribingData.html"><a href="DescribingData.html#mode"><i class="fa fa-check"></i><b>2.4.2</b> Mode</a></li>
<li class="chapter" data-level="2.4.3" data-path="DescribingData.html"><a href="DescribingData.html#median"><i class="fa fa-check"></i><b>2.4.3</b> Median</a></li>
<li class="chapter" data-level="2.4.4" data-path="DescribingData.html"><a href="DescribingData.html#mean"><i class="fa fa-check"></i><b>2.4.4</b> Mean</a></li>
<li class="chapter" data-level="2.4.5" data-path="DescribingData.html"><a href="DescribingData.html#what-does-the-mean-mean"><i class="fa fa-check"></i><b>2.4.5</b> What does the mean mean?</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-variation-differentness"><i class="fa fa-check"></i><b>2.5</b> Measures of Variation (Differentness)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="DescribingData.html"><a href="DescribingData.html#the-range"><i class="fa fa-check"></i><b>2.5.1</b> The Range</a></li>
<li class="chapter" data-level="2.5.2" data-path="DescribingData.html"><a href="DescribingData.html#the-difference-scores"><i class="fa fa-check"></i><b>2.5.2</b> The Difference Scores</a></li>
<li class="chapter" data-level="2.5.3" data-path="DescribingData.html"><a href="DescribingData.html#the-variance"><i class="fa fa-check"></i><b>2.5.3</b> The Variance</a></li>
<li class="chapter" data-level="2.5.4" data-path="DescribingData.html"><a href="DescribingData.html#the-standard-deviation"><i class="fa fa-check"></i><b>2.5.4</b> The Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="DescribingData.html"><a href="DescribingData.html#using-descriptive-statistics-with-data"><i class="fa fa-check"></i><b>2.6</b> Using Descriptive Statistics with data</a></li>
<li class="chapter" data-level="2.7" data-path="DescribingData.html"><a href="DescribingData.html#rolling-your-own-descriptive-statistics"><i class="fa fa-check"></i><b>2.7</b> Rolling your own descriptive statistics</a><ul>
<li class="chapter" data-level="2.7.1" data-path="DescribingData.html"><a href="DescribingData.html#absolute-deviations"><i class="fa fa-check"></i><b>2.7.1</b> Absolute deviations</a></li>
<li class="chapter" data-level="2.7.2" data-path="DescribingData.html"><a href="DescribingData.html#other-sign-inverting-operations"><i class="fa fa-check"></i><b>2.7.2</b> Other sign-inverting operations</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="DescribingData.html"><a href="DescribingData.html#numericalData"><i class="fa fa-check"></i><b>2.8</b> Examining numerical data</a><ul>
<li class="chapter" data-level="2.8.1" data-path="DescribingData.html"><a href="DescribingData.html#scatterPlots"><i class="fa fa-check"></i><b>2.8.1</b> Scatterplots for paired data</a></li>
<li class="chapter" data-level="2.8.2" data-path="DescribingData.html"><a href="DescribingData.html#dotPlot"><i class="fa fa-check"></i><b>2.8.2</b> Dot plots and the mean</a></li>
<li class="chapter" data-level="2.8.3" data-path="DescribingData.html"><a href="DescribingData.html#histogramsAndShape"><i class="fa fa-check"></i><b>2.8.3</b> Histograms and shape</a></li>
<li class="chapter" data-level="2.8.4" data-path="DescribingData.html"><a href="DescribingData.html#variability"><i class="fa fa-check"></i><b>2.8.4</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="2.8.5" data-path="DescribingData.html"><a href="DescribingData.html#box-plots-quartiles-and-the-median"><i class="fa fa-check"></i><b>2.8.5</b> Box plots, quartiles, and the median</a></li>
<li class="chapter" data-level="2.8.6" data-path="DescribingData.html"><a href="DescribingData.html#robust-statistics"><i class="fa fa-check"></i><b>2.8.6</b> Robust statistics</a></li>
<li class="chapter" data-level="2.8.7" data-path="DescribingData.html"><a href="DescribingData.html#transformingDataSubsection"><i class="fa fa-check"></i><b>2.8.7</b> Transforming data (special topic)</a></li>
<li class="chapter" data-level="2.8.8" data-path="DescribingData.html"><a href="DescribingData.html#mapping-data-special-topic"><i class="fa fa-check"></i><b>2.8.8</b> Mapping data (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="DescribingData.html"><a href="DescribingData.html#categoricalData"><i class="fa fa-check"></i><b>2.9</b> Considering categorical data</a><ul>
<li class="chapter" data-level="2.9.1" data-path="DescribingData.html"><a href="DescribingData.html#contingency-tables-and-bar-plots"><i class="fa fa-check"></i><b>2.9.1</b> Contingency tables and bar plots</a></li>
<li class="chapter" data-level="2.9.2" data-path="DescribingData.html"><a href="DescribingData.html#row-and-column-proportions"><i class="fa fa-check"></i><b>2.9.2</b> Row and column proportions</a></li>
<li class="chapter" data-level="2.9.3" data-path="DescribingData.html"><a href="DescribingData.html#segmentedBarPlotsAndIndependence"><i class="fa fa-check"></i><b>2.9.3</b> Segmented bar and mosaic plots</a></li>
<li class="chapter" data-level="2.9.4" data-path="DescribingData.html"><a href="DescribingData.html#the-only-pie-chart-you-will-see-in-this-book"><i class="fa fa-check"></i><b>2.9.4</b> The only pie chart you will see in this book</a></li>
<li class="chapter" data-level="2.9.5" data-path="DescribingData.html"><a href="DescribingData.html#comparingAcrossGroups"><i class="fa fa-check"></i><b>2.9.5</b> Comparing numerical data across groups</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Correlation.html"><a href="Correlation.html"><i class="fa fa-check"></i><b>3</b> Correlation</a><ul>
<li class="chapter" data-level="3.1" data-path="Correlation.html"><a href="Correlation.html#if-something-caused-something-else-to-change-what-would-that-look-like"><i class="fa fa-check"></i><b>3.1</b> If something caused something else to change, what would that look like?</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Correlation.html"><a href="Correlation.html#charlie-and-the-chocolate-factory"><i class="fa fa-check"></i><b>3.1.1</b> Charlie and the Chocolate factory</a></li>
<li class="chapter" data-level="3.1.2" data-path="Correlation.html"><a href="Correlation.html#scatterplots"><i class="fa fa-check"></i><b>3.1.2</b> Scatterplots</a></li>
<li class="chapter" data-level="3.1.3" data-path="Correlation.html"><a href="Correlation.html#positive-negative-and-no-correlation"><i class="fa fa-check"></i><b>3.1.3</b> Positive, Negative, and No-Correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Correlation.html"><a href="Correlation.html#pearsons-r"><i class="fa fa-check"></i><b>3.2</b> Pearson’s r</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Correlation.html"><a href="Correlation.html#the-idea-of-co-variance"><i class="fa fa-check"></i><b>3.2.1</b> The idea of co-variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Correlation.html"><a href="Correlation.html#turning-the-numbers-into-a-measure-of-co-variance"><i class="fa fa-check"></i><b>3.3</b> Turning the numbers into a measure of co-variance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Correlation.html"><a href="Correlation.html#co-variance-the-measure"><i class="fa fa-check"></i><b>3.3.1</b> Co-variance, the measure</a></li>
<li class="chapter" data-level="3.3.2" data-path="Correlation.html"><a href="Correlation.html#pearsons-r-we-there-yet"><i class="fa fa-check"></i><b>3.3.2</b> Pearson’s r we there yet</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Correlation.html"><a href="Correlation.html#examples-with-data"><i class="fa fa-check"></i><b>3.4</b> Examples with Data</a></li>
<li class="chapter" data-level="3.5" data-path="Correlation.html"><a href="Correlation.html#regression-a-mini-intro"><i class="fa fa-check"></i><b>3.5</b> Regression: A mini intro</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Correlation.html"><a href="Correlation.html#the-best-fit-line"><i class="fa fa-check"></i><b>3.5.1</b> The best fit line</a></li>
<li class="chapter" data-level="3.5.2" data-path="Correlation.html"><a href="Correlation.html#lines"><i class="fa fa-check"></i><b>3.5.2</b> Lines</a></li>
<li class="chapter" data-level="3.5.3" data-path="Correlation.html"><a href="Correlation.html#computing-the-best-fit-line"><i class="fa fa-check"></i><b>3.5.3</b> Computing the best fit line</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="Correlation.html"><a href="Correlation.html#interpreting-correlations"><i class="fa fa-check"></i><b>3.6</b> Interpreting Correlations</a><ul>
<li class="chapter" data-level="3.6.1" data-path="Correlation.html"><a href="Correlation.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>3.6.1</b> Correlation does not equal causation</a></li>
<li class="chapter" data-level="3.6.2" data-path="Correlation.html"><a href="Correlation.html#correlation-and-random-chance"><i class="fa fa-check"></i><b>3.6.2</b> Correlation and Random chance</a></li>
<li class="chapter" data-level="3.6.3" data-path="Correlation.html"><a href="Correlation.html#some-more-movies"><i class="fa fa-check"></i><b>3.6.3</b> Some more movies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html"><i class="fa fa-check"></i><b>4</b> Sampling and estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#samples-populations-and-sampling"><i class="fa fa-check"></i><b>4.1</b> Samples, populations and sampling</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#defining-a-population"><i class="fa fa-check"></i><b>4.1.1</b> Defining a population</a></li>
<li class="chapter" data-level="4.1.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#simple-random-samples"><i class="fa fa-check"></i><b>4.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="4.1.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>4.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="4.1.4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>4.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="4.1.5" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>4.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#the-law-of-large-numbers"><i class="fa fa-check"></i><b>4.2</b> The law of large numbers</a></li>
<li class="chapter" data-level="4.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#sampling-distributions-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>4.3</b> Sampling distributions and the central limit theorem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#sampling-distribution-of-the-sample-means"><i class="fa fa-check"></i><b>4.3.1</b> Sampling distribution of the sample means</a></li>
<li class="chapter" data-level="4.3.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#sampling-distributions-exist-for-any-sample-statistic"><i class="fa fa-check"></i><b>4.3.2</b> Sampling distributions exist for any sample statistic!</a></li>
<li class="chapter" data-level="4.3.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>4.3.3</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#estimating-population-parameters"><i class="fa fa-check"></i><b>4.4</b> Estimating population parameters</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#concrete-popopulation-parameters"><i class="fa fa-check"></i><b>4.4.1</b> Concrete popopulation parameters</a></li>
<li class="chapter" data-level="4.4.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#abstract-population-parameters"><i class="fa fa-check"></i><b>4.4.2</b> Abstract population parameters</a></li>
<li class="chapter" data-level="4.4.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#experiments-and-population-parameters"><i class="fa fa-check"></i><b>4.4.3</b> Experiments and Population parameters</a></li>
<li class="chapter" data-level="4.4.4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#interim-summary"><i class="fa fa-check"></i><b>4.4.4</b> Interim summary</a></li>
<li class="chapter" data-level="4.4.5" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>4.4.5</b> Estimating the population mean</a></li>
<li class="chapter" data-level="4.4.6" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#estimating-the-population-standard-deviation"><i class="fa fa-check"></i><b>4.4.6</b> Estimating the population standard deviation</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#estimating-a-confidence-intervalsecci"><i class="fa fa-check"></i><b>4.5</b> Estimating a confidence interval[sec:ci]</a><ul>
<li class="chapter" data-level="4.5.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#a-slight-mistake-in-the-formula"><i class="fa fa-check"></i><b>4.5.1</b> A slight mistake in the formula</a></li>
<li class="chapter" data-level="4.5.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#looking-at-confidence-intervals"><i class="fa fa-check"></i><b>4.5.2</b> Looking at confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#summary-1"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html"><i class="fa fa-check"></i><b>5</b> Foundations for inference</a><ul>
<li class="chapter" data-level="5.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#brief-review-of-experiments"><i class="fa fa-check"></i><b>5.1</b> Brief review of Experiments</a></li>
<li class="chapter" data-level="5.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#the-data-came-from-a-distribution"><i class="fa fa-check"></i><b>5.2</b> The data came from a distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#uniform-distribution"><i class="fa fa-check"></i><b>5.2.1</b> Uniform distribution</a></li>
<li class="chapter" data-level="5.2.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#not-all-samples-are-the-same-they-are-usually-quite-different"><i class="fa fa-check"></i><b>5.2.2</b> Not all samples are the same, they are usually quite different</a></li>
<li class="chapter" data-level="5.2.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#large-samples-are-more-like-the-distribution-they-came-from"><i class="fa fa-check"></i><b>5.2.3</b> Large samples are more like the distribution they came from</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#playing-with-distributions"><i class="fa fa-check"></i><b>5.3</b> Playing with distributions</a></li>
<li class="chapter" data-level="5.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#is-there-a-difference"><i class="fa fa-check"></i><b>5.4</b> Is there a difference?</a><ul>
<li class="chapter" data-level="5.4.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#chance-can-produce-differences"><i class="fa fa-check"></i><b>5.4.1</b> Chance can produce differences</a></li>
<li class="chapter" data-level="5.4.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#differences-due-to-chance-can-be-simulated"><i class="fa fa-check"></i><b>5.4.2</b> Differences due to chance can be simulated</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#chance-makes-some-differences-more-likely-than-others"><i class="fa fa-check"></i><b>5.5</b> Chance makes some differences more likely than others</a></li>
<li class="chapter" data-level="5.6" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#the-crump-test"><i class="fa fa-check"></i><b>5.6</b> The Crump Test</a><ul>
<li class="chapter" data-level="5.6.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#intuitive-methods"><i class="fa fa-check"></i><b>5.6.1</b> Intuitive methods</a></li>
<li class="chapter" data-level="5.6.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#part-1-frequency-based-intuition-about-occurence"><i class="fa fa-check"></i><b>5.6.2</b> Part 1: Frequency based intuition about occurence</a></li>
<li class="chapter" data-level="5.6.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#part-2-simulating-chance"><i class="fa fa-check"></i><b>5.6.3</b> Part 2: Simulating chance</a></li>
<li class="chapter" data-level="5.6.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#part-3-judgment-and-decision-making"><i class="fa fa-check"></i><b>5.6.4</b> Part 3: Judgment and Decision-making</a></li>
<li class="chapter" data-level="5.6.5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#part-4-experiment-design"><i class="fa fa-check"></i><b>5.6.5</b> Part 4: Experiment Design</a></li>
<li class="chapter" data-level="5.6.6" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#part-5-i-have-the-power"><i class="fa fa-check"></i><b>5.6.6</b> Part 5: I have the power</a></li>
<li class="chapter" data-level="5.6.7" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#summary-of-crump-test"><i class="fa fa-check"></i><b>5.6.7</b> Summary of Crump Test</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#the-randomization-test-permutation-test"><i class="fa fa-check"></i><b>5.7</b> The randomization test (permutation test)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#pretend-example-does-chewing-gum-improve-your-grades"><i class="fa fa-check"></i><b>5.7.1</b> Pretend example does chewing gum improve your grades?</a></li>
<li class="chapter" data-level="5.7.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#take-homes-so-far"><i class="fa fa-check"></i><b>5.7.2</b> Take homes so far</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#caseStudyGenderDiscrimination"><i class="fa fa-check"></i><b>5.8</b> Randomization case study: gender discrimination</a><ul>
<li class="chapter" data-level="5.8.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#variabilityWithinData"><i class="fa fa-check"></i><b>5.8.1</b> Variability within data</a></li>
<li class="chapter" data-level="5.8.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#simulatingTheStudy"><i class="fa fa-check"></i><b>5.8.2</b> Simulating the study</a></li>
<li class="chapter" data-level="5.8.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#checking-for-independence"><i class="fa fa-check"></i><b>5.8.3</b> Checking for independence</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#caseStudyOpportunityCost"><i class="fa fa-check"></i><b>5.9</b> Randomization case study: opportunity cost</a><ul>
<li class="chapter" data-level="5.9.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#exploring-the-data-set-before-the-analysis"><i class="fa fa-check"></i><b>5.9.1</b> Exploring the data set before the analysis</a></li>
<li class="chapter" data-level="5.9.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#results-from-chance-alone"><i class="fa fa-check"></i><b>5.9.2</b> Results from chance alone</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#HypothesisTesting"><i class="fa fa-check"></i><b>5.10</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="5.10.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#hypothesis-testing-in-the-us-court-system"><i class="fa fa-check"></i><b>5.10.1</b> Hypothesis testing in the US court system</a></li>
<li class="chapter" data-level="5.10.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#p-value-and-statistical-significance"><i class="fa fa-check"></i><b>5.10.2</b> p-value and statistical significance</a></li>
<li class="chapter" data-level="5.10.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#decision-errors"><i class="fa fa-check"></i><b>5.10.3</b> Decision errors</a></li>
<li class="chapter" data-level="5.10.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#significanceLevel"><i class="fa fa-check"></i><b>5.10.4</b> Choosing a significance level</a></li>
<li class="chapter" data-level="5.10.5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#IntroducingTwoSidedHypotheses"><i class="fa fa-check"></i><b>5.10.5</b> Introducing two-sided hypotheses</a></li>
<li class="chapter" data-level="5.10.6" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#InflatingType1ErrorRate"><i class="fa fa-check"></i><b>5.10.6</b> Controlling the Type 1 Error rate</a></li>
<li class="chapter" data-level="5.10.7" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#how-to-use-a-hypothesis-test"><i class="fa fa-check"></i><b>5.10.7</b> How to use a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="5.11" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#SimulationCaseStudies"><i class="fa fa-check"></i><b>5.11</b> Simulation case studies</a><ul>
<li class="chapter" data-level="5.11.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#medical-consultant"><i class="fa fa-check"></i><b>5.11.1</b> Medical consultant</a></li>
<li class="chapter" data-level="5.11.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#tappers-and-listeners"><i class="fa fa-check"></i><b>5.11.2</b> Tappers and listeners</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#CLTsection"><i class="fa fa-check"></i><b>5.12</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="5.12.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#null-distribution-from-the-case-studies"><i class="fa fa-check"></i><b>5.12.1</b> Null distribution from the case studies</a></li>
<li class="chapter" data-level="5.12.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#examples-of-future-settings-we-will-consider"><i class="fa fa-check"></i><b>5.12.2</b> Examples of future settings we will consider</a></li>
</ul></li>
<li class="chapter" data-level="5.13" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#normalDist"><i class="fa fa-check"></i><b>5.13</b> Normal distribution</a><ul>
<li class="chapter" data-level="5.13.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#NormalDistributionModelSubsection"><i class="fa fa-check"></i><b>5.13.1</b> Normal distribution model</a></li>
<li class="chapter" data-level="5.13.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#standardizing-with-z-scores"><i class="fa fa-check"></i><b>5.13.2</b> Standardizing with Z scores</a></li>
<li class="chapter" data-level="5.13.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#normal-probability-table"><i class="fa fa-check"></i><b>5.13.3</b> Normal probability table</a></li>
<li class="chapter" data-level="5.13.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#normal-probability-examples"><i class="fa fa-check"></i><b>5.13.4</b> Normal probability examples</a></li>
<li class="chapter" data-level="5.13.5" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#rule"><i class="fa fa-check"></i><b>5.13.5</b> 68-95-99.7 rule</a></li>
<li class="chapter" data-level="5.13.6" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#assessingNormal"><i class="fa fa-check"></i><b>5.13.6</b> Evaluating the normal approximation</a></li>
</ul></li>
<li class="chapter" data-level="5.14" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#ApplyingTheNormalModel"><i class="fa fa-check"></i><b>5.14</b> Applying the normal model</a><ul>
<li class="chapter" data-level="5.14.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#standard-error"><i class="fa fa-check"></i><b>5.14.1</b> Standard error</a></li>
<li class="chapter" data-level="5.14.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#normal-model-application-opportunity-cost"><i class="fa fa-check"></i><b>5.14.2</b> Normal model application: opportunity cost</a></li>
<li class="chapter" data-level="5.14.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#normal-model-application-medical-consultant"><i class="fa fa-check"></i><b>5.14.3</b> Normal model application: medical consultant</a></li>
<li class="chapter" data-level="5.14.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#conditions-for-applying-the-normal-model"><i class="fa fa-check"></i><b>5.14.4</b> Conditions for applying the normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.15" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#ConfidenceIntervals"><i class="fa fa-check"></i><b>5.15</b> Confidence intervals</a><ul>
<li class="chapter" data-level="5.15.1" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#capturing-the-population-parameter"><i class="fa fa-check"></i><b>5.15.1</b> Capturing the population parameter</a></li>
<li class="chapter" data-level="5.15.2" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#constructing-a-95-confidence-interval"><i class="fa fa-check"></i><b>5.15.2</b> Constructing a 95% confidence interval</a></li>
<li class="chapter" data-level="5.15.3" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#changingTheConfidenceLevelSection"><i class="fa fa-check"></i><b>5.15.3</b> Changing the confidence level</a></li>
<li class="chapter" data-level="5.15.4" data-path="foundations-for-inference.html"><a href="foundations-for-inference.html#interpretingCIs"><i class="fa fa-check"></i><b>5.15.4</b> Interpreting confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html"><i class="fa fa-check"></i><b>6</b> t-Tests</a><ul>
<li class="chapter" data-level="6.1" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#check-your-confidence-in-your-mean"><i class="fa fa-check"></i><b>6.1</b> Check your confidence in your mean</a></li>
<li class="chapter" data-level="6.2" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#one-sample-t-test"><i class="fa fa-check"></i><b>6.2</b> One-sample t-test</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#formulas-for-one-sample-t-test"><i class="fa fa-check"></i><b>6.2.1</b> Formulas for one-sample t-test</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#what-does-t-represent"><i class="fa fa-check"></i><b>6.2.2</b> What does t represent?</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#calculating-t-from-data"><i class="fa fa-check"></i><b>6.2.3</b> Calculating t from data</a></li>
<li class="chapter" data-level="6.2.4" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#how-does-t-behave"><i class="fa fa-check"></i><b>6.2.4</b> How does <span class="math inline">\(t\)</span> behave?</a></li>
<li class="chapter" data-level="6.2.5" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#making-a-decision"><i class="fa fa-check"></i><b>6.2.5</b> Making a decision</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html#paired-samples-t-test"><i class="fa fa-check"></i><b>6.3</b> Paired-samples <span class="math inline">\(t\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html"><i class="fa fa-check"></i><b>7</b> ANOVA</a><ul>
<li class="chapter" data-level="7.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#anovaAndRegrWithCategoricalVariables"><i class="fa fa-check"></i><b>7.1</b> Comparing many means with ANOVA</a><ul>
<li class="chapter" data-level="7.1.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#is-batting-performance-related-to-player-position-in-mlb"><i class="fa fa-check"></i><b>7.1.1</b> Is batting performance related to player position in MLB?</a></li>
<li class="chapter" data-level="7.1.2" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#analysis-of-variance-anova-and-the-f-test"><i class="fa fa-check"></i><b>7.1.2</b> Analysis of variance (ANOVA) and the F test</a></li>
<li class="chapter" data-level="7.1.3" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#reading-an-anova-table-from-software"><i class="fa fa-check"></i><b>7.1.3</b> Reading an ANOVA table from software</a></li>
<li class="chapter" data-level="7.1.4" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#graphical-diagnostics-for-an-anova-analysis"><i class="fa fa-check"></i><b>7.1.4</b> Graphical diagnostics for an ANOVA analysis</a></li>
<li class="chapter" data-level="7.1.5" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#multipleComparisonsAndControllingTheType1ErrorRate"><i class="fa fa-check"></i><b>7.1.5</b> Multiple comparisons and controlling Type 1 Error rate</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#bootstrapping-to-study-the-standard-deviation"><i class="fa fa-check"></i><b>7.2</b> Bootstrapping to study the standard deviation</a><ul>
<li class="chapter" data-level="7.2.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#bootstrap-samples-and-distributions"><i class="fa fa-check"></i><b>7.2.1</b> Bootstrap samples and distributions</a></li>
<li class="chapter" data-level="7.2.2" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#inference-using-the-bootstrap"><i class="fa fa-check"></i><b>7.2.2</b> Inference using the bootstrap</a></li>
<li class="chapter" data-level="7.2.3" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#frequently-asked-questions"><i class="fa fa-check"></i><b>7.2.3</b> Frequently asked questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>8</b> Repeated Measures ANOVA</a><ul>
<li class="chapter" data-level="8.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#anovaAndRegrWithCategoricalVariables"><i class="fa fa-check"></i><b>8.1</b> Comparing many means with ANOVA</a><ul>
<li class="chapter" data-level="8.1.1" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#is-batting-performance-related-to-player-position-in-mlb-1"><i class="fa fa-check"></i><b>8.1.1</b> Is batting performance related to player position in MLB?</a></li>
<li class="chapter" data-level="8.1.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#analysis-of-variance-anova-and-the-f-test-1"><i class="fa fa-check"></i><b>8.1.2</b> Analysis of variance (ANOVA) and the F test</a></li>
<li class="chapter" data-level="8.1.3" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#reading-an-anova-table-from-software-1"><i class="fa fa-check"></i><b>8.1.3</b> Reading an ANOVA table from software</a></li>
<li class="chapter" data-level="8.1.4" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#graphical-diagnostics-for-an-anova-analysis-1"><i class="fa fa-check"></i><b>8.1.4</b> Graphical diagnostics for an ANOVA analysis</a></li>
<li class="chapter" data-level="8.1.5" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#multipleComparisonsAndControllingTheType1ErrorRate"><i class="fa fa-check"></i><b>8.1.5</b> Multiple comparisons and controlling Type 1 Error rate</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#bootstrapping-to-study-the-standard-deviation-1"><i class="fa fa-check"></i><b>8.2</b> Bootstrapping to study the standard deviation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#bootstrap-samples-and-distributions-1"><i class="fa fa-check"></i><b>8.2.1</b> Bootstrap samples and distributions</a></li>
<li class="chapter" data-level="8.2.2" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#inference-using-the-bootstrap-1"><i class="fa fa-check"></i><b>8.2.2</b> Inference using the bootstrap</a></li>
<li class="chapter" data-level="8.2.3" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html#frequently-asked-questions-1"><i class="fa fa-check"></i><b>8.2.3</b> Frequently asked questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="factorial-anova.html"><a href="factorial-anova.html"><i class="fa fa-check"></i><b>9</b> Factorial ANOVA</a><ul>
<li class="chapter" data-level="9.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#anovaAndRegrWithCategoricalVariables"><i class="fa fa-check"></i><b>9.1</b> Comparing many means with ANOVA</a><ul>
<li class="chapter" data-level="9.1.1" data-path="factorial-anova.html"><a href="factorial-anova.html#is-batting-performance-related-to-player-position-in-mlb-2"><i class="fa fa-check"></i><b>9.1.1</b> Is batting performance related to player position in MLB?</a></li>
<li class="chapter" data-level="9.1.2" data-path="factorial-anova.html"><a href="factorial-anova.html#analysis-of-variance-anova-and-the-f-test-2"><i class="fa fa-check"></i><b>9.1.2</b> Analysis of variance (ANOVA) and the F test</a></li>
<li class="chapter" data-level="9.1.3" data-path="factorial-anova.html"><a href="factorial-anova.html#reading-an-anova-table-from-software-2"><i class="fa fa-check"></i><b>9.1.3</b> Reading an ANOVA table from software</a></li>
<li class="chapter" data-level="9.1.4" data-path="factorial-anova.html"><a href="factorial-anova.html#graphical-diagnostics-for-an-anova-analysis-2"><i class="fa fa-check"></i><b>9.1.4</b> Graphical diagnostics for an ANOVA analysis</a></li>
<li class="chapter" data-level="9.1.5" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#multipleComparisonsAndControllingTheType1ErrorRate"><i class="fa fa-check"></i><b>9.1.5</b> Multiple comparisons and controlling Type 1 Error rate</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="factorial-anova.html"><a href="factorial-anova.html#bootstrapping-to-study-the-standard-deviation-2"><i class="fa fa-check"></i><b>9.2</b> Bootstrapping to study the standard deviation</a><ul>
<li class="chapter" data-level="9.2.1" data-path="factorial-anova.html"><a href="factorial-anova.html#bootstrap-samples-and-distributions-2"><i class="fa fa-check"></i><b>9.2.1</b> Bootstrap samples and distributions</a></li>
<li class="chapter" data-level="9.2.2" data-path="factorial-anova.html"><a href="factorial-anova.html#inference-using-the-bootstrap-2"><i class="fa fa-check"></i><b>9.2.2</b> Inference using the bootstrap</a></li>
<li class="chapter" data-level="9.2.3" data-path="factorial-anova.html"><a href="factorial-anova.html#frequently-asked-questions-2"><i class="fa fa-check"></i><b>9.2.3</b> Frequently asked questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html"><i class="fa fa-check"></i><b>10</b> Mixed Design ANOVA</a><ul>
<li class="chapter" data-level="10.1" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#anovaAndRegrWithCategoricalVariables"><i class="fa fa-check"></i><b>10.1</b> Comparing many means with ANOVA</a><ul>
<li class="chapter" data-level="10.1.1" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#is-batting-performance-related-to-player-position-in-mlb-3"><i class="fa fa-check"></i><b>10.1.1</b> Is batting performance related to player position in MLB?</a></li>
<li class="chapter" data-level="10.1.2" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#analysis-of-variance-anova-and-the-f-test-3"><i class="fa fa-check"></i><b>10.1.2</b> Analysis of variance (ANOVA) and the F test</a></li>
<li class="chapter" data-level="10.1.3" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#reading-an-anova-table-from-software-3"><i class="fa fa-check"></i><b>10.1.3</b> Reading an ANOVA table from software</a></li>
<li class="chapter" data-level="10.1.4" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#graphical-diagnostics-for-an-anova-analysis-3"><i class="fa fa-check"></i><b>10.1.4</b> Graphical diagnostics for an ANOVA analysis</a></li>
<li class="chapter" data-level="10.1.5" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html#multipleComparisonsAndControllingTheType1ErrorRate"><i class="fa fa-check"></i><b>10.1.5</b> Multiple comparisons and controlling Type 1 Error rate</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#bootstrapping-to-study-the-standard-deviation-3"><i class="fa fa-check"></i><b>10.2</b> Bootstrapping to study the standard deviation</a><ul>
<li class="chapter" data-level="10.2.1" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#bootstrap-samples-and-distributions-3"><i class="fa fa-check"></i><b>10.2.1</b> Bootstrap samples and distributions</a></li>
<li class="chapter" data-level="10.2.2" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#inference-using-the-bootstrap-3"><i class="fa fa-check"></i><b>10.2.2</b> Inference using the bootstrap</a></li>
<li class="chapter" data-level="10.2.3" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html#frequently-asked-questions-3"><i class="fa fa-check"></i><b>10.2.3</b> Frequently asked questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html"><i class="fa fa-check"></i><b>11</b> Introduction to linear regression</a><ul>
<li class="chapter" data-level="11.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#lineFittingResidualsCorrelation"><i class="fa fa-check"></i><b>11.1</b> Line fitting, residuals, and correlation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#beginning-with-straight-lines"><i class="fa fa-check"></i><b>11.1.1</b> Beginning with straight lines</a></li>
<li class="chapter" data-level="11.1.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#fitting-a-line-by-eye"><i class="fa fa-check"></i><b>11.1.2</b> Fitting a line by eye</a></li>
<li class="chapter" data-level="11.1.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#residuals"><i class="fa fa-check"></i><b>11.1.3</b> Residuals</a></li>
<li class="chapter" data-level="11.1.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#describing-linear-relationships-with-correlation"><i class="fa fa-check"></i><b>11.1.4</b> Describing linear relationships with correlation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#fittingALineByLSR"><i class="fa fa-check"></i><b>11.2</b> Fitting a line by least squares regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#an-objective-measure-for-finding-the-best-line"><i class="fa fa-check"></i><b>11.2.1</b> An objective measure for finding the best line</a></li>
<li class="chapter" data-level="11.2.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#findingTheLeastSquaresLineSection"><i class="fa fa-check"></i><b>11.2.2</b> Finding the least squares line</a></li>
<li class="chapter" data-level="11.2.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#interpreting-regression-line-parameter-estimates"><i class="fa fa-check"></i><b>11.2.3</b> Interpreting regression line parameter estimates</a></li>
<li class="chapter" data-level="11.2.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#extrapolation-is-treacherous"><i class="fa fa-check"></i><b>11.2.4</b> Extrapolation is treacherous</a></li>
<li class="chapter" data-level="11.2.5" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#using-r2-to-describe-the-strength-of-a-fit"><i class="fa fa-check"></i><b>11.2.5</b> Using <span class="math inline">\(R^2\)</span> to describe the strength of a fit</a></li>
<li class="chapter" data-level="11.2.6" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#categoricalPredictorsWithTwoLevels"><i class="fa fa-check"></i><b>11.2.6</b> Categorical predictors with two levels</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#typesOfOutliersInLinearRegression"><i class="fa fa-check"></i><b>11.3</b> Types of outliers in linear regression</a></li>
<li class="chapter" data-level="11.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#inferenceForLinearRegression"><i class="fa fa-check"></i><b>11.4</b> Inference for linear regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#conditions-for-the-least-squares-line"><i class="fa fa-check"></i><b>11.4.1</b> Conditions for the least squares line</a></li>
<li class="chapter" data-level="11.4.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#midterm-elections-and-unemployment"><i class="fa fa-check"></i><b>11.4.2</b> Midterm elections and unemployment</a></li>
<li class="chapter" data-level="11.4.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#testStatisticForTheSlope"><i class="fa fa-check"></i><b>11.4.3</b> Understanding regression output from software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html"><i class="fa fa-check"></i><b>12</b> Multiple and logistic regression</a><ul>
<li class="chapter" data-level="12.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#introductionToMultipleRegression"><i class="fa fa-check"></i><b>12.1</b> Introduction to multiple regression</a><ul>
<li class="chapter" data-level="12.1.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#twoSingleVariableModelsForMarioKartData"><i class="fa fa-check"></i><b>12.1.1</b> A single-variable model for the Mario Kart data</a></li>
<li class="chapter" data-level="12.1.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#includingAndAssessingManyVariablesInAModel"><i class="fa fa-check"></i><b>12.1.2</b> Including and assessing many variables in a model</a></li>
<li class="chapter" data-level="12.1.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#adjusted-r2-as-a-better-estimate-of-explained-variance"><i class="fa fa-check"></i><b>12.1.3</b> Adjusted <span class="math inline">\(R^2\)</span> as a better estimate of explained variance</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelSelection"><i class="fa fa-check"></i><b>12.2</b> Model selection</a><ul>
<li class="chapter" data-level="12.2.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#identifying-variables-in-the-model-that-may-not-be-helpful"><i class="fa fa-check"></i><b>12.2.1</b> Identifying variables in the model that may not be helpful</a></li>
<li class="chapter" data-level="12.2.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#two-model-selection-strategies"><i class="fa fa-check"></i><b>12.2.2</b> Two model selection strategies</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#multipleRegressionModelAssumptions"><i class="fa fa-check"></i><b>12.3</b> Checking model assumptions using graphs</a></li>
<li class="chapter" data-level="12.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#logisticRegression"><i class="fa fa-check"></i><b>12.4</b> Logistic regression</a><ul>
<li class="chapter" data-level="12.4.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#email-data"><i class="fa fa-check"></i><b>12.4.1</b> Email data</a></li>
<li class="chapter" data-level="12.4.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelingTheProbabilityOfAnEvent"><i class="fa fa-check"></i><b>12.4.2</b> Modeling the probability of an event</a></li>
<li class="chapter" data-level="12.4.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#practical-decisions-in-the-email-application"><i class="fa fa-check"></i><b>12.4.3</b> Practical decisions in the email application</a></li>
<li class="chapter" data-level="12.4.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#diagnostics-for-the-email-classifier"><i class="fa fa-check"></i><b>12.4.4</b> Diagnostics for the email classifier</a></li>
<li class="chapter" data-level="12.4.5" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#improvingTheSetOfVariablesForASpamFilter"><i class="fa fa-check"></i><b>12.4.5</b> Improving the set of variables for a spam filter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html"><i class="fa fa-check"></i><b>13</b> Inference for categorical data</a><ul>
<li class="chapter" data-level="13.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#singleProportion"><i class="fa fa-check"></i><b>13.1</b> Inference for a single proportion</a><ul>
<li class="chapter" data-level="13.1.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#when-the-sample-proportion-is-nearly-normal"><i class="fa fa-check"></i><b>13.1.1</b> When the sample proportion is nearly normal</a></li>
<li class="chapter" data-level="13.1.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#confIntForPropSection"><i class="fa fa-check"></i><b>13.1.2</b> Confidence intervals for a proportion</a></li>
<li class="chapter" data-level="13.1.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#htForPropSection"><i class="fa fa-check"></i><b>13.1.3</b> Hypothesis testing for a proportion</a></li>
<li class="chapter" data-level="13.1.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#choosing-a-sample-size-when-estimating-a-proportion"><i class="fa fa-check"></i><b>13.1.4</b> Choosing a sample size when estimating a proportion</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#differenceOfTwoProportions"><i class="fa fa-check"></i><b>13.2</b> Difference of two proportions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#SampleDistributionOfTheDiffOfTwoProportions"><i class="fa fa-check"></i><b>13.2.1</b> Sample distribution of the difference of two proportions</a></li>
<li class="chapter" data-level="13.2.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#intervals-and-tests-for-p_1--p_2"><i class="fa fa-check"></i><b>13.2.2</b> Intervals and tests for <span class="math inline">\(p_1 -p_2\)</span></a></li>
<li class="chapter" data-level="13.2.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pooledHTForProportionsSection"><i class="fa fa-check"></i><b>13.2.3</b> Hypothesis testing when <span class="math inline">\(H_0: p_1=p_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#oneWayChiSquare"><i class="fa fa-check"></i><b>13.3</b> Testing for goodness of fit using chi-square (special topic)</a><ul>
<li class="chapter" data-level="13.3.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#creating-a-test-statistic-for-one-way-tables"><i class="fa fa-check"></i><b>13.3.1</b> Creating a test statistic for one-way tables</a></li>
<li class="chapter" data-level="13.3.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#chiSquareTestStatistic"><i class="fa fa-check"></i><b>13.3.2</b> The chi-square test statistic</a></li>
<li class="chapter" data-level="13.3.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-distribution-and-finding-areas"><i class="fa fa-check"></i><b>13.3.3</b> The chi-square distribution and finding areas</a></li>
<li class="chapter" data-level="13.3.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pValueForAChiSquareTest"><i class="fa fa-check"></i><b>13.3.4</b> Finding a p-value for a chi-square distribution</a></li>
<li class="chapter" data-level="13.3.5" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#evaluating-goodness-of-fit-for-a-distribution"><i class="fa fa-check"></i><b>13.3.5</b> Evaluating goodness of fit for a distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#twoWayTablesAndChiSquare"><i class="fa fa-check"></i><b>13.4</b> Testing for independence in two-way tables (special topic)</a><ul>
<li class="chapter" data-level="13.4.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#expected-counts-in-two-way-tables"><i class="fa fa-check"></i><b>13.4.1</b> Expected counts in two-way tables</a></li>
<li class="chapter" data-level="13.4.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-test-for-two-way-tables"><i class="fa fa-check"></i><b>13.4.2</b> The chi-square test for two-way tables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="gifs.html"><a href="gifs.html"><i class="fa fa-check"></i><b>14</b> GIFs</a><ul>
<li class="chapter" data-level="14.1" data-path="gifs.html"><a href="gifs.html#correlation-gifs"><i class="fa fa-check"></i><b>14.1</b> Correlation GIFs</a><ul>
<li class="chapter" data-level="14.1.1" data-path="gifs.html"><a href="gifs.html#n10-both-variables-drawn-from-a-uniform-distribution"><i class="fa fa-check"></i><b>14.1.1</b> N=10, both variables drawn from a uniform distribution</a></li>
<li class="chapter" data-level="14.1.2" data-path="gifs.html"><a href="gifs.html#correlation-between-random-deviates-from-uniform-distribution-across-four-sample-sizes"><i class="fa fa-check"></i><b>14.1.2</b> Correlation between random deviates from uniform distribution across four sample sizes</a></li>
<li class="chapter" data-level="14.1.3" data-path="gifs.html"><a href="gifs.html#correlation-between-random-deviates-from-normal-distribution-across-four-sample-sizes"><i class="fa fa-check"></i><b>14.1.3</b> Correlation between random deviates from normal distribution across four sample sizes</a></li>
<li class="chapter" data-level="14.1.4" data-path="gifs.html"><a href="gifs.html#type-i-errors-sampling-random-deviates-from-normal-distribution-with-regression-lines"><i class="fa fa-check"></i><b>14.1.4</b> Type I errors, sampling random deviates from normal distribution with regression lines</a></li>
<li class="chapter" data-level="14.1.5" data-path="gifs.html"><a href="gifs.html#cell-size-and-correlation"><i class="fa fa-check"></i><b>14.1.5</b> Cell-size and correlation</a></li>
<li class="chapter" data-level="14.1.6" data-path="gifs.html"><a href="gifs.html#regression"><i class="fa fa-check"></i><b>14.1.6</b> Regression</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="gifs.html"><a href="gifs.html#sampling-distributions"><i class="fa fa-check"></i><b>14.2</b> Sampling distributions</a><ul>
<li class="chapter" data-level="14.2.1" data-path="gifs.html"><a href="gifs.html#sampling-from-a-uniform-distribution"><i class="fa fa-check"></i><b>14.2.1</b> Sampling from a uniform distribution</a></li>
<li class="chapter" data-level="14.2.2" data-path="gifs.html"><a href="gifs.html#sampling-distribution-of-the-mean-normal-population-distribution-and-sample-histograms"><i class="fa fa-check"></i><b>14.2.2</b> Sampling distribution of the mean, Normal population distribution and sample histograms</a></li>
<li class="chapter" data-level="14.2.3" data-path="gifs.html"><a href="gifs.html#null-and-true-effect-samples-and-sampling-means"><i class="fa fa-check"></i><b>14.2.3</b> Null and True effect samples and sampling means</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="gifs.html"><a href="gifs.html#statistical-inference"><i class="fa fa-check"></i><b>14.3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="14.3.1" data-path="gifs.html"><a href="gifs.html#randomization-test"><i class="fa fa-check"></i><b>14.3.1</b> Randomization Test</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Answering questions with data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferenceForNumericalDatattest" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> t-Tests</h1>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.4.4</code></pre>
<p>One day, many moons ago, William Sealy Gosset got a job working for Guinness Breweries. They make that really dark and famous Irish beer called Guinness. What happens next went something like this (total fabrication, but mostly on point).</p>
<p>Guinness wanted all of its beers to the best beers. No mistakes, No bad beers. They wanted to improve their quality control, so that wherever you pour a Guiness, anywhere in the world, it always comes out just fantastic, let’s say 5 stars out of 5 every time. They want to be the best.</p>
<p>Guiness had some beer tasters, who were super-experts. Everytime they tasted a Guiness from the factor that wasn’t 5 out of 5, they knew right away.</p>
<p>But, Guiness had a big problem. They would make a keg of beer, and they would want to know if every single pint that would come out would be a 5 out of 5. So, the beer tasters drank pint after pint out of the keg, until it was gone. Some kegs were all 5 out of 5s. Some weren’t, Guiness needed to fix that. But, the biggest problem was that, after the testing, there <strong>was no beer left to sell</strong>, the testers drank it all (remember I’m kind of making this part up to illustrate a point, they probably still had beer to sell).</p>
<p>Guiness had a sampling and population problem. They wanted to know that the entire population of the beers they made were all 5 out of 5 stars. But, if they sampled the entire population, they would drink all of their beer, and wouldn’t have any left to sell.</p>
<p>Enter William Sealy Gosset. Gosset figured out the solution to the problem. He asked questions like this:</p>
<ol style="list-style-type: decimal">
<li>How many samples do I really need to take to know the whole population is 5 out of 5?</li>
<li>What’s the fewest amount of samples I need to take to know the above, that would mean Guiness could test fewer beers for quality, and sell more beers for profit, and making the product testing time shorter.</li>
</ol>
<p>Gosset solved those questions, and he invented something called the <em>Student’s t-test</em>. Gosset was working for Guinness, and could be fired for releasing trade-secrets that he invented (the t-test). But, Gosset published the work anyways, under a pseudonym. He called himself Student, hence Student’s t-test. Now you know the rest of the story.</p>
<p>It turns out this was a very nice thing for Gosset to have done. t-tests are used all the time, and they are useful, that’s why they are used. In this chapter we learn how they work.</p>
<p>You’ll be surprised to learn that what we’ve already talked about, (the Crump Test, and the Randomization Test), are both very very similar to the t-test. So, in general, you have already been thinking about the things you need to think about to understand t-tests. You’re probably wondering what is this <span class="math inline">\(t\)</span>, what does <span class="math inline">\(t\)</span> mean? We will tell you. Before we tell what it means, we first tell you about one more idea.</p>
<div id="check-your-confidence-in-your-mean" class="section level2">
<h2><span class="header-section-number">6.1</span> Check your confidence in your mean</h2>
<p>We’ve talked about getting a sample of data. We know we can find the mean, we know can find the standard deviation. We know we can look at the data in a histogram. These are all useful things to do for us to learn something about the properties of our data.</p>
<p>You might be thinking about these things, like mean and standard deviation, as very different things. The mean is about central tendency (where most of the data is), and the standard deviation is about variance (where most of the data isn’t). Yes, they are different things, but we can use them together to create useful new things.</p>
<p>What if I told you my sample mean was 50, and I told you nothing else about my sample. Would you be confident that most of the numbers were near 50? Would you wonder if there was a lot of variability in the sample, and many of the numbers were very different from 50. You should wonder all of those things. The mean alone, just by itself, doesn’t tell you anything about how confident you should be that the number represents all of the numbers in the sample.</p>
<p>It could be a representative number, when the standard deviation is very small, and all the numbers are close to 50. It could be a non-representative number, when the standard deviation is large, and many of the numbers are not near 50. You need to know the standard deviation in order to be confident in how well the mean represents the data.</p>
<p>How can we put the mean and the standard deviation together, to give us a new number that tells us about confidence in the mean?</p>
<p>We can do this using a ratio:</p>
<p><span class="math inline">\(\frac{mean}{\text{standard deviation}}\)</span></p>
<p>Think about what happens here. We are dividing a number by a number. Look at what happens:</p>
<p><span class="math inline">\(\frac{number}{\text{same number}} = 1\)</span></p>
<p><span class="math inline">\(\frac{number}{\text{smaller number}} = \text{big number}\)</span></p>
<p>compared to:</p>
<p><span class="math inline">\(\frac{number}{\text{bigger number}} = \text{smaller number}\)</span></p>
<p>Imagine we have a mean of 50, and a truly small standard deviation of 1. What do we get with our formula?</p>
<p>$ = 50 $</p>
<p>Imgaine we have a mean of 50, and a big standard deviation of 100. What do we get with our formula?</p>
<p>$ = 0.5 $</p>
<p>Notice, when we have a mean paired with a small standard deviation, our formula gives us a big number, like 50. When we have a mean paired with a large standard deviation, our formula gives us a small number, like 0.5. These numbers can tell us something about confidence in our mean, in a general way. We can be 50 confident in our mean in the first case, and only 0.5 (not at a lot) confident in the second case.</p>
<p>What did we do here? We created a descriptive statistic by dividing the mean by the standard deviation. And, we have a sense of how to interpet this number, when it’s big we’re more confident that the mean represents all of the numbers, when it’s small we are less confident. This is a useful kind of number, a ratio between what we think about our sample (the mean), and the variability in our sample (the standard deviation). Get used to this idea. Almost everything that follows in this textbook is based on this kind of ratio. We will see that our ratio becomes different kinds of “statistics”, and the ratios will look like this in general:</p>
<p><span class="math inline">\(\text{name of statistic} = \frac{\text{measure of what we know}}{\text{measure of what we don&#39;t know}}\)</span></p>
<p>or, to say it using different words:</p>
<p><span class="math inline">\(\text{name of statistic} = \frac{\text{measure of effect}}{\text{measure of error}}\)</span></p>
<p>In fact, this is the general formula for the t-test. Big surprise!</p>
</div>
<div id="one-sample-t-test" class="section level2">
<h2><span class="header-section-number">6.2</span> One-sample t-test</h2>
<p>Now we are ready to talk about t-test. We will talk about three of them. We start with the one-sample t-test.</p>
<p>Commonly, the one-sample t-test is used to estimate the chances that your sample came from a particular popualtion. Specifically, you might want to know whether the mean that you found from your sample, could have come from a particular population having a particular mean.</p>
<p>Straight away, the one-sample t-test becomes a little confusing (and I haven’t even described it yet). Offically, the uses known parameters from the population, like the mean of the population and the standard deviation of the population. However, most times you don’t know those parameters of the population! So, you have to estimate them from your sample. Remember, from the chapter on descriptive statistics, our sample mean is an unbiased estimate of the population mean. And, our sample standard deviation (the one where we divide by n-1) is an unbiased estimate of the population standard deviation. When Gosset developed the t-test, he recognized that he could use these estimates from his samples, to make the t-test. Here is the formula for the one sample t-test, we first use words, and then become more specific:</p>
<div id="formulas-for-one-sample-t-test" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Formulas for one-sample t-test</h3>
<p><span class="math inline">\(\text{name of statistic} = \frac{\text{measure of effect}}{\text{measure of error}}\)</span></p>
<p><span class="math inline">\(\text{t} = \frac{\text{measure of effect}}{\text{measure of error}}\)</span></p>
<p><span class="math inline">\(\text{t} = \frac{\text{Mean difference}}{\text{standard error}}\)</span></p>
<p><span class="math inline">\(\text{t} = \frac{\bar{X}-u}{S_{\bar{X}}}\)</span></p>
<p><span class="math inline">\(\text{t} = \frac{\text{Sample Mean  - Population Mean}}{\text{Sample Standard Error}}\)</span></p>
<p><span class="math inline">\(\text{Estimated Standard Error} = \text{Standard Error of Sample} = \frac{s}{\sqrt{N}}\)</span></p>
<p>Where, s is the sample standard deviation.</p>
<p>Some of you may have gone cross-eyed looking at all of this. Remember, we’ve seen it before when we divided our mean by the standard deviation in the first bit. The t-test is just a measure of a sample mean, divided by the standard error of the sample mean. That is it.</p>
</div>
<div id="what-does-t-represent" class="section level3">
<h3><span class="header-section-number">6.2.2</span> What does t represent?</h3>
<p><span class="math inline">\(t\)</span> gives us a measure of confidence, just like our previous ratio for dividing the mean by a standard deviations.</p>
<div class="marginnote">
<p>What does the t in t-test stand for? Apparently nothing. Gosset originally labelled it z. And, Fisher later called it t, perhaps because t comes after s, which is often used for the sample standard deviation.</p>
</div>
<p><span class="math inline">\(t\)</span> is a property of the data that you collect. You compute it with a sample mean, and a sample standard error (there’s one more thing in the one-sample formula, the population mean, which we get to in a moment). This is why we call <span class="math inline">\(t\)</span>, a sample-statistic. It’s a statistic we compute from the sample.</p>
<p>What kinds of numbers should we expect to find for these <span class="math inline">\(ts\)</span>? Well, wouldn’t that depend on a lot of things, how could we know that?</p>
<p>Let’s start small, and work through some examples. Imagine your sample mean is 5. You want to know if it came from a population that also has a mean of 5. In this case, what would t be? It would be zero, because we first subtract sample mean from the population mean, 5-5=0. So, t can be zero, when there is no difference.</p>
<p>Let’s say you take another sample, do you think the mean will be 5 every time, probably not. Let’s say the mean is 6. So, what can t be here? It will be a positive number, because 6-5= +1. But, will t be +1? That depends on the standard error of the sample. If the standard error of the sample is 1, then t could be 1, because 1/1 = 1.</p>
<p>If the sample standard error is smaller than 1, what happens to <span class="math inline">\(t\)</span>? It get’s bigger right? For example, 1 divided by 0.5 = 2. If the sample standard error was 0.5, t would be 2. And, what could we do with this information? Well, it be like a measure of confidence. As t get’s bigger we could be more confident in the mean difference we are measuring.</p>
<p>Can <span class="math inline">\(t\)</span> be smaller than 1? Sure, it can. If the sample standard error is big, say like 2, then <span class="math inline">\(t\)</span> will be smaller than one (in our case), e.g., 1/2 = .5. The direction of the difference between the sample mean and population mean, can also make the <span class="math inline">\(t\)</span> become negative. What if our sample mean was 4. Well, then <span class="math inline">\(t\)</span> will be negative, because the mean difference in the numerator will be negative, and the number in the bottom (denominator) will always be positive (remember why, it’s the standard error, computed from the sample standard deviation, which is always positive because of the squaring that we did.).</p>
<p>So, that is some intutions about what the kinds of values t can take. <span class="math inline">\(t\)</span> can be positive or negative, and big or small. Fine then, what can we do with this?</p>
</div>
<div id="calculating-t-from-data" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Calculating t from data</h3>
<p>Let’s briefly calculate a t-value from a small sample. Let’s say we had 10 students do a true/false quiz 5 questions on it. There’s a 50% chance of getting each answer correct.</p>
<p>Every student completes the 5 questions, we grade them, and then we find their performance (mean percent correct). Want we want to know is whether the students were guessing. If they were all guessing, then the sample mean should be about 50%, it shouldn’t be different from chance, which is 50%. Let’s look at the table:</p>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">students</th>
<th align="left">scores</th>
<th align="left">mean</th>
<th align="left">Difference_from_Mean</th>
<th align="left">Squared_Deviations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">50</td>
<td align="left">61</td>
<td align="left">-11</td>
<td align="left">121</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">70</td>
<td align="left">61</td>
<td align="left">9</td>
<td align="left">81</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">60</td>
<td align="left">61</td>
<td align="left">-1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">40</td>
<td align="left">61</td>
<td align="left">-21</td>
<td align="left">441</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">80</td>
<td align="left">61</td>
<td align="left">19</td>
<td align="left">361</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">30</td>
<td align="left">61</td>
<td align="left">-31</td>
<td align="left">961</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">90</td>
<td align="left">61</td>
<td align="left">29</td>
<td align="left">841</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">60</td>
<td align="left">61</td>
<td align="left">-1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">70</td>
<td align="left">61</td>
<td align="left">9</td>
<td align="left">81</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">60</td>
<td align="left">61</td>
<td align="left">-1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">Sums</td>
<td align="left">610</td>
<td align="left">610</td>
<td align="left">0</td>
<td align="left">2890</td>
</tr>
<tr class="even">
<td align="left">Means</td>
<td align="left">61</td>
<td align="left">61</td>
<td align="left">0</td>
<td align="left">289</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">sd</td>
<td align="left">17.92</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">SEM</td>
<td align="left">5.67</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left">t</td>
<td align="left">1.94003527336861</td>
</tr>
</tbody>
</table>
<p>You can see the column column has all of the test scores for each of the 10 students. We did the things we need to do to compute the standard deviation.</p>
<p>Remember the sample standard deviation is the square root of the sample variance, or:</p>
<p><span class="math inline">\(\text{sample standard deviation} = \sqrt{\frac{\sum_{i}^{n}({x_{i}-\bar{x})^2}}{N-1}}\)</span></p>
<p><span class="math inline">\(\text{sd} = \sqrt{\frac{2890}{10-1}} = 17.92\)</span></p>
<p>The standard error of the mean, is the standard deviation divided by the square root of N</p>
<p>$ =  =  = 5.67 $</p>
<p><span class="math inline">\(t\)</span> is the difference between our sample mean (61), and our population mean (50, assuming chance), divided by the standard error of the mean.</p>
<p><span class="math inline">\(\text{t} = \frac{\bar{X}-u}{S_{\bar{X}}} = \frac{\bar{X}-u}{SEM} = \frac{61-50}{5.67} = 1.94\)</span></p>
<p>And, that is you how calculate <span class="math inline">\(t\)</span>, by hand. It’s a pain. I was annoyed doing it this way. In the lab, you learn how to calculate <span class="math inline">\(t\)</span> using software, so it will just spit out <span class="math inline">\(t\)</span>. For example in R, all you have to do is this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(scores, <span class="dt">mu=</span><span class="dv">50</span>)</code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  scores
## t = 1.9412, df = 9, p-value = 0.08415
## alternative hypothesis: true mean is not equal to 50
## 95 percent confidence interval:
##  48.18111 73.81889
## sample estimates:
## mean of x 
##        61</code></pre>
</div>
<div id="how-does-t-behave" class="section level3">
<h3><span class="header-section-number">6.2.4</span> How does <span class="math inline">\(t\)</span> behave?</h3>
<p>If <span class="math inline">\(t\)</span> is just a number that we can from our sample (it is), what can we do with it? How can we use <span class="math inline">\(t\)</span> for statistical inference?</p>
<p>Remember back to the chapter on sampling and distributions, that’s where we discussed the sampling distribution of the sample mean. Remember, we made a lot of samples, then computed the mean for each sample, then we plotted a histogram of the sample means. Later, in that same section, we mentioned that we could generate sampling distributions for any statistic. That, for each sample, we could compute the mean, the standard deviation, the standard error, and now even <span class="math inline">\(t\)</span>, if we wanted to. We could generate 10,000 samples, and draw four histograms, one for each sampling distribution for each statistic.</p>
<p>This is exactly what I did, and the results are shown in the four figures below. I used a sample size of 20, and drew random observations for each sample from a normal distribution, with mean = 0, and standard deviation = 1. Let’s look at the sampling distributions for each of the statistics. <span class="math inline">\(t\)</span> was computed assuming with the population mean assumed to be 0.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_df&lt;-<span class="kw">data.frame</span>()
for(i in <span class="dv">1</span>:<span class="dv">10000</span>){
  sample&lt;-<span class="kw">rnorm</span>(<span class="dv">20</span>,<span class="dv">0</span>,<span class="dv">1</span>)
  sample_mean&lt;-<span class="kw">mean</span>(sample)
  sample_sd&lt;-<span class="kw">sd</span>(sample)
  sample_se&lt;-<span class="kw">sd</span>(sample)/<span class="kw">sqrt</span>(<span class="kw">length</span>(sample))
  sample_t&lt;-<span class="kw">as.numeric</span>(<span class="kw">t.test</span>(sample, <span class="dt">mu=</span><span class="dv">0</span>)$statistic)
  t_df&lt;-<span class="kw">data.frame</span>(i,sample_mean,sample_sd,sample_se,sample_t)
  all_df&lt;-<span class="kw">rbind</span>(all_df,t_df)
}

<span class="kw">library</span>(ggpubr)
a&lt;-<span class="kw">ggplot</span>(all_df,<span class="kw">aes</span>(<span class="dt">x=</span>sample_mean))+
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&quot;white&quot;</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>()
b&lt;-<span class="kw">ggplot</span>(all_df,<span class="kw">aes</span>(<span class="dt">x=</span>sample_sd))+
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&quot;white&quot;</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>()
c&lt;-<span class="kw">ggplot</span>(all_df,<span class="kw">aes</span>(<span class="dt">x=</span>sample_se))+
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&quot;white&quot;</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>()
d&lt;-<span class="kw">ggplot</span>(all_df,<span class="kw">aes</span>(<span class="dt">x=</span>sample_t))+
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">color=</span><span class="st">&quot;white&quot;</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>()

<span class="kw">ggarrange</span>(a,b,c,d,
          <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> <span class="dv">2</span>)</code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="statistics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We see four sampling distributions. This is how statistical summaries of these summaries behave. We have used the word chance windows before. These are four chance windows, measuring different aspects of the sample. In this case, all of the samples came from the same normal distribution. Because of sampling error, each sample is not identical. The means are not identical, the standard deviations are not identical, sample standard error of the means are not identical, and the <span class="math inline">\(t\)</span>s of the samples are not identical. They all have some variation, as shown by the histograms. This is how samples, of size 20 behave.</p>
<p>We can see straight away, that in this case, we are unlikely to get a sample mean of 2. That’s way outside the window. The range for the sampling distribution of the mean is around -.5 to +.5, and is centered on 0 (the population mean, would you believe!).</p>
<p>We are unlikely to get sample standard deviations of between .6 and 1.5, that is a different range, specific to the sample standard deviation.</p>
<p>Same thing with the sample standard error of the mean, the range here is even smaller, mostly between .1, and .3. You would rarely find a sample with a standard error of the mean greater than .3. Virtually never would you find one of say 1 (for this situation).</p>
<p>Now, look at <span class="math inline">\(t\)</span>. It’s range is basically between -3 and +3 here. 3s barely happen at all. you pretty much never see a 5 or -5 in this situation.</p>
<p>All of these sampling windows are chance windows, and they can all be used in the same way as we have used similar sampling distributions before (e.g., Crump Test, and Randomization Test) for statistical inference. For all of them we would follow the same process:</p>
<ol style="list-style-type: decimal">
<li>Generate these distributions</li>
<li>Look at your sample statistics for the dat you have (mean, sd, sem, and <span class="math inline">\(t\)</span>)</li>
<li>Find out how likely each value (or greater than that value) is</li>
<li>Obtain that probability</li>
<li>See if you think your sample statistics were probable or improbable.</li>
</ol>
<p>We’ll formalize this in a second. I just want you to know that what you will be doing is something that you have already done before. For example, in the Crump test and the Randomization test we focused on the distribution of mean differences. We could do that again here, but instead, we will focus on the distribution of <span class="math inline">\(t\)</span> values. We then apply the same kinds of decision rules to the <span class="math inline">\(t\)</span> distribution, as we did for the other distributions. Below you will see a graph you have already seen, except this time it is a distribution of <span class="math inline">\(t\)</span>s, not mean differences:</p>
<p>Remember, if we obtained a single <span class="math inline">\(t\)</span> from one sample we collected, we could consult this chance window below to find out the <span class="math inline">\(t\)</span> we obtained from the sample was likely or unlikely to occur by chance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample_t&lt;-all_df$sample_t

<span class="kw">ggplot</span>(all_df,<span class="kw">aes</span>(<span class="dt">x=</span>sample_t))+
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;rect&quot;</span>, <span class="dt">xmin=</span><span class="kw">min</span>(sample_t), <span class="dt">xmax=</span><span class="kw">max</span>(sample_t), <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span><span class="ot">Inf</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>, <span class="dt">fill=</span><span class="st">&quot;red&quot;</span>) +
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;rect&quot;</span>, <span class="dt">xmin=</span><span class="kw">min</span>(sample_t), <span class="dt">xmax=</span>-<span class="fl">1.94</span>, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span><span class="ot">Inf</span>, <span class="dt">alpha=</span><span class="fl">0.7</span>, <span class="dt">fill=</span><span class="st">&quot;light grey&quot;</span>) +
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&quot;rect&quot;</span>, <span class="dt">xmin=</span><span class="fl">1.94</span>, <span class="dt">xmax=</span><span class="kw">max</span>(sample_t), <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span><span class="ot">Inf</span>, <span class="dt">alpha=</span><span class="fl">0.7</span>, <span class="dt">fill=</span><span class="st">&quot;light grey&quot;</span>) +
<span class="st">  </span><span class="kw">geom_rect</span>(<span class="kw">aes</span>(<span class="dt">xmin=</span>-<span class="ot">Inf</span>, <span class="dt">xmax=</span><span class="kw">min</span>(sample_t), <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span><span class="ot">Inf</span>), <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">fill=</span><span class="st">&quot;lightgreen&quot;</span>)+
<span class="st">  </span><span class="kw">geom_rect</span>(<span class="kw">aes</span>(<span class="dt">xmin=</span><span class="kw">max</span>(sample_t), <span class="dt">xmax=</span><span class="ot">Inf</span>, <span class="dt">ymin=</span><span class="dv">0</span>, <span class="dt">ymax=</span><span class="ot">Inf</span>), <span class="dt">alpha=</span>.<span class="dv">5</span>, <span class="dt">fill=</span><span class="st">&quot;lightgreen&quot;</span>)+
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins=</span><span class="dv">50</span>, <span class="dt">color=</span><span class="st">&quot;white&quot;</span>)+
<span class="st">  </span><span class="kw">theme_classic</span>()+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">min</span>(sample_t))+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="kw">max</span>(sample_t))+
<span class="st">   </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> -<span class="fl">1.94</span>)+
<span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">1.94</span>)+
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Histogram of mean sample_ts between two samples (n=20) </span><span class="ch">\n</span>
<span class="st">          both drawn from the same normal distribution (u=0, sd=1)&quot;</span>)+
<span class="st">   </span><span class="kw">xlim</span>(-<span class="dv">8</span>,<span class="dv">8</span>)+
<span class="st">  </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">y =</span> <span class="dv">250</span>, <span class="dt">label =</span> <span class="st">&quot;CHANCE&quot;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">label =</span> label))+
<span class="st">  </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> -<span class="dv">7</span>, <span class="dt">y =</span> <span class="dv">250</span>, <span class="dt">label =</span> <span class="st">&quot;NOT </span><span class="ch">\n</span><span class="st"> CHANCE&quot;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">label =</span> label))+
<span class="st">  </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">7</span>, <span class="dt">y =</span> <span class="dv">250</span>, <span class="dt">label =</span> <span class="st">&quot;NOT </span><span class="ch">\n</span><span class="st"> CHANCE&quot;</span>), <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">label =</span> label))+
<span class="co">#  geom_label(data = data.frame(x = min(sample_t), y = 600,</span>
<span class="st"> </span><span class="co">#                              label = paste0(&quot;min \n&quot;,round(min(sample_t)))),</span>
<span class="st">  </span><span class="co">#                             aes(x = x, y = y, label = label))+</span>
<span class="st">   </span><span class="co">#geom_label(data = data.frame(x = max(sample_t), y = 600,</span>
<span class="st">   </span><span class="co">#                            label = paste0(&quot;max \n&quot;,round(max(sample_t)))),</span>
<span class="st">    </span><span class="co">#                           aes(x = x, y = y, label = label))+</span>
<span class="st">  </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> -<span class="dv">4</span>, <span class="dt">y =</span> <span class="dv">250</span>,
                               <span class="dt">label =</span> <span class="st">&quot;?&quot;</span>),
                               <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">label =</span> label))+
<span class="st">   </span><span class="kw">geom_label</span>(<span class="dt">data =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">4</span>, <span class="dt">y =</span> <span class="dv">250</span>,
                               <span class="dt">label =</span> <span class="st">&quot;?&quot;</span>),
                               <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">label =</span> label))+
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;mean sample_t&quot;</span>)</code></pre></div>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_bar).</code></pre>
<p><img src="statistics_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="making-a-decision" class="section level3">
<h3><span class="header-section-number">6.2.5</span> Making a decision</h3>
<p>From our early example involving the TRUE/FALSE quizzes, we are not ready to make some kind of decision about what happended there. We found a mean difference of 11. We found a <span class="math inline">\(t\)</span> = 1.9411765. The probability of this <span class="math inline">\(t\)</span> or larger occuring is <span class="math inline">\(p\)</span> = 0.0841503. We were testing the idea that our sample mean of 61 could have come from a normal distribution with mean = 50. The <span class="math inline">\(t\)</span> test tells us that the <span class="math inline">\(t\)</span> for our sample, or a larger one, would happen with p = 0.0841503. In other words, chance can do it a kind of small amount of time, but not often. In english, this means that all of the students could have been, guessing, but it wasn’t that likely that were just guessing.</p>
<p>We’re guessing that you are still a little bit confused about <span class="math inline">\(t\)</span> values, and what we are doing here. We are going to skip ahead to the next <span class="math inline">\(t\)</span>-test, called a <strong>paired samples t-test</strong>. We will also fill in some more things about <span class="math inline">\(t\)</span>-tests that are more obvious when discussing paired samples t-test. In fact, spoiler alert, we will find out that paired samples t-test is actually a one-sample t-test in disguise (WHAT!), yes it is. If the one-sample <span class="math inline">\(t\)</span>-test didn’t make sense to you, read the next section.</p>
</div>
</div>
<div id="paired-samples-t-test" class="section level2">
<h2><span class="header-section-number">6.3</span> Paired-samples <span class="math inline">\(t\)</span>-test</h2>
<p>For me (Crump), everything often comes down to a paired samples t-test. It just happens that many things I do reduce down to a test like this.</p>
<p>I am a cognitive psychologist, I conduct research on people to learn how they do things like remember, pay attention, and learn skills. There are lots of Psychologists like me, who do very similar things.</p>
<p>We all often conduct the same kinds of experiments. They go like this, and they are called <strong>repeated measures</strong> designs. They are called <strong>repeated measures</strong> designs, because we measure how one person does something more than once, we <strong>repeat</strong> the measure.</p>
<p>So, I might measure somebody doing something in condition A, and measure somebody doing something in Condition B, and then I see that same person does different things in the two conditions. I <strong>repeatedly measure</strong> the same person in both conditions. I am interested in whether the experimental manipulation changes something about how people perform the task in question.</p>
<!--

Chapters [FoundationForInference] and [inferenceForCategoricalData] introduced us to inference for proportions using the normal model, and in Section [oneWayChiSquare], we encountered the chi-square distribution, which is useful for working with categorical data with many levels. In this chapter, our focus will be on numerical data, where we will encounter two more distributions: the $t$ distribution (looks a lot like the normal distribution) and the $F$ distribution. Our general approach will be:

1.  Determine which point estimate or test statistic is useful.

2.  Identify an appropriate distribution for the point estimate or test statistic.

3.  Apply the hypothesis and confidence interval techniques from Chapter [FoundationForInference] using the distribution from step 2.

## One-sample means with the $t$ distribution {#oneSampleMeansWithTDistribution}

The sampling distribution associated with a sample mean or difference of two sample means is, if certain conditions are satisfied, nearly normal. However, this becomes more complex when the sample size is small, where *small* here typically means a sample size smaller than 30 observations. For this reason, we’ll use a new distribution called the $t$ distribution that will often work for both small and large samples of numerical data.

### Two examples using the normal distribution

Before we get started with the $t$ distribution, let’s take a look at two applications where it is okay to use the normal model for the sample mean. For the case of a single mean, the standard error of the sample mean can be calculated as

$$\begin{aligned}
SE = \frac{\sigma}{\sqrt{n}}\end{aligned}$$

where $\sigma$ is the population standard deviation and $n$ is the sample size. Generally we use the sample standard deviation, denoted by $s$, in place of the population standard deviation when we compute the standard error:

$$\begin{aligned}
SE \approx \frac{s}{\sqrt{n}}\end{aligned}$$

If we look at this formula, there are some characteristics that we can think about intuitively.

-   If we examine the standard error formula, we would see that a larger $s$ corresponds to a larger $SE$. This makes intuitive sense: if the data are more volatile, then we’ll be less certain of the location of the true mean, so the standard error should be bigger. On the other hand, if the observations all fall very close together, then $s$ will be small, and the sample mean should be a more precise estimate of the true mean.

-   In the formula, the larger the sample size $n$, the smaller the standard error. This matches our intuition: we expect estimates to be more precise when we have more data, so the standard error $SE$ should get smaller when $n$ gets bigger.

As we did with proportions, we’ll also need to check a few conditions before using the normal model. We’ll forgo describing those details until later this section, but these conditions have been verified for the two examples below.

<span>We’ve taken a random sample of 100 runners from a race called the Cherry Blossom Run in Washington, DC, which was a race with 16,924 participants.[^4.1] The sample data for the 100 runners is summarized in Table [run10SampDF], histograms of the run time and age of participants are in Figure [run10SampHistograms], and summary statistics are available in Table [ptEstimatesNetTimeAge]. Create a 95% confidence interval for the average time it takes runners in the Cherry Blossom Run to complete the race.</span> We can use the same confidence interval formula for the mean that we used for a proportion:

$$\begin{aligned}
\text{point estimate}\ \pm\ 1.96 \times SE\end{aligned}$$

In this case, the best estimate of the overall mean is the sample mean, $\bar{x} = 95.61$ minutes. The standard error can be calculated using sample standard deviation ($s = 15.78$), the sample size ($n=100$), and the standard error formula:

$$\begin{aligned}
SE = \frac{s}{\sqrt{n}} = \frac{15.78}{\sqrt{100}} = 1.578\end{aligned}$$

Finally, we can calculate a 95% confidence interval:

$$\begin{aligned}
\text{point estimate}\ \pm\ z^{\star} \times SE \quad\rightarrow\quad
95.61 \pm 1.96 \times 1.578 \quad\rightarrow\quad
(92.52, 98.70)\end{aligned}$$

We are 95% confident that the average time for all runners in the 2012 Cherry Blossom Run is between 92.52 and 98.70 minutes.

          ID       time        age     gender      state
  ---------- ---------- ---------- ---------- ----------
           1      88.31         59          M         MD
           2     100.67         32          M         VA
           3     109.52         33          F         VA
    $\vdots$   $\vdots$   $\vdots$   $\vdots$   $\vdots$
         100      89.49         26          M         DC

  : Four observations for the **run10Samp** data set, which represents a simple random sample of 100 runners from the 2012 Cherry Blossom Run.

[run10SampDF]

![Histograms of **time** and **age** for the sample Cherry Blossom Run data. The average time is in the mid-90s, and the average age is in the mid-30s. The age distribution is moderately skewed to the right.](04/figures/run10SampHistograms/run10SampHistograms)

[run10SampHistograms]

  ----------------- ------- -------
  sample mean         95.61   35.05
  sample median       95.37   32.50
  sample st. dev.     15.78    8.97
  ----------------- ------- -------

  : Point estimates and parameter values for the **time** variable.

[ptEstimatesNetTimeAge]

Use the data to calculate a 90% confidence interval for the average age of participants in the 2012 Cherry Blossom Run. The conditions for applying the normal model have already been verified.[^4.2]

<span>The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. Is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips? The conditions necessary for applying the normal model have been checked and are satisfied.</span> The question has been framed in terms of two possibilities: the nutrition label accurately lists the correct average calories per bag of chips or it does not, which may be framed as a hypothesis test:

-   The average is listed correctly. $\mu = 130$

-   The nutrition label is incorrect. $\mu \neq 130$

The observed average is $\bar{x} = 134$ and the standard error may be calculated as $SE = \frac{17}{\sqrt{35}} = 2.87$. First, we draw a picture summarizing this scenario.

![image](04/figures/potatoChips/potatoChips)

We can compute a test statistic as the Z score:

$$\begin{aligned}
Z = \frac{134 - 130}{2.87} = 1.39\end{aligned}$$

The upper-tail area is 0.0823, so the p-value is $2 \times 0.0823 = 0.1646$. Since the p-value is larger than 0.05, we do not reject the null hypothesis. That is, there is not enough evidence to show the nutrition label has incorrect information.

The normal model works well when the sample size is larger than about 30. For smaller sample sizes, we run into a problem: our estimate of $s$, which is used to compute the standard error, isn’t as reliable when the sample size is small. To solve this problem, we’ll use a new distribution: the $t$ distribution.

### Introducing the $t$ distribution {#introducingTheTDistribution}

A $t$ distribution, shown as a solid line in Figure [tDistCompareToNormalDist], has a bell shape that looks very similar to a normal distribution (dotted line). However, its tails are thicker, which means observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution.[^4.3] When our sample is small, the value $s$ used to compute the standard error isn’t very reliable. The extra thick tails of the $t$ distribution are exactly the correction we need to resolve this problem.

![Comparison of a $t$ distribution (solid line) and a normal distribution (dotted line).](04/figures/tDistCompareToNormalDist/tDistCompareToNormalDist)

[tDistCompareToNormalDist]

The $t$ distribution, always centered at zero, has a single parameter: degrees of freedom. The describe the precise form of the bell-shaped $t$ distribution. Several $t$ distributions are shown in Figure [tDistConvergeToNormalDist] with various degrees of freedom. When there are more degrees of freedom, the $t$ distribution looks very much like the standard normal distribution.

![The larger the degrees of freedom, the more closely the $t$ distribution resembles the standard normal model.](04/figures/tDistConvergeToNormalDist/tDistConvergeToNormalDist)

[tDistConvergeToNormalDist]

<span> The degrees of freedom describe the shape of the $t$ distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model.</span>

When the degrees of freedom is about 30 or more, the $t$ distribution is nearly indistinguishable from the normal distribution, e.g. see Figure [tDistConvergeToNormalDist]. In Section [tDistSolutionToSEProblem], we relate degrees of freedom to sample size.

We will find it very useful to become familiar with the $t$ distribution, because it plays a very similar role to the normal distribution during inference for numerical data. We use a , partially shown in Table [tTableSample], in place of the normal probability table for small sample numerical data. A larger table is presented in Appendix . Alternatively, we could use statistical software to get this same information.

<span>r | rrr rr</span> one tail &

0.100 &

0.050 &

0.025 &

0.010 &

0.005\
two tails & 0.200 & 0.100 & 0.050 & 0.020 & 0.010\
 1 & <span>3.08</span> & <span>6.31</span> & <span>12.71</span> & <span>31.82</span> & <span>63.66</span>\
2 & <span>1.89</span> & <span>2.92</span> & <span>4.30</span> & <span>6.96</span> & <span>9.92</span>\
3 & <span>1.64</span> & <span>2.35</span> & <span>3.18</span> & <span>4.54</span> & <span>5.84</span>\
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &\
17 & <span>1.33</span> & <span>1.74</span> & <span>2.11</span> & <span>2.57</span> & <span>2.90</span>\
 & & & & &\
19 & <span>1.33</span> & <span>1.73</span> & <span>2.09</span> & <span>2.54</span> & <span>2.86</span>\
20 & <span>1.33</span> & <span>1.72</span> & <span>2.09</span> & <span>2.53</span> & <span>2.85</span>\
$\vdots$ & $\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &\
400 & <span>1.28</span> & <span>1.65</span> & <span>1.97</span> & <span>2.34</span> & <span>2.59</span>\
500 & <span>1.28</span> & <span>1.65</span> & <span>1.96</span> & <span>2.33</span> & <span>2.59</span>\
$\infty$ & <span>1.28</span> & <span>1.65</span> & <span>1.96</span> & <span>2.33</span> & <span>2.58</span>\

[tTableSample]

Each row in the $t$ table represents a $t$ distribution with different degrees of freedom. The columns correspond to tail probabilities. For instance, if we know we are working with the $t$ distribution with $df=18$, we can examine row 18, which is in Table [tTableSample]. If we want the value in this row that identifies the cutoff for an upper tail of 10%, we can look in the column where *one tail* is 0.100. This cutoff is 1.33. If we had wanted the cutoff for the lower 10%, we would use -1.33. Just like the normal distribution, all $t$ distributions are symmetric.

<span>What proportion of the $t$ distribution with 18 degrees of freedom falls below -2.10?</span> Just like a normal probability problem, we first draw the picture in Figure [tDistDF18LeftTail2Point10] and shade the area below -2.10. To find this area, we identify the appropriate row: $df=18$. Then we identify the column containing the absolute value of -2.10; it is the third column. Because we are looking for just one tail, we examine the top line of the table, which shows that a one tail area for a value in the third row corresponds to 0.025. About 2.5% of the distribution falls below -2.10. In the next example we encounter a case where the exact $t$ value is not listed in the table.

![The $t$ distribution with 18 degrees of freedom. The area below -2.10 has been shaded.](04/figures/tDistDF18LeftTail2Point10/tDistDF18LeftTail2Point10)

[tDistDF18LeftTail2Point10]

<span>A $t$ distribution with 20 degrees of freedom is shown in the left panel of Figure [tDistDF20RightTail1Point65]. Estimate the proportion of the distribution falling above 1.65.</span> We identify the row in the $t$ table using the degrees of freedom: $df=20$. Then we look for 1.65; it is not listed. It falls between the first and second columns. Since these values bound 1.65, their tail areas will bound the tail area corresponding to 1.65. We identify the one tail area of the first and second columns, 0.050 and 0.10, and we conclude that between 5% and 10% of the distribution is more than 1.65 standard deviations above the mean. If we like, we can identify the precise area using statistical software: 0.0573.

![Left: The $t$ distribution with 20 degrees of freedom, with the area above 1.65 shaded. Right: The $t$ distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded.](04/figures/tDistDF20RightTail1Point65/tDistDF20RightTail1Point65)

[tDistDF20RightTail1Point65]

<span>A $t$ distribution with 2 degrees of freedom is shown in the right panel of Figure [tDistDF20RightTail1Point65]. Estimate the proportion of the distribution falling more than 3 units from the mean (above or below).</span> As before, first identify the appropriate row: $df=2$. Next, find the columns that capture 3; because $2.92 < 3 < 4.30$, we use the second and third columns. Finally, we find bounds for the tail areas by looking at the two tail values: 0.05 and 0.10. We use the two tail values because we are looking for two (symmetric) tails.

What proportion of the $t$ distribution with 19 degrees of freedom falls above -1.79 units?[^4.4]

### Applying the $t$ distribution to the single-mean situation {#tDistSolutionToSEProblem}

When estimating the mean and standard error from a sample of numerical data, the $t$ distribution is a little more accurate than the normal model. This is true for both small and large samples, though the benefits for larger samples are limited.

<span> Use the $t$ distribution for inference of the sample mean when observations are independent and nearly normal. You may relax the nearly normal condition as the sample size increases. For example, the data distribution may be moderately skewed when the sample size is at least 30. </span>

Before applying the $t$ distribution for inference about a single mean, we check two conditions.

Independence of observations.

:   We verify this condition just as we did before. We collect a simple random sample from less than 10% of the population, or if the data are from an experiment or random process, we carefully check to the best of our abilities that the observations were independent.

Observations come from a nearly normal distribution.

:   This second condition is difficult to verify with small data sets. We often (i) take a look at a plot of the data for obvious departures from the normal model, usually in the form of prominent outliers, and (ii) consider whether any previous experiences alert us that the data may not be nearly normal. However, if the sample size is somewhat large, then we can relax this condition, e.g. moderate skew is acceptable when the sample size is 30 or more, and strong skew is acceptable when the size is about 60 or more.

When examining a sample mean and estimated standard error from a sample of $n$ independent and nearly normal observations, we use a $t$ distribution with $n-1$ degrees of freedom ($df$). For example, if the sample size was 19, then we would use the $t$ distribution with $df=19-1=18$ degrees of freedom and proceed in the same way as we did in Chapter [inferenceForCategoricalData], except that *now we use the $t$ table*.

<span> If the sample has $n$ observations and we are examining a single mean, then we use the $t$ distribution with $df=n-1$ degrees of freedom.</span>

### One sample $t$ confidence intervals {#oneSampleTConfidenceIntervals}

Dolphins are at the top of the oceanic food chain, which causes dangerous substances such as mercury to concentrate in their organs and muscles. This is an important problem for both dolphins and other animals, like humans, who occasionally eat them. For instance, this is particularly relevant in Japan where school meals have included dolphin at times.

![A Risso’s dolphin.](04/figures/rissosDolphin/rissosDolphin.jpg "fig:")\

[rissosDolphin]

Here we identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan.[^4.5] The data are summarized in Table [summaryStatsOfHgInMuscleOfRissosDolphins]. The minimum and maximum observed values can be used to evaluate whether or not there are obvious outliers or skew.

  ----- ----------- ----- --------- ---------
   $n$   $\bar{x}$   $s$   minimum   maximum
   19       4.4      2.3     1.7       9.2
  ----- ----------- ----- --------- ---------

  : Summary of mercury content in the muscle of 19 Risso’s dolphins from the Taiji area. Measurements are in $\mu$g/wet g (micrograms of mercury per wet gram of muscle).

[summaryStatsOfHgInMuscleOfRissosDolphins]

<span>Are the independence and normality conditions satisfied for this data set?</span> The observations are a simple random sample and consist of less than 10% of the population, therefore independence is reasonable. Ideally we would see a visualization of the data to check for skew and outliers. However, we can instead examine the summary statistics in Table [summaryStatsOfHgInMuscleOfRissosDolphins], which do not suggest any skew or outliers. All observations are within 2.5 standard deviations of the mean. Based on this evidence, the normality assumption seems reasonable.

In the normal model, we used $z^{\star}$ and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the $t$ distribution:

$$\begin{aligned}
\bar{x} \ \pm\  t^{\star}_{df} \times SE\end{aligned}$$

[

$t^{\star}_{df}$

\
Multiplication\
factor for\
$t$ conf. interval]

$t^{\star}_{df}$

\
Multiplication\
factor for\
$t$ conf. interval

The sample mean and estimated standard error are computed just as in our earlier examples that used the normal model ($\bar{x} = 4.4$ and $SE = s/\sqrt{n} = 0.528$). The value $t^{\star}_{df}$ is a cutoff we obtain based on the confidence level and the $t$ distribution with $df$ degrees of freedom. Before determining this cutoff, we will first need the degrees of freedom.

In our current example, we should use the $t$ distribution with $df = n - 1 = 19 - 1 = 18$ degrees of freedom. Then identifying $t_{18}^{\star}$ is similar to how we found $z^{\star}$:

-   For a 95% confidence interval, we want to find the cutoff $t^{\star}_{18}$ such that 95% of the $t$ distribution is between -$t^{\star}_{18}$ and $t^{\star}_{18}$.

-   We look in the $t$ table on page , find the column with area totaling 0.05 in the two tails (third column), and then the row with 18 degrees of freedom: $t^{\star}_{18} = 2.10$.

Generally the value of $t^{\star}_{df}$ is slightly larger than what we would get under the normal model with $z^{\star}$.

Finally, we can substitute all the values into the confidence interval equation to create the 95% confidence interval for the average mercury content in muscles from Risso’s dolphins that pass through the Taiji area:

$$\begin{aligned}
\bar{x} \ \pm\  t^{\star}_{18} \times SE
    \quad \to \quad
4.4 \ \pm\  2.10 \times 0.528
    \quad \to \quad
(3.29, 5.51)\end{aligned}$$

We are 95% confident the average mercury content of muscles in Risso’s dolphins is between 3.29 and 5.51 $\mu$g/wet gram, which is considered extremely high.

Based on a sample of $n$ independent and nearly normal observations, a confidence interval for the population mean is

$$\begin{aligned}
\bar{x} \ \pm\  t^{\star}_{df} \times SE\end{aligned}$$

where $\bar{x}$ is the sample mean, $t^{\star}_{df}$ corresponds to the confidence level and degrees of freedom, and $SE$ is the standard error as estimated by the sample. The normality condition may be relaxed for larger sample sizes.

[croakerWhiteFishPacificExerConditions] The FDA’s webpage provides some data on mercury content of fish.[^4.6] Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent. Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?[^4.7]

<span>Estimate the standard error of the sample mean using the data summaries in Guided Practice [croakerWhiteFishPacificExerConditions]. If we are to use the $t$ distribution to create a 90% confidence interval for the actual mean of the mercury content, identify the degrees of freedom we should use and also find $t^{\star}_{df}$.</span> [croakerWhiteFishPacificExerSEDFTStar] The standard error: $SE = \frac{0.069}{\sqrt{15}} = 0.0178$. Degrees of freedom: $df = n - 1 = 14$.

Looking in the column where two tails is 0.100 (for a 90% confidence interval) and row $df=14$, we identify $t^{\star}_{14} = 1.76$.

Using the results of Guided Practice [croakerWhiteFishPacificExerConditions] and Example [croakerWhiteFishPacificExerSEDFTStar], compute a 90% confidence interval for the average mercury content of croaker white fish (Pacific).[^4.8]

### One sample $t$ tests {#oneSampleTTests}

Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Run, comparing runners in 2006 and 2012. Technological advances in shoes, training, and diet might suggest runners would be faster in 2012. An opposing viewpoint might say that with the average body mass index on the rise, people tend to run slower. In fact, all of these components might be influencing run time.

The average time for all runners who finished the Cherry Blossom Run in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2012 Cherry Blossom Run whether runners in this race are getting faster or slower, versus the other possibility that there has been no change.

What are appropriate hypotheses for this context?[^4.9]

The data come from a simple random sample from less than 10% of all participants, so the observations are independent. However, should we be worried about skew in the data? A histogram of the differences was shown in the left panel of Figure . [^4.10]

With independence satisfied and skew not a concern, we can proceed with performing a hypothesis test using the $t$ distribution.

The sample mean and sample standard deviation are 95.61 and 15.78 minutes, respectively. Recall that the sample size is 100. What is the p-value for the test, and what is your conclusion?[^4.11]

<span> To help us remember to use the $t$ distribution, we use a $T$ to represent the test statistic, and we often call this a **T score**. The Z score and T score are computed in the exact same way and are conceptually identical: each represents how many standard errors the observed value is from the null value.</span>

## Paired data {#pairedData}

Are textbooks actually cheaper online? Here we compare the price of textbooks at the University of California, Los Angeles’ (UCLA’s) bookstore and prices at Amazon.com. Seventy-three UCLA courses were randomly sampled in Spring 2010, representing less than 10% of all UCLA courses.[^4.12] A portion of the data set is shown in Table [textbooksDF].

             dept       course           ucla     amazon       diff
  ---------- ---------- ---------- ---------- ---------- ----------
           1 Am Ind     C170            27.67      27.95      -0.28
           2 Anthro     9               40.59      31.14       9.45
           3 Anthro     135T            31.68      32.00      -0.32
           4 Anthro     191HB           16.00      11.52       4.48
    $\vdots$ $\vdots$   $\vdots$     $\vdots$   $\vdots$   $\vdots$
          72 Wom Std    M144            23.76      18.72       5.04
          73 Wom Std    285             27.70      18.22       9.48

  : Six cases of the **textbooks** data set.

[textbooksDF]

### Paired observations

Each textbook has two corresponding prices in the data set: one for the UCLA bookstore and one for Amazon. Therefore, each textbook price from the UCLA bookstore has a natural correspondence with a textbook price from Amazon. When two sets of observations have this special correspondence, they are said to be **paired**.

<span> Two sets of observations are *paired* if each observation in one set has a special correspondence or connection with exactly one observation in the other data set.</span>

To analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations. In the **textbook** data set, we look at the difference in prices, which is represented as the **diff** variable in the **textbooks** data. Here the differences are taken as

$$\begin{aligned}
\text{UCLA price} - \text{Amazon price}\end{aligned}$$

for each book. It is important that we always subtract using a consistent order; here Amazon prices are always subtracted from UCLA prices. A histogram of these differences is shown in Figure [diffInTextbookPricesS10]. Using differences between paired observations is a common and useful way to analyze paired data.

![Histogram of the difference in price for each book sampled. These data are strongly skewed.](04/figures/textbooksS10/diffInTextbookPricesS10)

[diffInTextbookPricesS10]

The first difference shown in Table [textbooksDF] is computed as $27.67-27.95=-0.28$. Verify the differences are calculated correctly for observations 2 and 3.[^4.13]

### Inference for paired data

To analyze a paired data set, we simply analyze the differences. We can use the same $t$ distribution techniques we applied in the last section.

<span>ccccc</span> $n_{_{diff}}$ &

& $\bar{x}_{_{diff}}$ &

& $s_{_{diff}}$

\
73 && 12.76 && 14.26\

[textbooksSummaryStats]

<span>Set up and implement a hypothesis test to determine whether, on average, there is a difference between Amazon’s price for a book and the UCLA bookstore’s price.</span> [htForDiffInUCLAAndAmazonTextbookPrices] We are considering two scenarios: there is no difference or there is some difference in average prices.

-   $\mu_{diff}=0$. There is no difference in the average textbook price.

-   $\mu_{diff} \neq 0$. There is a difference in average prices.

Can the $t$ distribution be used for this application? The observations are based on a simple random sample from less than 10% of all books sold at the bookstore, so independence is reasonable. While the distribution is strongly skewed, the sample is reasonably large ($n=73$), so we can proceed. Because the conditions are reasonably satisfied, we can apply the $t$ distribution to this setting.

We compute the standard error associated with $\bar{x}_{diff}$ using the standard deviation of the differences ($s_{_{diff}}=14.26$) and the number of differences ($n_{_{diff}}=73$): $$SE_{\bar{x}_{diff}} = \frac{s_{diff}}{\sqrt{n_{diff}}} = \frac{14.26}{\sqrt{73}} = 1.67$$ To visualize the p-value, the sampling distribution of $\bar{x}_{diff}$ is drawn as though $H_0$ is true, which is shown in Figure [textbooksS10HTTails]. The p-value is represented by the two (very) small tails.

To find the tail areas, we compute the test statistic, which is the T score of $\bar{x}_{diff}$ under the null condition that the actual mean difference is 0: $$T = \frac{\bar{x}_{diff} - 0}{SE_{x_{diff}}} = \frac{12.76 - 0}{1.67} = 7.59$$ The degrees of freedom are $df = 73 - 1 = 72$. If we examined Appendix , we would see that this value is larger than any in the 70 df row (we round down for $df$ when using the table), meaning the two-tailed p-value is less than 0.01. If we used statistical software, we would find the p-value is less than 1-in-10 billion! Because the p-value is less than 0.05, we reject the null hypothesis. We have found convincing evidence that Amazon is, on average, cheaper than the UCLA bookstore for UCLA course textbooks.

![Sampling distribution for the mean difference in book prices, if the true average difference is zero.](04/figures/textbooksS10/textbooksS10HTTails)

[textbooksS10HTTails]

Create a 95% confidence interval for the average price difference between books at the UCLA bookstore and books on Amazon.[^4.14]

In the textbook price example, we applied the $t$ distribution. However, as we mentioned in the last section, the $t$ distribution looks a lot like the normal distribution when the degrees of freedom are larger than about 30. In such cases, including this one, it would be reasonable to use the normal distribution in place of the $t$ distribution.

## Difference of two means {#differenceOfTwoMeans}

In this section we consider a difference in two population means, $\mu_1 - \mu_2$, under the condition that the data are not paired. Just as with a single sample, we identify conditions to ensure we can use the $t$ distribution with a point estimate of the difference, $\bar{x}_1 - \bar{x}_2$.

We apply these methods in three contexts: determining whether stem cells can improve heart function, exploring the impact of pregnant womens’ smoking habits on birth weights of newborns, and exploring whether there is statistically significant evidence that one variations of an exam is harder than another variation. This section is motivated by questions like “Is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke?”

### Confidence interval for a differences of means

Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack? Table [summaryStatsForSheepHeartDataWhoReceivedMiceESCs] contains summary statistics for an experiment to test ESCs in sheep that had a heart attack. Each of these sheep was randomly assigned to the ESC or control group, and the change in their hearts’ pumping capacity was measured in the study. A positive value corresponds to increased pumping capacity, which generally suggests a stronger recovery. Our goal will be to identify a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity relative to the control group.

A point estimate of the difference in the heart pumping variable can be found using the difference in the sample means:

$$\begin{aligned}
\bar{x}_{esc} - \bar{x}_{control}\ =\ 3.50 - (-4.33)\ =\ 7.83\end{aligned}$$

<span>l rrrrr</span>

& $n$ & $\bar{x}$ & $s$\
ESCs & 9 & 3.50 & 5.17\
control & 9 & -4.33 & 2.76\

[summaryStatsForSheepHeartDataWhoReceivedMiceESCs]

![Histograms for both the embryonic stem cell group and the control group. Higher values are associated with greater improvement. We don’t see any evidence of skew in these data; however, it is worth noting that skew would be difficult to detect with such a small sample.](04/figures/stemCellTherapyForHearts/stemCellTherapyForHearts)

[stemCellTherapyForHearts]

<span> [ConditionsForTwoSampleTDist]The $t$ distribution can be used for inference when working with the standardized difference of two means if (1) each sample meets the conditions for using the $t$ distribution and (2) the samples are independent.</span>

<span>Can the point estimate, $\bar{x}_{esc} - \bar{x}_{control} = 7.83$, be analyzed using the $t$ distribution?</span> We check the two required conditions:

1.  In this study, the sheep were independent of each other. Additionally, the distributions in Figure [stemCellTherapyForHearts] don’t show any clear deviations from normality, where we watch for prominent outliers in particular for such small samples. These findings imply each sample mean could itself be modeled using a $t$ distribution.

2.  The sheep in each group were also independent of each other.

Because both conditions are met, we can use the $t$ distribution to model the difference of the two sample means.

Before we construct a confidence interval, we must calculate the standard error of the point estimate of the difference. For this, we use the following formula, where just as before we substitute the sample standard deviations into the formula:

$$\begin{aligned}
SE_{\bar{x}_{esc} - \bar{x}_{control}}
    &= \sqrt{\frac{\sigma_{esc}^2}{n_{esc}} + \frac{\sigma_{control}^2}{n_{control}}} \\
    &\approx \sqrt{\frac{s_{esc}^2}{n_{esc}} + \frac{s_{control}^2}{n_{control}}}
    = \sqrt{\frac{5.17^2}{9} + \frac{2.76^2}{9}} = 1.95\end{aligned}$$

Because we will use the $t$ distribution, we also must identify the appropriate degrees of freedom. This can be done using computer software. An alternative technique is to use the smaller of $n_1 - 1$ and $n_2 - 1$, which is the method we will typically apply in the examples and guided practice.[^4.15]

The sample difference of two means, $\bar{x}_1 - \bar{x}_2$, can be modeled using the $t$ distribution and the standard error

$$\begin{aligned}
\textstyle
SE_{\bar{x}_{1} - \bar{x}_{2}} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\label{seOfDifferenceInMeans}\end{aligned}$$

when each sample mean can itself be modeled using a $t$ distribution and the samples are independent. To calculate the degrees of freedom, use statistical software or the smaller of $n_1 - 1$ and $n_2 - 1$.

<span>Calculate a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity of sheep after they’ve suffered a heart attack.</span> We will use the sample difference and the standard error for that point estimate from our earlier calculations:

$$\begin{aligned}
& \bar{x}_{esc} - \bar{x}_{control} = 7.83 \\
& SE = \sqrt{\frac{5.17^2}{9} + \frac{2.76^2}{9}} = 1.95\end{aligned}$$

Using $df = 8$, we can identify the appropriate $t^{\star}_{df} = t^{\star}_{8}$ for a 95% confidence interval as 2.31. Finally, we can enter the values into the confidence interval formula:

$$\begin{aligned}
\text{point estimate} \ \pm\ z^{\star}SE \quad\rightarrow\quad
7.83 \ \pm\ 2.31\times 1.95 \quad\rightarrow\quad (3.38, 12.38)\end{aligned}$$

We are 95% confident that embryonic stem cells improve the heart’s pumping function in sheep that have suffered a heart attack by 3.38% to 12.38%.

### Hypothesis tests based on a difference in means

A data set called represents a random sample of 150 cases of mothers and their newborns in North Carolina over a year. Four cases from this data set are represented in Table [babySmokeDF]. We are particularly interested in two variables: **weight** and **smoke**. The **weight** variable represents the weights of the newborns and the **smoke** variable describes which mothers smoked during pregnancy. We would like to know, is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke? We will use the North Carolina sample to try to answer this question. The smoking group includes 50 cases and the nonsmoking group contains 100 cases, represented in Figure [babySmokePlotOfTwoGroupsToExamineSkew].

                   fAge       mAge      weeks     weight sexBaby    smoke
  ---------- ---------- ---------- ---------- ---------- ---------- -----------
           1         NA         13         37       5.00 female     nonsmoker
           2         NA         14         36       5.88 female     nonsmoker
           3         19         15         41       8.13 male       smoker
    $\vdots$   $\vdots$   $\vdots$   $\vdots$   $\vdots$ $\vdots$   
         150         45         50         36       9.25 female     nonsmoker

  : Four cases from the data set. The value “NA”, shown for the first two entries of the first variable, indicates that piece of data is missing.

[babySmokeDF]

![The top panel represents birth weights for infants whose mothers smoked. The bottom panel represents the birth weights for infants whose mothers who did not smoke. The distributions exhibit moderate-to-strong and strong skew, respectively.](04/figures/babySmokePlotOfTwoGroupsToExamineSkew/babySmokePlotOfTwoGroupsToExamineSkew)

[babySmokePlotOfTwoGroupsToExamineSkew]

<span>Set up appropriate hypotheses to evaluate whether there is a relationship between a mother smoking and average birth weight.</span>[babySmokeHTForWeight] The null hypothesis represents the case of no difference between the groups.

-   There is no difference in average birth weight for newborns from mothers who did and did not smoke. In statistical notation: $\mu_{n} - \mu_{s} = 0$, where $\mu_{n}$ represents non-smoking mothers and $\mu_s$ represents mothers who smoked.

-   There is some difference in average newborn weights from mothers who did and did not smoke ($\mu_{n} - \mu_{s} \neq 0$).

We check the two conditions necessary to apply the $t$ distribution to the difference in sample means. (1) Because the data come from a simple random sample and consist of less than 10% of all such cases, the observations are independent. Additionally, while each distribution is strongly skewed, the sample sizes of 50 and 100 would make it reasonable to model each mean separately using a $t$ distribution. The skew is reasonable for these sample sizes of 50 and 100. (2) The independence reasoning applied in (1) also ensures the observations in each sample are independent. Since both conditions are satisfied, the difference in sample means may be modeled using a $t$ distribution.

  ------------ ------ ------
  mean           6.78   7.18
  st. dev.       1.43   1.60
  samp. size       50    100
  ------------ ------ ------

  : Summary statistics for the data set.

[summaryStatsOfBirthWeightForNewbornsFromSmokingAndNonsmokingMothers]

The summary statistics in Table [summaryStatsOfBirthWeightForNewbornsFromSmokingAndNonsmokingMothers] may be useful for this exercise. (a) What is the point estimate of the population difference, $\mu_{n} - \mu_{s}$? (b) Compute the standard error of the point estimate from part (a).[^4.16]

<span>Draw a picture to represent the p-value for the hypothesis test from Example [babySmokeHTForWeight].</span> [pictureOfPValueForEstimateOfDiffOfMeansOfBirthWeights] To depict the p-value, we draw the distribution of the point estimate as though $H_0$ were true and shade areas representing at least as much evidence against $H_0$ as what was observed. Both tails are shaded because it is a two-sided test.

![image](04/figures/distOfDiffOfSampleMeansForBWOfBabySmokeData/distOfDiffOfSampleMeansForBWOfBabySmokeData)

<span>Compute the p-value of the hypothesis test using the figure in Example [pictureOfPValueForEstimateOfDiffOfMeansOfBirthWeights], and evaluate the hypotheses using a significance level of $\alpha=0.05$.</span> [babySmokeHTForWeightComputePValueAndEvalHT] We start by computing the T score:

$$\begin{aligned}
T = \frac{\ 0.40 - 0\ }{0.26} = 1.54\end{aligned}$$

Next, we compare this value to values in the $t$ table in Appendix , where we use the smaller of $n_n - 1 = 99$ and $n_s - 1 = 49$ as the degrees of freedom: $df = 49$. The T score falls between the first and second columns in the $df = 49$ row of the $t$ table, meaning the two-tailed p-value falls between 0.10 and 0.20 (reminder, find tail areas along the top of the table). This p-value is larger than the significance value, 0.05, so we fail to reject the null hypothesis. There is insufficient evidence to say there is a difference in average birth weight of newborns from North Carolina mothers who did smoke during pregnancy and newborns from North Carolina mothers who did not smoke during pregnancy.

Does the conclusion to Example [babySmokeHTForWeightComputePValueAndEvalHT] mean that smoking and average birth weight are unrelated?[^4.17]

[babySmokeHTIDingHowToDetectDifferences] If we made a Type 2 Error and there is a difference, what could we have done differently in data collection to be more likely to detect such a difference?[^4.18]

### Case study: two versions of a course exam

An instructor decided to run two slight variations of the same exam. Prior to passing out the exams, she shuffled the exams together to ensure each student received a random version. Summary statistics for how students performed on these two exams are shown in Table [summaryStatsForTwoVersionsOfExams]. Anticipating complaints from students who took Version B, she would like to evaluate whether the difference observed in the groups is so large that it provides convincing evidence that Version B was more difficult (on average) than Version A.

<span>l rrrrr</span> Version

& $n$ & $\bar{x}$ & $s$ & min & max\
A & 30 & 79.4 & 14 & 45 & 100\
B & 27 & 74.1 & 20 & 32 & 100\

[summaryStatsForTwoVersionsOfExams]

[htSetupForEvaluatingTwoExamVersions] Construct a hypotheses to evaluate whether the observed difference in sample means, $\bar{x}_A - \bar{x}_B=5.3$, is due to chance.[^4.19]

[conditionsForTDistForEvaluatingTwoExamVersions] To evaluate the hypotheses in Guided Practice [htSetupForEvaluatingTwoExamVersions] using the $t$ distribution, we must first verify assumptions. (a) Does it seem reasonable that the scores are independent within each group? (b) What about the normality / skew condition for observations in each group? (c) Do you think scores from the two groups would be independent of each other, i.e. the two samples are independent?[^4.20]

After verifying the conditions for each sample and confirming the samples are independent of each other, we are ready to conduct the test using the $t$ distribution. In this case, we are estimating the true difference in average test scores using the sample data, so the point estimate is $\bar{x}_A - \bar{x}_B = 5.3$. The standard error of the estimate can be calculated as

$$\begin{aligned}
SE = \sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}} = \sqrt{\frac{14^2}{30} + \frac{20^2}{27}} = 4.62\end{aligned}$$

Finally, we construct the test statistic:

$$\begin{aligned}
T = \frac{\text{point estimate} - \text{null value}}{SE} = \frac{(79.4-74.1) - 0}{4.62} = 1.15\end{aligned}$$

If we have a computer handy, we can identify the degrees of freedom as 45.97. Otherwise we use the smaller of $n_1-1$ and $n_2-1$: $df=26$.

![The $t$ distribution with 26 degrees of freedom. The shaded right tail represents values with $T \geq 1.15$. Because it is a two-sided test, we also shade the corresponding lower tail.](04/figures/pValueOfTwoTailAreaOfExamVersionsWhereDFIs26/pValueOfTwoTailAreaOfExamVersionsWhereDFIs26)

[pValueOfTwoTailAreaOfExamVersionsWhereDFIs26]

<span>Identify the p-value using $df = 26$ and provide a conclusion in the context of the case study.</span> We examine row $df=26$ in the $t$ table. Because this value is smaller than the value in the left column, the p-value is larger than 0.200 (two tails!). Because the p-value is so large, we do not reject the null hypothesis. That is, the data do not convincingly show that one exam version is more difficult than the other, and the teacher should not be convinced that she should add points to the Version B exam scores.

### Summary for inference using the $t$ distribution

**Hypothesis tests.** When applying the $t$ distribution for a hypothesis test, we proceed as follows:

-   Write appropriate hypotheses.

-   Verify conditions for using the $t$ distribution.

    -   One-sample or differences from paired data: the observations (or differences) must be independent and nearly normal. For larger sample sizes, we can relax the nearly normal requirement, e.g. slight skew is okay or sample sizes of 15, moderate skew for sample sizes of 30, and strong skew for sample sizes of 60.

    -   For a difference of means when the data are not paired: each sample mean must separately satisfy the one-sample conditions for the $t$ distribution, and the data in the groups must also be independent.

-   Compute the point estimate of interest, the standard error, and the degrees of freedom. For $df$, use $n-1$ for one sample, and for two samples use either statistical software or the smaller of $n_1 - 1$ and $n_2 - 1$.

-   Compute the T score and p-value.

-   Make a conclusion based on the p-value, and write a conclusion in context and in plain language so anyone can understand the result.

**Confidence intervals.** Similarly, the following is how we generally computed a confidence interval using a $t$ distribution:

-   Verify conditions for using the $t$ distribution. (See above.)

-   Compute the point estimate of interest, the standard error, the degrees of freedom, and $t^{\star}_df$.

-   Calculate the confidence interval using the general formula, point estimate $\pm t_{df}^{\star} SE$.

-   Put the conclusions in context and in plain language so even non-statisticians can understand the results.

### Pooled standard deviation estimate (special topic) {#pooledStandardDeviations}

Occasionally, two populations will have standard deviations that are so similar that they can be treated as identical. For example, historical data or a well-understood biological mechanism may justify this strong assumption. In such cases, we can make the $t$ distribution approach slightly more precise by using a pooled standard deviation.

The **pooled standard deviation** of two groups is a way to use data from both samples to better estimate the standard deviation and standard error. If $s_1^{}$ and $s_2^{}$ are the standard deviations of groups 1 and 2 and there are good reasons to believe that the population standard deviations are equal, then we can obtain an improved estimate of the group variances by pooling their data:

$$\begin{aligned}
s_{pooled}^2 = \frac{s_1^2\times (n_1-1) + s_2^2\times (n_2-1)}{n_1 + n_2 - 2}\end{aligned}$$

where $n_1$ and $n_2$ are the sample sizes, as before. To use this new statistic, we substitute $s_{pooled}^2$ in place of $s_1^2$ and $s_2^2$ in the standard error formula, and we use an updated formula for the degrees of freedom:

$$\begin{aligned}
df = n_1 + n_2 - 2\end{aligned}$$

The benefits of pooling the standard deviation are realized through obtaining a better estimate of the standard deviation for each group and using a larger degrees of freedom parameter for the $t$ distribution. Both of these changes may permit a more accurate model of the sampling distribution of $\bar{x}_1 - \bar{x}_2$.

<span>Pooling standard deviations should be done only after careful research</span> <span>A pooled standard deviation is only appropriate when background research indicates the population standard deviations are nearly equal. When the sample size is large and the condition may be adequately checked with data, the benefits of pooling the standard deviations greatly diminishes.</span>

[^4.1]: See [www.cherryblossom.org](http://www.cherryblossom.org).

[^4.2]: As before, we identify the point estimate, $\bar{x} = 35.05$, and the standard error, $SE = 8.97 / \sqrt{100} = 0.897$. Next, we apply the formula for a 90% confidence interval, which uses $z^{\star} = 1.65$: $35.05 \pm 1.65\times 0.897 \to (33.57, 36.53)$. We are 90% confident that the average age of all participants in the 2012 Cherry Blossom Run is between 33.57 and 36.53 years.

[^4.3]: The standard deviation of the $t$ distribution is actually a little more than 1. However, it is useful to always think of the $t$ distribution as having a standard deviation of 1 in all of our applications.

[^4.4]: We find the shaded area *above* -1.79 (we leave the picture to you). The small left tail is between 0.025 and 0.05, so the larger upper region must have an area between 0.95 and 0.975.

[^4.5]: Taiji was featured in the movie *The Cove*, and it is a significant source of dolphin and whale meat in Japan. Thousands of dolphins pass through the Taiji area annually, and we will assume these 19 dolphins represent a simple random sample from those dolphins. Data reference: Endo T and Haraguchi K. 2009. High mercury levels in hair samples from residents of Taiji, a Japanese whaling town. Marine Pollution Bulletin 60(5):743-747.

[^4.6]: http://www.fda.gov/food/foodborneillnesscontaminants/metals/ucm115644.htm

[^4.7]: There are no obvious outliers; all observations are within 2 standard deviations of the mean. If there is skew, it is not evident. There are no red flags for the normal model based on this (limited) information, and we do not have reason to believe the mercury content is not nearly normal in this type of fish.

[^4.8]: $\bar{x} \ \pm\ t^{\star}_{14} \times SE \ \to\  0.287 \ \pm\  1.76\times 0.0178\ \to\ (0.256, 0.318)$. We are 90% confident that the average mercury content of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.

[^4.9]: $H_0$: The average 10 mile run time was the same for 2006 and 2012. $\mu = 93.29$ minutes. $H_A$: The average 10 mile run time for 2012 was *different* than that of 2006. $\mu \neq 93.29$ minutes.

[^4.10]: With a sample of 100, we should only be concerned if there is extreme skew. The histogram of the data suggest, at worst, slight skew.

[^4.11]: With the conditions satisfied for the $t$ distribution, we can compute the standard error ($SE = 15.78 / \sqrt{100} = 1.58$ and the *T score*: $T = \frac{95.61 - 93.29}{1.58} = 1.47$. (There is more on this after the guided practice, but a T score and Z score are basically the same thing.) For $df = 100 - 1 = 99$, we would find $T = 1.47$ to fall between the first and second column, which means the p-value is between 0.05 and 0.10 (use $df = 90$ and consider two tails since the test is two-sided). Because the p-value is greater than 0.05, we do not reject the null hypothesis. That is, the data do not provide strong evidence that the average run time for the Cherry Blossom Run in 2012 is any different than the 2006 average.

[^4.12]: When a class had multiple books, only the most expensive text was considered.

[^4.13]: Observation 2: $40.59 - 31.14 = 9.45$. Observation 3: $31.68 - 32.00 = -0.32$.

[^4.14]: Conditions have already verified and the standard error computed in Example [htForDiffInUCLAAndAmazonTextbookPrices]. To find the interval, identify $t^{\star}_{72}$ (use $df=70$ in the table, $t^{\star}_{70} = 1.99$) and plug it, the point estimate, and the standard error into the confidence interval formula: $$\text{point estimate} \ \pm\ z^{\star}SE \quad\to\quad 12.76 \ \pm\ 1.99\times 1.67 \quad\to\quad (9.44, 16.08)$$ We are 95% confident that Amazon is, on average, between \$9.44 and \$16.08 cheaper than the UCLA bookstore for UCLA course books.

[^4.15]: This technique for degrees of freedom is conservative with respect to a Type 1 Error; it is more difficult to reject the null hypothesis using this $df$ method. In this example, computer software would have provided us a more precise degrees of freedom of $df = 12.225$.

[^4.16]: (a) The difference in sample means is an appropriate point estimate: $\bar{x}_{n} - \bar{x}_{s} = 0.40$. (b) The standard error of the estimate can be estimated using Equation ([seOfDifferenceInMeans]):

    $$\begin{aligned}
    SE = \sqrt{\frac{\sigma_n^2}{n_n} + \frac{\sigma_s^2}{n_s}}
        \approx \sqrt{\frac{s_n^2}{n_n} + \frac{s_s^2}{n_s}}
        = \sqrt{\frac{1.60^2}{100} + \frac{1.43^2}{50}}
        = 0.26\end{aligned}$$

[^4.17]: Absolutely not. It is possible that there is some difference but we did not detect it. If there is a difference, we made a Type 2 Error. Notice: we also don’t have enough information to, if there is an actual difference difference, confidently say which direction that difference would be in.

[^4.18]: We could have collected more data. If the sample sizes are larger, we tend to have a better shot at finding a difference if one exists.

[^4.19]: Because the teacher did not expect one exam to be more difficult prior to examining the test results, she should use a two-sided hypothesis test. $H_0$: the exams are equally difficult, on average. $\mu_A - \mu_B = 0$. $H_A$: one exam was more difficult than the other, on average. $\mu_A - \mu_B \neq 0$.

[^4.20]: \(a) It is probably reasonable to conclude the scores are independent, provided there was no cheating. (b) The summary statistics suggest the data are roughly symmetric about the mean, and it doesn’t seem unreasonable to suggest the data might be normal. Note that since these samples are each nearing 30, moderate skew in the data would be acceptable. (c) It seems reasonable to suppose that the samples are independent since the exams were handed out randomly.

-->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foundations-for-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inferenceForNumericalDataANOVA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/statistics/blob/master/06-ttests.Rmd",
"text": "Edit"
},
"download": ["statistics.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
