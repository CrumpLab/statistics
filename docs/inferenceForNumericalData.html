<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Answering questions with data</title>
  <meta name="description" content="An introductory statistics textbook for psychology students">
  <meta name="generator" content="bookdown 0.6 and GitBook 2.6.7">

  <meta property="og:title" content="Answering questions with data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introductory statistics textbook for psychology students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Answering questions with data" />
  
  <meta name="twitter:description" content="An introductory statistics textbook for psychology students" />
  

<meta name="author" content="Matthew J. C. Crump">


<meta name="date" content="2018-04-11">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="inferenceForCategoricalData.html">
<link rel="next" href="linRegrForTwoVar.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/javascript">
mattcrump=1;
</script>



<link rel="stylesheet" href="tufte.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Programming</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introductionToData.html"><a href="introductionToData.html"><i class="fa fa-check"></i><b>1</b> Introduction to data</a><ul>
<li class="chapter" data-level="1.1" data-path="introductionToData.html"><a href="introductionToData.html#basicExampleOfStentsAndStrokes"><i class="fa fa-check"></i><b>1.1</b> Case study: using stents to prevent strokes</a></li>
<li class="chapter" data-level="1.2" data-path="introductionToData.html"><a href="introductionToData.html#dataBasics"><i class="fa fa-check"></i><b>1.2</b> Data basics</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introductionToData.html"><a href="introductionToData.html#observations-variables-and-data-matrices"><i class="fa fa-check"></i><b>1.2.1</b> Observations, variables, and data matrices</a></li>
<li class="chapter" data-level="1.2.2" data-path="introductionToData.html"><a href="introductionToData.html#variableTypes"><i class="fa fa-check"></i><b>1.2.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="introductionToData.html"><a href="introductionToData.html#variableRelations"><i class="fa fa-check"></i><b>1.2.3</b> Relationships between variables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introductionToData.html"><a href="introductionToData.html#overviewOfDataCollectionPrinciples"><i class="fa fa-check"></i><b>1.3</b> Overview of data collection principles</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introductionToData.html"><a href="introductionToData.html#populationsAndSamples"><i class="fa fa-check"></i><b>1.3.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.3.2" data-path="introductionToData.html"><a href="introductionToData.html#anecdotalEvidenceSubsection"><i class="fa fa-check"></i><b>1.3.2</b> Anecdotal evidence</a></li>
<li class="chapter" data-level="1.3.3" data-path="introductionToData.html"><a href="introductionToData.html#sampling-from-a-population"><i class="fa fa-check"></i><b>1.3.3</b> Sampling from a population</a></li>
<li class="chapter" data-level="1.3.4" data-path="introductionToData.html"><a href="introductionToData.html#explanatoryAndResponse"><i class="fa fa-check"></i><b>1.3.4</b> Explanatory and response variables</a></li>
<li class="chapter" data-level="1.3.5" data-path="introductionToData.html"><a href="introductionToData.html#introducing-observational-studies-and-experiments"><i class="fa fa-check"></i><b>1.3.5</b> Introducing observational studies and experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introductionToData.html"><a href="introductionToData.html#observational-studies-and-sampling-strategies"><i class="fa fa-check"></i><b>1.4</b> Observational studies and sampling strategies</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introductionToData.html"><a href="introductionToData.html#observational-studies"><i class="fa fa-check"></i><b>1.4.1</b> Observational studies</a></li>
<li class="chapter" data-level="1.4.2" data-path="introductionToData.html"><a href="introductionToData.html#threeSamplingMethods"><i class="fa fa-check"></i><b>1.4.2</b> Three sampling methods (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introductionToData.html"><a href="introductionToData.html#experimentsSection"><i class="fa fa-check"></i><b>1.5</b> Experiments</a><ul>
<li class="chapter" data-level="1.5.1" data-path="introductionToData.html"><a href="introductionToData.html#experimentalDesignPrinciples"><i class="fa fa-check"></i><b>1.5.1</b> Principles of experimental design</a></li>
<li class="chapter" data-level="1.5.2" data-path="introductionToData.html"><a href="introductionToData.html#biasInHumanExperiments"><i class="fa fa-check"></i><b>1.5.2</b> Reducing bias in human experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introductionToData.html"><a href="introductionToData.html#numericalData"><i class="fa fa-check"></i><b>1.6</b> Examining numerical data</a><ul>
<li class="chapter" data-level="1.6.1" data-path="introductionToData.html"><a href="introductionToData.html#scatterPlots"><i class="fa fa-check"></i><b>1.6.1</b> Scatterplots for paired data</a></li>
<li class="chapter" data-level="1.6.2" data-path="introductionToData.html"><a href="introductionToData.html#dotPlot"><i class="fa fa-check"></i><b>1.6.2</b> Dot plots and the mean</a></li>
<li class="chapter" data-level="1.6.3" data-path="introductionToData.html"><a href="introductionToData.html#histogramsAndShape"><i class="fa fa-check"></i><b>1.6.3</b> Histograms and shape</a></li>
<li class="chapter" data-level="1.6.4" data-path="introductionToData.html"><a href="introductionToData.html#variability"><i class="fa fa-check"></i><b>1.6.4</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="1.6.5" data-path="introductionToData.html"><a href="introductionToData.html#box-plots-quartiles-and-the-median"><i class="fa fa-check"></i><b>1.6.5</b> Box plots, quartiles, and the median</a></li>
<li class="chapter" data-level="1.6.6" data-path="introductionToData.html"><a href="introductionToData.html#robust-statistics"><i class="fa fa-check"></i><b>1.6.6</b> Robust statistics</a></li>
<li class="chapter" data-level="1.6.7" data-path="introductionToData.html"><a href="introductionToData.html#transformingDataSubsection"><i class="fa fa-check"></i><b>1.6.7</b> Transforming data (special topic)</a></li>
<li class="chapter" data-level="1.6.8" data-path="introductionToData.html"><a href="introductionToData.html#mapping-data-special-topic"><i class="fa fa-check"></i><b>1.6.8</b> Mapping data (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introductionToData.html"><a href="introductionToData.html#categoricalData"><i class="fa fa-check"></i><b>1.7</b> Considering categorical data</a><ul>
<li class="chapter" data-level="1.7.1" data-path="introductionToData.html"><a href="introductionToData.html#contingency-tables-and-bar-plots"><i class="fa fa-check"></i><b>1.7.1</b> Contingency tables and bar plots</a></li>
<li class="chapter" data-level="1.7.2" data-path="introductionToData.html"><a href="introductionToData.html#row-and-column-proportions"><i class="fa fa-check"></i><b>1.7.2</b> Row and column proportions</a></li>
<li class="chapter" data-level="1.7.3" data-path="introductionToData.html"><a href="introductionToData.html#segmentedBarPlotsAndIndependence"><i class="fa fa-check"></i><b>1.7.3</b> Segmented bar and mosaic plots</a></li>
<li class="chapter" data-level="1.7.4" data-path="introductionToData.html"><a href="introductionToData.html#the-only-pie-chart-you-will-see-in-this-book"><i class="fa fa-check"></i><b>1.7.4</b> The only pie chart you will see in this book</a></li>
<li class="chapter" data-level="1.7.5" data-path="introductionToData.html"><a href="introductionToData.html#comparingAcrossGroups"><i class="fa fa-check"></i><b>1.7.5</b> Comparing numerical data across groups</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="FoundationForInference.html"><a href="FoundationForInference.html"><i class="fa fa-check"></i><b>2</b> Foundation for inference</a><ul>
<li class="chapter" data-level="2.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#caseStudyGenderDiscrimination"><i class="fa fa-check"></i><b>2.1</b> Randomization case study: gender discrimination</a><ul>
<li class="chapter" data-level="2.1.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#variabilityWithinData"><i class="fa fa-check"></i><b>2.1.1</b> Variability within data</a></li>
<li class="chapter" data-level="2.1.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#simulatingTheStudy"><i class="fa fa-check"></i><b>2.1.2</b> Simulating the study</a></li>
<li class="chapter" data-level="2.1.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#checking-for-independence"><i class="fa fa-check"></i><b>2.1.3</b> Checking for independence</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#caseStudyOpportunityCost"><i class="fa fa-check"></i><b>2.2</b> Randomization case study: opportunity cost</a><ul>
<li class="chapter" data-level="2.2.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#exploring-the-data-set-before-the-analysis"><i class="fa fa-check"></i><b>2.2.1</b> Exploring the data set before the analysis</a></li>
<li class="chapter" data-level="2.2.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#results-from-chance-alone"><i class="fa fa-check"></i><b>2.2.2</b> Results from chance alone</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#HypothesisTesting"><i class="fa fa-check"></i><b>2.3</b> Hypothesis testing</a><ul>
<li class="chapter" data-level="2.3.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#hypothesis-testing-in-the-us-court-system"><i class="fa fa-check"></i><b>2.3.1</b> Hypothesis testing in the US court system</a></li>
<li class="chapter" data-level="2.3.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#p-value-and-statistical-significance"><i class="fa fa-check"></i><b>2.3.2</b> p-value and statistical significance</a></li>
<li class="chapter" data-level="2.3.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#decision-errors"><i class="fa fa-check"></i><b>2.3.3</b> Decision errors</a></li>
<li class="chapter" data-level="2.3.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#significanceLevel"><i class="fa fa-check"></i><b>2.3.4</b> Choosing a significance level</a></li>
<li class="chapter" data-level="2.3.5" data-path="FoundationForInference.html"><a href="FoundationForInference.html#IntroducingTwoSidedHypotheses"><i class="fa fa-check"></i><b>2.3.5</b> Introducing two-sided hypotheses</a></li>
<li class="chapter" data-level="2.3.6" data-path="FoundationForInference.html"><a href="FoundationForInference.html#InflatingType1ErrorRate"><i class="fa fa-check"></i><b>2.3.6</b> Controlling the Type 1 Error rate</a></li>
<li class="chapter" data-level="2.3.7" data-path="FoundationForInference.html"><a href="FoundationForInference.html#how-to-use-a-hypothesis-test"><i class="fa fa-check"></i><b>2.3.7</b> How to use a hypothesis test</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#SimulationCaseStudies"><i class="fa fa-check"></i><b>2.4</b> Simulation case studies</a><ul>
<li class="chapter" data-level="2.4.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#medical-consultant"><i class="fa fa-check"></i><b>2.4.1</b> Medical consultant</a></li>
<li class="chapter" data-level="2.4.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#tappers-and-listeners"><i class="fa fa-check"></i><b>2.4.2</b> Tappers and listeners</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="FoundationForInference.html"><a href="FoundationForInference.html#CLTsection"><i class="fa fa-check"></i><b>2.5</b> Central Limit Theorem</a><ul>
<li class="chapter" data-level="2.5.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#null-distribution-from-the-case-studies"><i class="fa fa-check"></i><b>2.5.1</b> Null distribution from the case studies</a></li>
<li class="chapter" data-level="2.5.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#examples-of-future-settings-we-will-consider"><i class="fa fa-check"></i><b>2.5.2</b> Examples of future settings we will consider</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normalDist"><i class="fa fa-check"></i><b>2.6</b> Normal distribution</a><ul>
<li class="chapter" data-level="2.6.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#NormalDistributionModelSubsection"><i class="fa fa-check"></i><b>2.6.1</b> Normal distribution model</a></li>
<li class="chapter" data-level="2.6.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#standardizing-with-z-scores"><i class="fa fa-check"></i><b>2.6.2</b> Standardizing with Z scores</a></li>
<li class="chapter" data-level="2.6.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-probability-table"><i class="fa fa-check"></i><b>2.6.3</b> Normal probability table</a></li>
<li class="chapter" data-level="2.6.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-probability-examples"><i class="fa fa-check"></i><b>2.6.4</b> Normal probability examples</a></li>
<li class="chapter" data-level="2.6.5" data-path="FoundationForInference.html"><a href="FoundationForInference.html#rule"><i class="fa fa-check"></i><b>2.6.5</b> 68-95-99.7 rule</a></li>
<li class="chapter" data-level="2.6.6" data-path="FoundationForInference.html"><a href="FoundationForInference.html#assessingNormal"><i class="fa fa-check"></i><b>2.6.6</b> Evaluating the normal approximation</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="FoundationForInference.html"><a href="FoundationForInference.html#ApplyingTheNormalModel"><i class="fa fa-check"></i><b>2.7</b> Applying the normal model</a><ul>
<li class="chapter" data-level="2.7.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#standard-error"><i class="fa fa-check"></i><b>2.7.1</b> Standard error</a></li>
<li class="chapter" data-level="2.7.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-model-application-opportunity-cost"><i class="fa fa-check"></i><b>2.7.2</b> Normal model application: opportunity cost</a></li>
<li class="chapter" data-level="2.7.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#normal-model-application-medical-consultant"><i class="fa fa-check"></i><b>2.7.3</b> Normal model application: medical consultant</a></li>
<li class="chapter" data-level="2.7.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#conditions-for-applying-the-normal-model"><i class="fa fa-check"></i><b>2.7.4</b> Conditions for applying the normal model</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="FoundationForInference.html"><a href="FoundationForInference.html#ConfidenceIntervals"><i class="fa fa-check"></i><b>2.8</b> Confidence intervals</a><ul>
<li class="chapter" data-level="2.8.1" data-path="FoundationForInference.html"><a href="FoundationForInference.html#capturing-the-population-parameter"><i class="fa fa-check"></i><b>2.8.1</b> Capturing the population parameter</a></li>
<li class="chapter" data-level="2.8.2" data-path="FoundationForInference.html"><a href="FoundationForInference.html#constructing-a-95-confidence-interval"><i class="fa fa-check"></i><b>2.8.2</b> Constructing a 95% confidence interval</a></li>
<li class="chapter" data-level="2.8.3" data-path="FoundationForInference.html"><a href="FoundationForInference.html#changingTheConfidenceLevelSection"><i class="fa fa-check"></i><b>2.8.3</b> Changing the confidence level</a></li>
<li class="chapter" data-level="2.8.4" data-path="FoundationForInference.html"><a href="FoundationForInference.html#interpretingCIs"><i class="fa fa-check"></i><b>2.8.4</b> Interpreting confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html"><i class="fa fa-check"></i><b>3</b> Inference for categorical data</a><ul>
<li class="chapter" data-level="3.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#singleProportion"><i class="fa fa-check"></i><b>3.1</b> Inference for a single proportion</a><ul>
<li class="chapter" data-level="3.1.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#when-the-sample-proportion-is-nearly-normal"><i class="fa fa-check"></i><b>3.1.1</b> When the sample proportion is nearly normal</a></li>
<li class="chapter" data-level="3.1.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#confIntForPropSection"><i class="fa fa-check"></i><b>3.1.2</b> Confidence intervals for a proportion</a></li>
<li class="chapter" data-level="3.1.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#htForPropSection"><i class="fa fa-check"></i><b>3.1.3</b> Hypothesis testing for a proportion</a></li>
<li class="chapter" data-level="3.1.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#choosing-a-sample-size-when-estimating-a-proportion"><i class="fa fa-check"></i><b>3.1.4</b> Choosing a sample size when estimating a proportion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#differenceOfTwoProportions"><i class="fa fa-check"></i><b>3.2</b> Difference of two proportions</a><ul>
<li class="chapter" data-level="3.2.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#SampleDistributionOfTheDiffOfTwoProportions"><i class="fa fa-check"></i><b>3.2.1</b> Sample distribution of the difference of two proportions</a></li>
<li class="chapter" data-level="3.2.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#intervals-and-tests-for-p_1--p_2"><i class="fa fa-check"></i><b>3.2.2</b> Intervals and tests for <span class="math inline">\(p_1 -p_2\)</span></a></li>
<li class="chapter" data-level="3.2.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pooledHTForProportionsSection"><i class="fa fa-check"></i><b>3.2.3</b> Hypothesis testing when <span class="math inline">\(H_0: p_1=p_2\)</span></a></li>
</ul></li>
<li><a href="inferenceForCategoricalData.html#oneWayChiSquare"><span class="toc-section-number">3.3</span> Testing for goodness of fit using chi-square<br />
(special topic)</a><ul>
<li class="chapter" data-level="3.3.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#creating-a-test-statistic-for-one-way-tables"><i class="fa fa-check"></i><b>3.3.1</b> Creating a test statistic for one-way tables</a></li>
<li class="chapter" data-level="3.3.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#chiSquareTestStatistic"><i class="fa fa-check"></i><b>3.3.2</b> The chi-square test statistic</a></li>
<li class="chapter" data-level="3.3.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-distribution-and-finding-areas"><i class="fa fa-check"></i><b>3.3.3</b> The chi-square distribution and finding areas</a></li>
<li class="chapter" data-level="3.3.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pValueForAChiSquareTest"><i class="fa fa-check"></i><b>3.3.4</b> Finding a p-value for a chi-square distribution</a></li>
<li class="chapter" data-level="3.3.5" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#evaluating-goodness-of-fit-for-a-distribution"><i class="fa fa-check"></i><b>3.3.5</b> Evaluating goodness of fit for a distribution</a></li>
</ul></li>
<li><a href="inferenceForCategoricalData.html#twoWayTablesAndChiSquare"><span class="toc-section-number">3.4</span> Testing for independence in two-way tables<br />
(special topic)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#expected-counts-in-two-way-tables"><i class="fa fa-check"></i><b>3.4.1</b> Expected counts in two-way tables</a></li>
<li class="chapter" data-level="3.4.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-test-for-two-way-tables"><i class="fa fa-check"></i><b>3.4.2</b> The chi-square test for two-way tables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html"><i class="fa fa-check"></i><b>4</b> Inference for numerical data</a><ul>
<li class="chapter" data-level="4.1" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#oneSampleMeansWithTDistribution"><i class="fa fa-check"></i><b>4.1</b> One-sample means with the <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="4.1.1" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#two-examples-using-the-normal-distribution"><i class="fa fa-check"></i><b>4.1.1</b> Two examples using the normal distribution</a></li>
<li class="chapter" data-level="4.1.2" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#introducingTheTDistribution"><i class="fa fa-check"></i><b>4.1.2</b> Introducing the <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="4.1.3" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#tDistSolutionToSEProblem"><i class="fa fa-check"></i><b>4.1.3</b> Applying the <span class="math inline">\(t\)</span> distribution to the single-mean situation</a></li>
<li class="chapter" data-level="4.1.4" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#oneSampleTConfidenceIntervals"><i class="fa fa-check"></i><b>4.1.4</b> One sample <span class="math inline">\(t\)</span> confidence intervals</a></li>
<li class="chapter" data-level="4.1.5" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#oneSampleTTests"><i class="fa fa-check"></i><b>4.1.5</b> One sample <span class="math inline">\(t\)</span> tests</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#pairedData"><i class="fa fa-check"></i><b>4.2</b> Paired data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#paired-observations"><i class="fa fa-check"></i><b>4.2.1</b> Paired observations</a></li>
<li class="chapter" data-level="4.2.2" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#inference-for-paired-data"><i class="fa fa-check"></i><b>4.2.2</b> Inference for paired data</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#differenceOfTwoMeans"><i class="fa fa-check"></i><b>4.3</b> Difference of two means</a><ul>
<li class="chapter" data-level="4.3.1" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#confidence-interval-for-a-differences-of-means"><i class="fa fa-check"></i><b>4.3.1</b> Confidence interval for a differences of means</a></li>
<li class="chapter" data-level="4.3.2" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#hypothesis-tests-based-on-a-difference-in-means"><i class="fa fa-check"></i><b>4.3.2</b> Hypothesis tests based on a difference in means</a></li>
<li class="chapter" data-level="4.3.3" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#case-study-two-versions-of-a-course-exam"><i class="fa fa-check"></i><b>4.3.3</b> Case study: two versions of a course exam</a></li>
<li class="chapter" data-level="4.3.4" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#summary-for-inference-using-the-t-distribution"><i class="fa fa-check"></i><b>4.3.4</b> Summary for inference using the <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="4.3.5" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#pooledStandardDeviations"><i class="fa fa-check"></i><b>4.3.5</b> Pooled standard deviation estimate (special topic)</a></li>
</ul></li>
<li><a href="inferenceForNumericalData.html#anovaAndRegrWithCategoricalVariables"><span class="toc-section-number">4.4</span> Comparing many means with ANOVA<br />
(special topic)</a><ul>
<li class="chapter" data-level="4.4.1" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#is-batting-performance-related-to-player-position-in-mlb"><i class="fa fa-check"></i><b>4.4.1</b> Is batting performance related to player position in MLB?</a></li>
<li class="chapter" data-level="4.4.2" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#analysis-of-variance-anova-and-the-f-test"><i class="fa fa-check"></i><b>4.4.2</b> Analysis of variance (ANOVA) and the F test</a></li>
<li class="chapter" data-level="4.4.3" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#reading-an-anova-table-from-software"><i class="fa fa-check"></i><b>4.4.3</b> Reading an ANOVA table from software</a></li>
<li class="chapter" data-level="4.4.4" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#graphical-diagnostics-for-an-anova-analysis"><i class="fa fa-check"></i><b>4.4.4</b> Graphical diagnostics for an ANOVA analysis</a></li>
<li class="chapter" data-level="4.4.5" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#multipleComparisonsAndControllingTheType1ErrorRate"><i class="fa fa-check"></i><b>4.4.5</b> Multiple comparisons and controlling Type 1 Error rate</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#bootstrapping-to-study-the-standard-deviation"><i class="fa fa-check"></i><b>4.5</b> Bootstrapping to study the standard deviation</a><ul>
<li class="chapter" data-level="4.5.1" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#bootstrap-samples-and-distributions"><i class="fa fa-check"></i><b>4.5.1</b> Bootstrap samples and distributions</a></li>
<li class="chapter" data-level="4.5.2" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#inference-using-the-bootstrap"><i class="fa fa-check"></i><b>4.5.2</b> Inference using the bootstrap</a></li>
<li class="chapter" data-level="4.5.3" data-path="inferenceForNumericalData.html"><a href="inferenceForNumericalData.html#frequently-asked-questions"><i class="fa fa-check"></i><b>4.5.3</b> Frequently asked questions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html"><i class="fa fa-check"></i><b>5</b> Introduction to linear regression</a><ul>
<li class="chapter" data-level="5.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#lineFittingResidualsCorrelation"><i class="fa fa-check"></i><b>5.1</b> Line fitting, residuals, and correlation</a><ul>
<li class="chapter" data-level="5.1.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#beginning-with-straight-lines"><i class="fa fa-check"></i><b>5.1.1</b> Beginning with straight lines</a></li>
<li class="chapter" data-level="5.1.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#fitting-a-line-by-eye"><i class="fa fa-check"></i><b>5.1.2</b> Fitting a line by eye</a></li>
<li class="chapter" data-level="5.1.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#residuals"><i class="fa fa-check"></i><b>5.1.3</b> Residuals</a></li>
<li class="chapter" data-level="5.1.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#describing-linear-relationships-with-correlation"><i class="fa fa-check"></i><b>5.1.4</b> Describing linear relationships with correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#fittingALineByLSR"><i class="fa fa-check"></i><b>5.2</b> Fitting a line by least squares regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#an-objective-measure-for-finding-the-best-line"><i class="fa fa-check"></i><b>5.2.1</b> An objective measure for finding the best line</a></li>
<li class="chapter" data-level="5.2.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#findingTheLeastSquaresLineSection"><i class="fa fa-check"></i><b>5.2.2</b> Finding the least squares line</a></li>
<li class="chapter" data-level="5.2.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#interpreting-regression-line-parameter-estimates"><i class="fa fa-check"></i><b>5.2.3</b> Interpreting regression line parameter estimates</a></li>
<li class="chapter" data-level="5.2.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#extrapolation-is-treacherous"><i class="fa fa-check"></i><b>5.2.4</b> Extrapolation is treacherous</a></li>
<li class="chapter" data-level="5.2.5" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#using-r2-to-describe-the-strength-of-a-fit"><i class="fa fa-check"></i><b>5.2.5</b> Using <span class="math inline">\(R^2\)</span> to describe the strength of a fit</a></li>
<li class="chapter" data-level="5.2.6" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#categoricalPredictorsWithTwoLevels"><i class="fa fa-check"></i><b>5.2.6</b> Categorical predictors with two levels</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#typesOfOutliersInLinearRegression"><i class="fa fa-check"></i><b>5.3</b> Types of outliers in linear regression</a></li>
<li class="chapter" data-level="5.4" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#inferenceForLinearRegression"><i class="fa fa-check"></i><b>5.4</b> Inference for linear regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#conditions-for-the-least-squares-line"><i class="fa fa-check"></i><b>5.4.1</b> Conditions for the least squares line</a></li>
<li class="chapter" data-level="5.4.2" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#midterm-elections-and-unemployment"><i class="fa fa-check"></i><b>5.4.2</b> Midterm elections and unemployment</a></li>
<li class="chapter" data-level="5.4.3" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html#testStatisticForTheSlope"><i class="fa fa-check"></i><b>5.4.3</b> Understanding regression output from software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html"><i class="fa fa-check"></i><b>6</b> Multiple and logistic regression</a><ul>
<li class="chapter" data-level="6.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#introductionToMultipleRegression"><i class="fa fa-check"></i><b>6.1</b> Introduction to multiple regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#twoSingleVariableModelsForMarioKartData"><i class="fa fa-check"></i><b>6.1.1</b> A single-variable model for the Mario Kart data</a></li>
<li class="chapter" data-level="6.1.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#includingAndAssessingManyVariablesInAModel"><i class="fa fa-check"></i><b>6.1.2</b> Including and assessing many variables in a model</a></li>
<li class="chapter" data-level="6.1.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#adjusted-r2-as-a-better-estimate-of-explained-variance"><i class="fa fa-check"></i><b>6.1.3</b> Adjusted <span class="math inline">\(R^2\)</span> as a better estimate of explained variance</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelSelection"><i class="fa fa-check"></i><b>6.2</b> Model selection</a><ul>
<li class="chapter" data-level="6.2.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#identifying-variables-in-the-model-that-may-not-be-helpful"><i class="fa fa-check"></i><b>6.2.1</b> Identifying variables in the model that may not be helpful</a></li>
<li class="chapter" data-level="6.2.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#two-model-selection-strategies"><i class="fa fa-check"></i><b>6.2.2</b> Two model selection strategies</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#multipleRegressionModelAssumptions"><i class="fa fa-check"></i><b>6.3</b> Checking model assumptions using graphs</a></li>
<li class="chapter" data-level="6.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#logisticRegression"><i class="fa fa-check"></i><b>6.4</b> Logistic regression</a><ul>
<li class="chapter" data-level="6.4.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#email-data"><i class="fa fa-check"></i><b>6.4.1</b> Email data</a></li>
<li class="chapter" data-level="6.4.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelingTheProbabilityOfAnEvent"><i class="fa fa-check"></i><b>6.4.2</b> Modeling the probability of an event</a></li>
<li class="chapter" data-level="6.4.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#practical-decisions-in-the-email-application"><i class="fa fa-check"></i><b>6.4.3</b> Practical decisions in the email application</a></li>
<li class="chapter" data-level="6.4.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#diagnostics-for-the-email-classifier"><i class="fa fa-check"></i><b>6.4.4</b> Diagnostics for the email classifier</a></li>
<li class="chapter" data-level="6.4.5" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#improvingTheSetOfVariablesForASpamFilter"><i class="fa fa-check"></i><b>6.4.5</b> Improving the set of variables for a spam filter</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Answering questions with data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferenceForNumericalData" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Inference for numerical data</h1>
<p>Chapters [FoundationForInference] and [inferenceForCategoricalData] introduced us to inference for proportions using the normal model, and in Section [oneWayChiSquare], we encountered the chi-square distribution, which is useful for working with categorical data with many levels. In this chapter, our focus will be on numerical data, where we will encounter two more distributions: the <span class="math inline">\(t\)</span> distribution (looks a lot like the normal distribution) and the <span class="math inline">\(F\)</span> distribution. Our general approach will be:</p>
<ol style="list-style-type: decimal">
<li><p>Determine which point estimate or test statistic is useful.</p></li>
<li><p>Identify an appropriate distribution for the point estimate or test statistic.</p></li>
<li><p>Apply the hypothesis and confidence interval techniques from Chapter [FoundationForInference] using the distribution from step 2.</p></li>
</ol>
<div id="oneSampleMeansWithTDistribution" class="section level2">
<h2><span class="header-section-number">4.1</span> One-sample means with the <span class="math inline">\(t\)</span> distribution</h2>
<p>The sampling distribution associated with a sample mean or difference of two sample means is, if certain conditions are satisfied, nearly normal. However, this becomes more complex when the sample size is small, where <em>small</em> here typically means a sample size smaller than 30 observations. For this reason, we’ll use a new distribution called the <span class="math inline">\(t\)</span> distribution that will often work for both small and large samples of numerical data.</p>
<div id="two-examples-using-the-normal-distribution" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Two examples using the normal distribution</h3>
<p>Before we get started with the <span class="math inline">\(t\)</span> distribution, let’s take a look at two applications where it is okay to use the normal model for the sample mean. For the case of a single mean, the standard error of the sample mean can be calculated as</p>
<p><span class="math display">\[\begin{aligned}
SE = \frac{\sigma}{\sqrt{n}}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the population standard deviation and <span class="math inline">\(n\)</span> is the sample size. Generally we use the sample standard deviation, denoted by <span class="math inline">\(s\)</span>, in place of the population standard deviation when we compute the standard error:</p>
<p><span class="math display">\[\begin{aligned}
SE \approx \frac{s}{\sqrt{n}}\end{aligned}\]</span></p>
<p>If we look at this formula, there are some characteristics that we can think about intuitively.</p>
<ul>
<li><p>If we examine the standard error formula, we would see that a larger <span class="math inline">\(s\)</span> corresponds to a larger <span class="math inline">\(SE\)</span>. This makes intuitive sense: if the data are more volatile, then we’ll be less certain of the location of the true mean, so the standard error should be bigger. On the other hand, if the observations all fall very close together, then <span class="math inline">\(s\)</span> will be small, and the sample mean should be a more precise estimate of the true mean.</p></li>
<li><p>In the formula, the larger the sample size <span class="math inline">\(n\)</span>, the smaller the standard error. This matches our intuition: we expect estimates to be more precise when we have more data, so the standard error <span class="math inline">\(SE\)</span> should get smaller when <span class="math inline">\(n\)</span> gets bigger.</p></li>
</ul>
<p>As we did with proportions, we’ll also need to check a few conditions before using the normal model. We’ll forgo describing those details until later this section, but these conditions have been verified for the two examples below.</p>
<p><span>We’ve taken a random sample of 100 runners from a race called the Cherry Blossom Run in Washington, DC, which was a race with 16,924 participants.<a href="#fn123" class="footnoteRef" id="fnref123"><sup>123</sup></a> The sample data for the 100 runners is summarized in Table [run10SampDF], histograms of the run time and age of participants are in Figure [run10SampHistograms], and summary statistics are available in Table [ptEstimatesNetTimeAge]. Create a 95% confidence interval for the average time it takes runners in the Cherry Blossom Run to complete the race.</span> We can use the same confidence interval formula for the mean that we used for a proportion:</p>
<p><span class="math display">\[\begin{aligned}
\text{point estimate}\ \pm\ 1.96 \times SE\end{aligned}\]</span></p>
<p>In this case, the best estimate of the overall mean is the sample mean, <span class="math inline">\(\bar{x} = 95.61\)</span> minutes. The standard error can be calculated using sample standard deviation (<span class="math inline">\(s = 15.78\)</span>), the sample size (<span class="math inline">\(n=100\)</span>), and the standard error formula:</p>
<p><span class="math display">\[\begin{aligned}
SE = \frac{s}{\sqrt{n}} = \frac{15.78}{\sqrt{100}} = 1.578\end{aligned}\]</span></p>
<p>Finally, we can calculate a 95% confidence interval:</p>
<p><span class="math display">\[\begin{aligned}
\text{point estimate}\ \pm\ z^{\star} \times SE \quad\rightarrow\quad
95.61 \pm 1.96 \times 1.578 \quad\rightarrow\quad
(92.52, 98.70)\end{aligned}\]</span></p>
<p>We are 95% confident that the average time for all runners in the 2012 Cherry Blossom Run is between 92.52 and 98.70 minutes.</p>
<table>
<caption>Four observations for the <strong>run10Samp</strong> data set, which represents a simple random sample of 100 runners from the 2012 Cherry Blossom Run.</caption>
<thead>
<tr class="header">
<th align="right">ID</th>
<th align="right">time</th>
<th align="right">age</th>
<th align="right">gender</th>
<th align="right">state</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">88.31</td>
<td align="right">59</td>
<td align="right">M</td>
<td align="right">MD</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">100.67</td>
<td align="right">32</td>
<td align="right">M</td>
<td align="right">VA</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">109.52</td>
<td align="right">33</td>
<td align="right">F</td>
<td align="right">VA</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="right">100</td>
<td align="right">89.49</td>
<td align="right">26</td>
<td align="right">M</td>
<td align="right">DC</td>
</tr>
</tbody>
</table>
<p>[run10SampDF]</p>
<div class="figure">
<img src="04/figures/run10SampHistograms/run10SampHistograms" alt="Histograms of time and age for the sample Cherry Blossom Run data. The average time is in the mid-90s, and the average age is in the mid-30s. The age distribution is moderately skewed to the right." />
<p class="caption">Histograms of <strong>time</strong> and <strong>age</strong> for the sample Cherry Blossom Run data. The average time is in the mid-90s, and the average age is in the mid-30s. The age distribution is moderately skewed to the right.</p>
</div>
<p>[run10SampHistograms]</p>
<table>
<caption>Point estimates and parameter values for the <strong>time</strong> variable.</caption>
<tbody>
<tr class="odd">
<td align="left">sample mean</td>
<td align="right">95.61</td>
<td align="right">35.05</td>
</tr>
<tr class="even">
<td align="left">sample median</td>
<td align="right">95.37</td>
<td align="right">32.50</td>
</tr>
<tr class="odd">
<td align="left">sample st. dev.</td>
<td align="right">15.78</td>
<td align="right">8.97</td>
</tr>
</tbody>
</table>
<p>[ptEstimatesNetTimeAge]</p>
<p>Use the data to calculate a 90% confidence interval for the average age of participants in the 2012 Cherry Blossom Run. The conditions for applying the normal model have already been verified.<a href="#fn124" class="footnoteRef" id="fnref124"><sup>124</sup></a></p>
<p><span>The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. Is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips? The conditions necessary for applying the normal model have been checked and are satisfied.</span> The question has been framed in terms of two possibilities: the nutrition label accurately lists the correct average calories per bag of chips or it does not, which may be framed as a hypothesis test:</p>
<ul>
<li><p>The average is listed correctly. <span class="math inline">\(\mu = 130\)</span></p></li>
<li><p>The nutrition label is incorrect. <span class="math inline">\(\mu \neq 130\)</span></p></li>
</ul>
<p>The observed average is <span class="math inline">\(\bar{x} = 134\)</span> and the standard error may be calculated as <span class="math inline">\(SE = \frac{17}{\sqrt{35}} = 2.87\)</span>. First, we draw a picture summarizing this scenario.</p>
<div class="figure">
<img src="04/figures/potatoChips/potatoChips" alt="image" />
<p class="caption">image</p>
</div>
<p>We can compute a test statistic as the Z score:</p>
<p><span class="math display">\[\begin{aligned}
Z = \frac{134 - 130}{2.87} = 1.39\end{aligned}\]</span></p>
<p>The upper-tail area is 0.0823, so the p-value is <span class="math inline">\(2 \times 0.0823 = 0.1646\)</span>. Since the p-value is larger than 0.05, we do not reject the null hypothesis. That is, there is not enough evidence to show the nutrition label has incorrect information.</p>
<p>The normal model works well when the sample size is larger than about 30. For smaller sample sizes, we run into a problem: our estimate of <span class="math inline">\(s\)</span>, which is used to compute the standard error, isn’t as reliable when the sample size is small. To solve this problem, we’ll use a new distribution: the <span class="math inline">\(t\)</span> distribution.</p>
</div>
<div id="introducingTheTDistribution" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Introducing the <span class="math inline">\(t\)</span> distribution</h3>
<p>A <span class="math inline">\(t\)</span> distribution, shown as a solid line in Figure [tDistCompareToNormalDist], has a bell shape that looks very similar to a normal distribution (dotted line). However, its tails are thicker, which means observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution.<a href="#fn125" class="footnoteRef" id="fnref125"><sup>125</sup></a> When our sample is small, the value <span class="math inline">\(s\)</span> used to compute the standard error isn’t very reliable. The extra thick tails of the <span class="math inline">\(t\)</span> distribution are exactly the correction we need to resolve this problem.</p>
<div class="figure">
<img src="04/figures/tDistCompareToNormalDist/tDistCompareToNormalDist" alt="Comparison of a t distribution (solid line) and a normal distribution (dotted line)." />
<p class="caption">Comparison of a <span class="math inline">\(t\)</span> distribution (solid line) and a normal distribution (dotted line).</p>
</div>
<p>[tDistCompareToNormalDist]</p>
<p>The <span class="math inline">\(t\)</span> distribution, always centered at zero, has a single parameter: degrees of freedom. The describe the precise form of the bell-shaped <span class="math inline">\(t\)</span> distribution. Several <span class="math inline">\(t\)</span> distributions are shown in Figure [tDistConvergeToNormalDist] with various degrees of freedom. When there are more degrees of freedom, the <span class="math inline">\(t\)</span> distribution looks very much like the standard normal distribution.</p>
<div class="figure">
<img src="04/figures/tDistConvergeToNormalDist/tDistConvergeToNormalDist" alt="The larger the degrees of freedom, the more closely the t distribution resembles the standard normal model." />
<p class="caption">The larger the degrees of freedom, the more closely the <span class="math inline">\(t\)</span> distribution resembles the standard normal model.</p>
</div>
<p>[tDistConvergeToNormalDist]</p>
<p><span> The degrees of freedom describe the shape of the <span class="math inline">\(t\)</span> distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model.</span></p>
<p>When the degrees of freedom is about 30 or more, the <span class="math inline">\(t\)</span> distribution is nearly indistinguishable from the normal distribution, e.g. see Figure [tDistConvergeToNormalDist]. In Section [tDistSolutionToSEProblem], we relate degrees of freedom to sample size.</p>
<p>We will find it very useful to become familiar with the <span class="math inline">\(t\)</span> distribution, because it plays a very similar role to the normal distribution during inference for numerical data. We use a , partially shown in Table [tTableSample], in place of the normal probability table for small sample numerical data. A larger table is presented in Appendix . Alternatively, we could use statistical software to get this same information.</p>
<p><span>r | rrr rr</span> one tail &amp;</p>
<p>0.100 &amp;</p>
<p>0.050 &amp;</p>
<p>0.025 &amp;</p>
<p>0.010 &amp;</p>
<p>0.005<br />
two tails &amp; 0.200 &amp; 0.100 &amp; 0.050 &amp; 0.020 &amp; 0.010<br />
1 &amp; <span>3.08</span> &amp; <span>6.31</span> &amp; <span>12.71</span> &amp; <span>31.82</span> &amp; <span>63.66</span><br />
2 &amp; <span>1.89</span> &amp; <span>2.92</span> &amp; <span>4.30</span> &amp; <span>6.96</span> &amp; <span>9.92</span><br />
3 &amp; <span>1.64</span> &amp; <span>2.35</span> &amp; <span>3.18</span> &amp; <span>4.54</span> &amp; <span>5.84</span><br />
<span class="math inline">\(\vdots\)</span> &amp; <span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<br />
17 &amp; <span>1.33</span> &amp; <span>1.74</span> &amp; <span>2.11</span> &amp; <span>2.57</span> &amp; <span>2.90</span><br />
&amp; &amp; &amp; &amp; &amp;<br />
19 &amp; <span>1.33</span> &amp; <span>1.73</span> &amp; <span>2.09</span> &amp; <span>2.54</span> &amp; <span>2.86</span><br />
20 &amp; <span>1.33</span> &amp; <span>1.72</span> &amp; <span>2.09</span> &amp; <span>2.53</span> &amp; <span>2.85</span><br />
<span class="math inline">\(\vdots\)</span> &amp; <span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<br />
400 &amp; <span>1.28</span> &amp; <span>1.65</span> &amp; <span>1.97</span> &amp; <span>2.34</span> &amp; <span>2.59</span><br />
500 &amp; <span>1.28</span> &amp; <span>1.65</span> &amp; <span>1.96</span> &amp; <span>2.33</span> &amp; <span>2.59</span><br />
<span class="math inline">\(\infty\)</span> &amp; <span>1.28</span> &amp; <span>1.65</span> &amp; <span>1.96</span> &amp; <span>2.33</span> &amp; <span>2.58</span><br />
[tTableSample]</p>
<p>Each row in the <span class="math inline">\(t\)</span> table represents a <span class="math inline">\(t\)</span> distribution with different degrees of freedom. The columns correspond to tail probabilities. For instance, if we know we are working with the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=18\)</span>, we can examine row 18, which is in Table [tTableSample]. If we want the value in this row that identifies the cutoff for an upper tail of 10%, we can look in the column where <em>one tail</em> is 0.100. This cutoff is 1.33. If we had wanted the cutoff for the lower 10%, we would use -1.33. Just like the normal distribution, all <span class="math inline">\(t\)</span> distributions are symmetric.</p>
<p><span>What proportion of the <span class="math inline">\(t\)</span> distribution with 18 degrees of freedom falls below -2.10?</span> Just like a normal probability problem, we first draw the picture in Figure [tDistDF18LeftTail2Point10] and shade the area below -2.10. To find this area, we identify the appropriate row: <span class="math inline">\(df=18\)</span>. Then we identify the column containing the absolute value of -2.10; it is the third column. Because we are looking for just one tail, we examine the top line of the table, which shows that a one tail area for a value in the third row corresponds to 0.025. About 2.5% of the distribution falls below -2.10. In the next example we encounter a case where the exact <span class="math inline">\(t\)</span> value is not listed in the table.</p>
<div class="figure">
<img src="04/figures/tDistDF18LeftTail2Point10/tDistDF18LeftTail2Point10" alt="The t distribution with 18 degrees of freedom. The area below -2.10 has been shaded." />
<p class="caption">The <span class="math inline">\(t\)</span> distribution with 18 degrees of freedom. The area below -2.10 has been shaded.</p>
</div>
<p>[tDistDF18LeftTail2Point10]</p>
<p><span>A <span class="math inline">\(t\)</span> distribution with 20 degrees of freedom is shown in the left panel of Figure [tDistDF20RightTail1Point65]. Estimate the proportion of the distribution falling above 1.65.</span> We identify the row in the <span class="math inline">\(t\)</span> table using the degrees of freedom: <span class="math inline">\(df=20\)</span>. Then we look for 1.65; it is not listed. It falls between the first and second columns. Since these values bound 1.65, their tail areas will bound the tail area corresponding to 1.65. We identify the one tail area of the first and second columns, 0.050 and 0.10, and we conclude that between 5% and 10% of the distribution is more than 1.65 standard deviations above the mean. If we like, we can identify the precise area using statistical software: 0.0573.</p>
<div class="figure">
<img src="04/figures/tDistDF20RightTail1Point65/tDistDF20RightTail1Point65" alt="Left: The t distribution with 20 degrees of freedom, with the area above 1.65 shaded. Right: The t distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded." />
<p class="caption">Left: The <span class="math inline">\(t\)</span> distribution with 20 degrees of freedom, with the area above 1.65 shaded. Right: The <span class="math inline">\(t\)</span> distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded.</p>
</div>
<p>[tDistDF20RightTail1Point65]</p>
<p><span>A <span class="math inline">\(t\)</span> distribution with 2 degrees of freedom is shown in the right panel of Figure [tDistDF20RightTail1Point65]. Estimate the proportion of the distribution falling more than 3 units from the mean (above or below).</span> As before, first identify the appropriate row: <span class="math inline">\(df=2\)</span>. Next, find the columns that capture 3; because <span class="math inline">\(2.92 &lt; 3 &lt; 4.30\)</span>, we use the second and third columns. Finally, we find bounds for the tail areas by looking at the two tail values: 0.05 and 0.10. We use the two tail values because we are looking for two (symmetric) tails.</p>
<p>What proportion of the <span class="math inline">\(t\)</span> distribution with 19 degrees of freedom falls above -1.79 units?<a href="#fn126" class="footnoteRef" id="fnref126"><sup>126</sup></a></p>
</div>
<div id="tDistSolutionToSEProblem" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Applying the <span class="math inline">\(t\)</span> distribution to the single-mean situation</h3>
<p>When estimating the mean and standard error from a sample of numerical data, the <span class="math inline">\(t\)</span> distribution is a little more accurate than the normal model. This is true for both small and large samples, though the benefits for larger samples are limited.</p>
<p><span> Use the <span class="math inline">\(t\)</span> distribution for inference of the sample mean when observations are independent and nearly normal. You may relax the nearly normal condition as the sample size increases. For example, the data distribution may be moderately skewed when the sample size is at least 30. </span></p>
<p>Before applying the <span class="math inline">\(t\)</span> distribution for inference about a single mean, we check two conditions.</p>
<dl>
<dt>Independence of observations.</dt>
<dd><p>We verify this condition just as we did before. We collect a simple random sample from less than 10% of the population, or if the data are from an experiment or random process, we carefully check to the best of our abilities that the observations were independent.</p>
</dd>
<dt>Observations come from a nearly normal distribution.</dt>
<dd><p>This second condition is difficult to verify with small data sets. We often (i) take a look at a plot of the data for obvious departures from the normal model, usually in the form of prominent outliers, and (ii) consider whether any previous experiences alert us that the data may not be nearly normal. However, if the sample size is somewhat large, then we can relax this condition, e.g. moderate skew is acceptable when the sample size is 30 or more, and strong skew is acceptable when the size is about 60 or more.</p>
</dd>
</dl>
<p>When examining a sample mean and estimated standard error from a sample of <span class="math inline">\(n\)</span> independent and nearly normal observations, we use a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom (<span class="math inline">\(df\)</span>). For example, if the sample size was 19, then we would use the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=19-1=18\)</span> degrees of freedom and proceed in the same way as we did in Chapter [inferenceForCategoricalData], except that <em>now we use the <span class="math inline">\(t\)</span> table</em>.</p>
<p><span> If the sample has <span class="math inline">\(n\)</span> observations and we are examining a single mean, then we use the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=n-1\)</span> degrees of freedom.</span></p>
</div>
<div id="oneSampleTConfidenceIntervals" class="section level3">
<h3><span class="header-section-number">4.1.4</span> One sample <span class="math inline">\(t\)</span> confidence intervals</h3>
<p>Dolphins are at the top of the oceanic food chain, which causes dangerous substances such as mercury to concentrate in their organs and muscles. This is an important problem for both dolphins and other animals, like humans, who occasionally eat them. For instance, this is particularly relevant in Japan where school meals have included dolphin at times.</p>
<p><img src="04/figures/rissosDolphin/rissosDolphin.jpg" title="fig:" alt="A Risso’s dolphin." /><br />
[rissosDolphin]</p>
<p>Here we identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan.<a href="#fn127" class="footnoteRef" id="fnref127"><sup>127</sup></a> The data are summarized in Table [summaryStatsOfHgInMuscleOfRissosDolphins]. The minimum and maximum observed values can be used to evaluate whether or not there are obvious outliers or skew.</p>
<table>
<caption>Summary of mercury content in the muscle of 19 Risso’s dolphins from the Taiji area. Measurements are in <span class="math inline">\(\mu\)</span>g/wet g (micrograms of mercury per wet gram of muscle).</caption>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(n\)</span></td>
<td align="center"><span class="math inline">\(\bar{x}\)</span></td>
<td align="center"><span class="math inline">\(s\)</span></td>
<td align="center">minimum</td>
<td align="center">maximum</td>
</tr>
<tr class="even">
<td align="center">19</td>
<td align="center">4.4</td>
<td align="center">2.3</td>
<td align="center">1.7</td>
<td align="center">9.2</td>
</tr>
</tbody>
</table>
<p>[summaryStatsOfHgInMuscleOfRissosDolphins]</p>
<p><span>Are the independence and normality conditions satisfied for this data set?</span> The observations are a simple random sample and consist of less than 10% of the population, therefore independence is reasonable. Ideally we would see a visualization of the data to check for skew and outliers. However, we can instead examine the summary statistics in Table [summaryStatsOfHgInMuscleOfRissosDolphins], which do not suggest any skew or outliers. All observations are within 2.5 standard deviations of the mean. Based on this evidence, the normality assumption seems reasonable.</p>
<p>In the normal model, we used <span class="math inline">\(z^{\star}\)</span> and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the <span class="math inline">\(t\)</span> distribution:</p>
<p><span class="math display">\[\begin{aligned}
\bar{x} \ \pm\  t^{\star}_{df} \times SE\end{aligned}\]</span></p>
<p>[</p>
<p><span class="math inline">\(t^{\star}_{df}\)</span></p>
<p><br />
Multiplication<br />
factor for<br />
<span class="math inline">\(t\)</span> conf. interval]</p>
<p><span class="math inline">\(t^{\star}_{df}\)</span></p>
<p><br />
Multiplication<br />
factor for<br />
<span class="math inline">\(t\)</span> conf. interval</p>
<p>The sample mean and estimated standard error are computed just as in our earlier examples that used the normal model (<span class="math inline">\(\bar{x} = 4.4\)</span> and <span class="math inline">\(SE = s/\sqrt{n} = 0.528\)</span>). The value <span class="math inline">\(t^{\star}_{df}\)</span> is a cutoff we obtain based on the confidence level and the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df\)</span> degrees of freedom. Before determining this cutoff, we will first need the degrees of freedom.</p>
<p>In our current example, we should use the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df = n - 1 = 19 - 1 = 18\)</span> degrees of freedom. Then identifying <span class="math inline">\(t_{18}^{\star}\)</span> is similar to how we found <span class="math inline">\(z^{\star}\)</span>:</p>
<ul>
<li><p>For a 95% confidence interval, we want to find the cutoff <span class="math inline">\(t^{\star}_{18}\)</span> such that 95% of the <span class="math inline">\(t\)</span> distribution is between -<span class="math inline">\(t^{\star}_{18}\)</span> and <span class="math inline">\(t^{\star}_{18}\)</span>.</p></li>
<li><p>We look in the <span class="math inline">\(t\)</span> table on page , find the column with area totaling 0.05 in the two tails (third column), and then the row with 18 degrees of freedom: <span class="math inline">\(t^{\star}_{18} = 2.10\)</span>.</p></li>
</ul>
<p>Generally the value of <span class="math inline">\(t^{\star}_{df}\)</span> is slightly larger than what we would get under the normal model with <span class="math inline">\(z^{\star}\)</span>.</p>
<p>Finally, we can substitute all the values into the confidence interval equation to create the 95% confidence interval for the average mercury content in muscles from Risso’s dolphins that pass through the Taiji area:</p>
<p><span class="math display">\[\begin{aligned}
\bar{x} \ \pm\  t^{\star}_{18} \times SE
    \quad \to \quad
4.4 \ \pm\  2.10 \times 0.528
    \quad \to \quad
(3.29, 5.51)\end{aligned}\]</span></p>
<p>We are 95% confident the average mercury content of muscles in Risso’s dolphins is between 3.29 and 5.51 <span class="math inline">\(\mu\)</span>g/wet gram, which is considered extremely high.</p>
<p>Based on a sample of <span class="math inline">\(n\)</span> independent and nearly normal observations, a confidence interval for the population mean is</p>
<p><span class="math display">\[\begin{aligned}
\bar{x} \ \pm\  t^{\star}_{df} \times SE\end{aligned}\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> is the sample mean, <span class="math inline">\(t^{\star}_{df}\)</span> corresponds to the confidence level and degrees of freedom, and <span class="math inline">\(SE\)</span> is the standard error as estimated by the sample. The normality condition may be relaxed for larger sample sizes.</p>
<p>[croakerWhiteFishPacificExerConditions] The FDA’s webpage provides some data on mercury content of fish.<a href="#fn128" class="footnoteRef" id="fnref128"><sup>128</sup></a> Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent. Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?<a href="#fn129" class="footnoteRef" id="fnref129"><sup>129</sup></a></p>
<p><span>Estimate the standard error of the sample mean using the data summaries in Guided Practice [croakerWhiteFishPacificExerConditions]. If we are to use the <span class="math inline">\(t\)</span> distribution to create a 90% confidence interval for the actual mean of the mercury content, identify the degrees of freedom we should use and also find <span class="math inline">\(t^{\star}_{df}\)</span>.</span> [croakerWhiteFishPacificExerSEDFTStar] The standard error: <span class="math inline">\(SE = \frac{0.069}{\sqrt{15}} = 0.0178\)</span>. Degrees of freedom: <span class="math inline">\(df = n - 1 = 14\)</span>.</p>
<p>Looking in the column where two tails is 0.100 (for a 90% confidence interval) and row <span class="math inline">\(df=14\)</span>, we identify <span class="math inline">\(t^{\star}_{14} = 1.76\)</span>.</p>
<p>Using the results of Guided Practice [croakerWhiteFishPacificExerConditions] and Example [croakerWhiteFishPacificExerSEDFTStar], compute a 90% confidence interval for the average mercury content of croaker white fish (Pacific).<a href="#fn130" class="footnoteRef" id="fnref130"><sup>130</sup></a></p>
</div>
<div id="oneSampleTTests" class="section level3">
<h3><span class="header-section-number">4.1.5</span> One sample <span class="math inline">\(t\)</span> tests</h3>
<p>Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Run, comparing runners in 2006 and 2012. Technological advances in shoes, training, and diet might suggest runners would be faster in 2012. An opposing viewpoint might say that with the average body mass index on the rise, people tend to run slower. In fact, all of these components might be influencing run time.</p>
<p>The average time for all runners who finished the Cherry Blossom Run in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2012 Cherry Blossom Run whether runners in this race are getting faster or slower, versus the other possibility that there has been no change.</p>
<p>What are appropriate hypotheses for this context?<a href="#fn131" class="footnoteRef" id="fnref131"><sup>131</sup></a></p>
<p>The data come from a simple random sample from less than 10% of all participants, so the observations are independent. However, should we be worried about skew in the data? A histogram of the differences was shown in the left panel of Figure . <a href="#fn132" class="footnoteRef" id="fnref132"><sup>132</sup></a></p>
<p>With independence satisfied and skew not a concern, we can proceed with performing a hypothesis test using the <span class="math inline">\(t\)</span> distribution.</p>
<p>The sample mean and sample standard deviation are 95.61 and 15.78 minutes, respectively. Recall that the sample size is 100. What is the p-value for the test, and what is your conclusion?<a href="#fn133" class="footnoteRef" id="fnref133"><sup>133</sup></a></p>
<p><span> To help us remember to use the <span class="math inline">\(t\)</span> distribution, we use a <span class="math inline">\(T\)</span> to represent the test statistic, and we often call this a <strong>T score</strong>. The Z score and T score are computed in the exact same way and are conceptually identical: each represents how many standard errors the observed value is from the null value.</span></p>
</div>
</div>
<div id="pairedData" class="section level2">
<h2><span class="header-section-number">4.2</span> Paired data</h2>
<p>Are textbooks actually cheaper online? Here we compare the price of textbooks at the University of California, Los Angeles’ (UCLA’s) bookstore and prices at Amazon.com. Seventy-three UCLA courses were randomly sampled in Spring 2010, representing less than 10% of all UCLA courses.<a href="#fn134" class="footnoteRef" id="fnref134"><sup>134</sup></a> A portion of the data set is shown in Table [textbooksDF].</p>
<table>
<caption>Six cases of the <strong>textbooks</strong> data set.</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">dept</th>
<th align="left">course</th>
<th align="right">ucla</th>
<th align="right">amazon</th>
<th align="right">diff</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="left">Am Ind</td>
<td align="left">C170</td>
<td align="right">27.67</td>
<td align="right">27.95</td>
<td align="right">-0.28</td>
</tr>
<tr class="even">
<td>2</td>
<td align="left">Anthro</td>
<td align="left">9</td>
<td align="right">40.59</td>
<td align="right">31.14</td>
<td align="right">9.45</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="left">Anthro</td>
<td align="left">135T</td>
<td align="right">31.68</td>
<td align="right">32.00</td>
<td align="right">-0.32</td>
</tr>
<tr class="even">
<td>4</td>
<td align="left">Anthro</td>
<td align="left">191HB</td>
<td align="right">16.00</td>
<td align="right">11.52</td>
<td align="right">4.48</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td align="left"><span class="math inline">\(\vdots\)</span></td>
<td align="left"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td>72</td>
<td align="left">Wom Std</td>
<td align="left">M144</td>
<td align="right">23.76</td>
<td align="right">18.72</td>
<td align="right">5.04</td>
</tr>
<tr class="odd">
<td>73</td>
<td align="left">Wom Std</td>
<td align="left">285</td>
<td align="right">27.70</td>
<td align="right">18.22</td>
<td align="right">9.48</td>
</tr>
</tbody>
</table>
<p>[textbooksDF]</p>
<div id="paired-observations" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Paired observations</h3>
<p>Each textbook has two corresponding prices in the data set: one for the UCLA bookstore and one for Amazon. Therefore, each textbook price from the UCLA bookstore has a natural correspondence with a textbook price from Amazon. When two sets of observations have this special correspondence, they are said to be <strong>paired</strong>.</p>
<p><span> Two sets of observations are <em>paired</em> if each observation in one set has a special correspondence or connection with exactly one observation in the other data set.</span></p>
<p>To analyze paired data, it is often useful to look at the difference in outcomes of each pair of observations. In the <strong>textbook</strong> data set, we look at the difference in prices, which is represented as the <strong>diff</strong> variable in the <strong>textbooks</strong> data. Here the differences are taken as</p>
<p><span class="math display">\[\begin{aligned}
\text{UCLA price} - \text{Amazon price}\end{aligned}\]</span></p>
<p>for each book. It is important that we always subtract using a consistent order; here Amazon prices are always subtracted from UCLA prices. A histogram of these differences is shown in Figure [diffInTextbookPricesS10]. Using differences between paired observations is a common and useful way to analyze paired data.</p>
<div class="figure">
<img src="04/figures/textbooksS10/diffInTextbookPricesS10" alt="Histogram of the difference in price for each book sampled. These data are strongly skewed." />
<p class="caption">Histogram of the difference in price for each book sampled. These data are strongly skewed.</p>
</div>
<p>[diffInTextbookPricesS10]</p>
<p>The first difference shown in Table [textbooksDF] is computed as <span class="math inline">\(27.67-27.95=-0.28\)</span>. Verify the differences are calculated correctly for observations 2 and 3.<a href="#fn135" class="footnoteRef" id="fnref135"><sup>135</sup></a></p>
</div>
<div id="inference-for-paired-data" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Inference for paired data</h3>
<p>To analyze a paired data set, we simply analyze the differences. We can use the same <span class="math inline">\(t\)</span> distribution techniques we applied in the last section.</p>
<p><span>ccccc</span> <span class="math inline">\(n_{_{diff}}\)</span> &amp;</p>
<p>&amp; <span class="math inline">\(\bar{x}_{_{diff}}\)</span> &amp;</p>
<p>&amp; <span class="math inline">\(s_{_{diff}}\)</span></p>
<p><br />
73 &amp;&amp; 12.76 &amp;&amp; 14.26<br />
[textbooksSummaryStats]</p>
<p><span>Set up and implement a hypothesis test to determine whether, on average, there is a difference between Amazon’s price for a book and the UCLA bookstore’s price.</span> [htForDiffInUCLAAndAmazonTextbookPrices] We are considering two scenarios: there is no difference or there is some difference in average prices.</p>
<ul>
<li><p><span class="math inline">\(\mu_{diff}=0\)</span>. There is no difference in the average textbook price.</p></li>
<li><p><span class="math inline">\(\mu_{diff} \neq 0\)</span>. There is a difference in average prices.</p></li>
</ul>
<p>Can the <span class="math inline">\(t\)</span> distribution be used for this application? The observations are based on a simple random sample from less than 10% of all books sold at the bookstore, so independence is reasonable. While the distribution is strongly skewed, the sample is reasonably large (<span class="math inline">\(n=73\)</span>), so we can proceed. Because the conditions are reasonably satisfied, we can apply the <span class="math inline">\(t\)</span> distribution to this setting.</p>
<p>We compute the standard error associated with <span class="math inline">\(\bar{x}_{diff}\)</span> using the standard deviation of the differences (<span class="math inline">\(s_{_{diff}}=14.26\)</span>) and the number of differences (<span class="math inline">\(n_{_{diff}}=73\)</span>): <span class="math display">\[SE_{\bar{x}_{diff}} = \frac{s_{diff}}{\sqrt{n_{diff}}} = \frac{14.26}{\sqrt{73}} = 1.67\]</span> To visualize the p-value, the sampling distribution of <span class="math inline">\(\bar{x}_{diff}\)</span> is drawn as though <span class="math inline">\(H_0\)</span> is true, which is shown in Figure [textbooksS10HTTails]. The p-value is represented by the two (very) small tails.</p>
<p>To find the tail areas, we compute the test statistic, which is the T score of <span class="math inline">\(\bar{x}_{diff}\)</span> under the null condition that the actual mean difference is 0: <span class="math display">\[T = \frac{\bar{x}_{diff} - 0}{SE_{x_{diff}}} = \frac{12.76 - 0}{1.67} = 7.59\]</span> The degrees of freedom are <span class="math inline">\(df = 73 - 1 = 72\)</span>. If we examined Appendix , we would see that this value is larger than any in the 70 df row (we round down for <span class="math inline">\(df\)</span> when using the table), meaning the two-tailed p-value is less than 0.01. If we used statistical software, we would find the p-value is less than 1-in-10 billion! Because the p-value is less than 0.05, we reject the null hypothesis. We have found convincing evidence that Amazon is, on average, cheaper than the UCLA bookstore for UCLA course textbooks.</p>
<div class="figure">
<img src="04/figures/textbooksS10/textbooksS10HTTails" alt="Sampling distribution for the mean difference in book prices, if the true average difference is zero." />
<p class="caption">Sampling distribution for the mean difference in book prices, if the true average difference is zero.</p>
</div>
<p>[textbooksS10HTTails]</p>
<p>Create a 95% confidence interval for the average price difference between books at the UCLA bookstore and books on Amazon.<a href="#fn136" class="footnoteRef" id="fnref136"><sup>136</sup></a></p>
<p>In the textbook price example, we applied the <span class="math inline">\(t\)</span> distribution. However, as we mentioned in the last section, the <span class="math inline">\(t\)</span> distribution looks a lot like the normal distribution when the degrees of freedom are larger than about 30. In such cases, including this one, it would be reasonable to use the normal distribution in place of the <span class="math inline">\(t\)</span> distribution.</p>
</div>
</div>
<div id="differenceOfTwoMeans" class="section level2">
<h2><span class="header-section-number">4.3</span> Difference of two means</h2>
<p>In this section we consider a difference in two population means, <span class="math inline">\(\mu_1 - \mu_2\)</span>, under the condition that the data are not paired. Just as with a single sample, we identify conditions to ensure we can use the <span class="math inline">\(t\)</span> distribution with a point estimate of the difference, <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>.</p>
<p>We apply these methods in three contexts: determining whether stem cells can improve heart function, exploring the impact of pregnant womens’ smoking habits on birth weights of newborns, and exploring whether there is statistically significant evidence that one variations of an exam is harder than another variation. This section is motivated by questions like “Is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke?”</p>
<div id="confidence-interval-for-a-differences-of-means" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Confidence interval for a differences of means</h3>
<p>Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack? Table [summaryStatsForSheepHeartDataWhoReceivedMiceESCs] contains summary statistics for an experiment to test ESCs in sheep that had a heart attack. Each of these sheep was randomly assigned to the ESC or control group, and the change in their hearts’ pumping capacity was measured in the study. A positive value corresponds to increased pumping capacity, which generally suggests a stronger recovery. Our goal will be to identify a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity relative to the control group.</p>
<p>A point estimate of the difference in the heart pumping variable can be found using the difference in the sample means:</p>
<p><span class="math display">\[\begin{aligned}
\bar{x}_{esc} - \bar{x}_{control}\ =\ 3.50 - (-4.33)\ =\ 7.83\end{aligned}\]</span></p>
<p><span>l rrrrr</span></p>
<p>&amp; <span class="math inline">\(n\)</span> &amp; <span class="math inline">\(\bar{x}\)</span> &amp; <span class="math inline">\(s\)</span><br />
ESCs &amp; 9 &amp; 3.50 &amp; 5.17<br />
control &amp; 9 &amp; -4.33 &amp; 2.76<br />
[summaryStatsForSheepHeartDataWhoReceivedMiceESCs]</p>
<div class="figure">
<img src="04/figures/stemCellTherapyForHearts/stemCellTherapyForHearts" alt="Histograms for both the embryonic stem cell group and the control group. Higher values are associated with greater improvement. We don’t see any evidence of skew in these data; however, it is worth noting that skew would be difficult to detect with such a small sample." />
<p class="caption">Histograms for both the embryonic stem cell group and the control group. Higher values are associated with greater improvement. We don’t see any evidence of skew in these data; however, it is worth noting that skew would be difficult to detect with such a small sample.</p>
</div>
<p>[stemCellTherapyForHearts]</p>
<p><span> [ConditionsForTwoSampleTDist]The <span class="math inline">\(t\)</span> distribution can be used for inference when working with the standardized difference of two means if (1) each sample meets the conditions for using the <span class="math inline">\(t\)</span> distribution and (2) the samples are independent.</span></p>
<p><span>Can the point estimate, <span class="math inline">\(\bar{x}_{esc} - \bar{x}_{control} = 7.83\)</span>, be analyzed using the <span class="math inline">\(t\)</span> distribution?</span> We check the two required conditions:</p>
<ol style="list-style-type: decimal">
<li><p>In this study, the sheep were independent of each other. Additionally, the distributions in Figure [stemCellTherapyForHearts] don’t show any clear deviations from normality, where we watch for prominent outliers in particular for such small samples. These findings imply each sample mean could itself be modeled using a <span class="math inline">\(t\)</span> distribution.</p></li>
<li><p>The sheep in each group were also independent of each other.</p></li>
</ol>
<p>Because both conditions are met, we can use the <span class="math inline">\(t\)</span> distribution to model the difference of the two sample means.</p>
<p>Before we construct a confidence interval, we must calculate the standard error of the point estimate of the difference. For this, we use the following formula, where just as before we substitute the sample standard deviations into the formula:</p>
<p><span class="math display">\[\begin{aligned}
SE_{\bar{x}_{esc} - \bar{x}_{control}}
    &amp;= \sqrt{\frac{\sigma_{esc}^2}{n_{esc}} + \frac{\sigma_{control}^2}{n_{control}}} \\
    &amp;\approx \sqrt{\frac{s_{esc}^2}{n_{esc}} + \frac{s_{control}^2}{n_{control}}}
    = \sqrt{\frac{5.17^2}{9} + \frac{2.76^2}{9}} = 1.95\end{aligned}\]</span></p>
<p>Because we will use the <span class="math inline">\(t\)</span> distribution, we also must identify the appropriate degrees of freedom. This can be done using computer software. An alternative technique is to use the smaller of <span class="math inline">\(n_1 - 1\)</span> and <span class="math inline">\(n_2 - 1\)</span>, which is the method we will typically apply in the examples and guided practice.<a href="#fn137" class="footnoteRef" id="fnref137"><sup>137</sup></a></p>
<p>The sample difference of two means, <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>, can be modeled using the <span class="math inline">\(t\)</span> distribution and the standard error</p>
<p><span class="math display">\[\begin{aligned}
\textstyle
SE_{\bar{x}_{1} - \bar{x}_{2}} = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\label{seOfDifferenceInMeans}\end{aligned}\]</span></p>
<p>when each sample mean can itself be modeled using a <span class="math inline">\(t\)</span> distribution and the samples are independent. To calculate the degrees of freedom, use statistical software or the smaller of <span class="math inline">\(n_1 - 1\)</span> and <span class="math inline">\(n_2 - 1\)</span>.</p>
<p><span>Calculate a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity of sheep after they’ve suffered a heart attack.</span> We will use the sample difference and the standard error for that point estimate from our earlier calculations:</p>
<p><span class="math display">\[\begin{aligned}
&amp; \bar{x}_{esc} - \bar{x}_{control} = 7.83 \\
&amp; SE = \sqrt{\frac{5.17^2}{9} + \frac{2.76^2}{9}} = 1.95\end{aligned}\]</span></p>
<p>Using <span class="math inline">\(df = 8\)</span>, we can identify the appropriate <span class="math inline">\(t^{\star}_{df} = t^{\star}_{8}\)</span> for a 95% confidence interval as 2.31. Finally, we can enter the values into the confidence interval formula:</p>
<p><span class="math display">\[\begin{aligned}
\text{point estimate} \ \pm\ z^{\star}SE \quad\rightarrow\quad
7.83 \ \pm\ 2.31\times 1.95 \quad\rightarrow\quad (3.38, 12.38)\end{aligned}\]</span></p>
<p>We are 95% confident that embryonic stem cells improve the heart’s pumping function in sheep that have suffered a heart attack by 3.38% to 12.38%.</p>
</div>
<div id="hypothesis-tests-based-on-a-difference-in-means" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Hypothesis tests based on a difference in means</h3>
<p>A data set called represents a random sample of 150 cases of mothers and their newborns in North Carolina over a year. Four cases from this data set are represented in Table [babySmokeDF]. We are particularly interested in two variables: <strong>weight</strong> and <strong>smoke</strong>. The <strong>weight</strong> variable represents the weights of the newborns and the <strong>smoke</strong> variable describes which mothers smoked during pregnancy. We would like to know, is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke? We will use the North Carolina sample to try to answer this question. The smoking group includes 50 cases and the nonsmoking group contains 100 cases, represented in Figure [babySmokePlotOfTwoGroupsToExamineSkew].</p>
<table>
<caption>Four cases from the data set. The value “NA”, shown for the first two entries of the first variable, indicates that piece of data is missing.</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">fAge</th>
<th align="right">mAge</th>
<th align="right">weeks</th>
<th align="right">weight</th>
<th align="left">sexBaby</th>
<th align="left">smoke</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="right">NA</td>
<td align="right">13</td>
<td align="right">37</td>
<td align="right">5.00</td>
<td align="left">female</td>
<td align="left">nonsmoker</td>
</tr>
<tr class="even">
<td>2</td>
<td align="right">NA</td>
<td align="right">14</td>
<td align="right">36</td>
<td align="right">5.88</td>
<td align="left">female</td>
<td align="left">nonsmoker</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="right">19</td>
<td align="right">15</td>
<td align="right">41</td>
<td align="right">8.13</td>
<td align="left">male</td>
<td align="left">smoker</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="left"><span class="math inline">\(\vdots\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td>150</td>
<td align="right">45</td>
<td align="right">50</td>
<td align="right">36</td>
<td align="right">9.25</td>
<td align="left">female</td>
<td align="left">nonsmoker</td>
</tr>
</tbody>
</table>
<p>[babySmokeDF]</p>
<div class="figure">
<img src="04/figures/babySmokePlotOfTwoGroupsToExamineSkew/babySmokePlotOfTwoGroupsToExamineSkew" alt="The top panel represents birth weights for infants whose mothers smoked. The bottom panel represents the birth weights for infants whose mothers who did not smoke. The distributions exhibit moderate-to-strong and strong skew, respectively." />
<p class="caption">The top panel represents birth weights for infants whose mothers smoked. The bottom panel represents the birth weights for infants whose mothers who did not smoke. The distributions exhibit moderate-to-strong and strong skew, respectively.</p>
</div>
<p>[babySmokePlotOfTwoGroupsToExamineSkew]</p>
<p><span>Set up appropriate hypotheses to evaluate whether there is a relationship between a mother smoking and average birth weight.</span>[babySmokeHTForWeight] The null hypothesis represents the case of no difference between the groups.</p>
<ul>
<li><p>There is no difference in average birth weight for newborns from mothers who did and did not smoke. In statistical notation: <span class="math inline">\(\mu_{n} - \mu_{s} = 0\)</span>, where <span class="math inline">\(\mu_{n}\)</span> represents non-smoking mothers and <span class="math inline">\(\mu_s\)</span> represents mothers who smoked.</p></li>
<li><p>There is some difference in average newborn weights from mothers who did and did not smoke (<span class="math inline">\(\mu_{n} - \mu_{s} \neq 0\)</span>).</p></li>
</ul>
<p>We check the two conditions necessary to apply the <span class="math inline">\(t\)</span> distribution to the difference in sample means. (1) Because the data come from a simple random sample and consist of less than 10% of all such cases, the observations are independent. Additionally, while each distribution is strongly skewed, the sample sizes of 50 and 100 would make it reasonable to model each mean separately using a <span class="math inline">\(t\)</span> distribution. The skew is reasonable for these sample sizes of 50 and 100. (2) The independence reasoning applied in (1) also ensures the observations in each sample are independent. Since both conditions are satisfied, the difference in sample means may be modeled using a <span class="math inline">\(t\)</span> distribution.</p>
<table>
<caption>Summary statistics for the data set.</caption>
<tbody>
<tr class="odd">
<td align="left">mean</td>
<td align="right">6.78</td>
<td align="right">7.18</td>
</tr>
<tr class="even">
<td align="left">st. dev.</td>
<td align="right">1.43</td>
<td align="right">1.60</td>
</tr>
<tr class="odd">
<td align="left">samp. size</td>
<td align="right">50</td>
<td align="right">100</td>
</tr>
</tbody>
</table>
<p>[summaryStatsOfBirthWeightForNewbornsFromSmokingAndNonsmokingMothers]</p>
<p>The summary statistics in Table [summaryStatsOfBirthWeightForNewbornsFromSmokingAndNonsmokingMothers] may be useful for this exercise. (a) What is the point estimate of the population difference, <span class="math inline">\(\mu_{n} - \mu_{s}\)</span>? (b) Compute the standard error of the point estimate from part (a).<a href="#fn138" class="footnoteRef" id="fnref138"><sup>138</sup></a></p>
<p><span>Draw a picture to represent the p-value for the hypothesis test from Example [babySmokeHTForWeight].</span> [pictureOfPValueForEstimateOfDiffOfMeansOfBirthWeights] To depict the p-value, we draw the distribution of the point estimate as though <span class="math inline">\(H_0\)</span> were true and shade areas representing at least as much evidence against <span class="math inline">\(H_0\)</span> as what was observed. Both tails are shaded because it is a two-sided test.</p>
<div class="figure">
<img src="04/figures/distOfDiffOfSampleMeansForBWOfBabySmokeData/distOfDiffOfSampleMeansForBWOfBabySmokeData" alt="image" />
<p class="caption">image</p>
</div>
<p><span>Compute the p-value of the hypothesis test using the figure in Example [pictureOfPValueForEstimateOfDiffOfMeansOfBirthWeights], and evaluate the hypotheses using a significance level of <span class="math inline">\(\alpha=0.05\)</span>.</span> [babySmokeHTForWeightComputePValueAndEvalHT] We start by computing the T score:</p>
<p><span class="math display">\[\begin{aligned}
T = \frac{\ 0.40 - 0\ }{0.26} = 1.54\end{aligned}\]</span></p>
<p>Next, we compare this value to values in the <span class="math inline">\(t\)</span> table in Appendix , where we use the smaller of <span class="math inline">\(n_n - 1 = 99\)</span> and <span class="math inline">\(n_s - 1 = 49\)</span> as the degrees of freedom: <span class="math inline">\(df = 49\)</span>. The T score falls between the first and second columns in the <span class="math inline">\(df = 49\)</span> row of the <span class="math inline">\(t\)</span> table, meaning the two-tailed p-value falls between 0.10 and 0.20 (reminder, find tail areas along the top of the table). This p-value is larger than the significance value, 0.05, so we fail to reject the null hypothesis. There is insufficient evidence to say there is a difference in average birth weight of newborns from North Carolina mothers who did smoke during pregnancy and newborns from North Carolina mothers who did not smoke during pregnancy.</p>
<p>Does the conclusion to Example [babySmokeHTForWeightComputePValueAndEvalHT] mean that smoking and average birth weight are unrelated?<a href="#fn139" class="footnoteRef" id="fnref139"><sup>139</sup></a></p>
<p>[babySmokeHTIDingHowToDetectDifferences] If we made a Type 2 Error and there is a difference, what could we have done differently in data collection to be more likely to detect such a difference?<a href="#fn140" class="footnoteRef" id="fnref140"><sup>140</sup></a></p>
</div>
<div id="case-study-two-versions-of-a-course-exam" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Case study: two versions of a course exam</h3>
<p>An instructor decided to run two slight variations of the same exam. Prior to passing out the exams, she shuffled the exams together to ensure each student received a random version. Summary statistics for how students performed on these two exams are shown in Table [summaryStatsForTwoVersionsOfExams]. Anticipating complaints from students who took Version B, she would like to evaluate whether the difference observed in the groups is so large that it provides convincing evidence that Version B was more difficult (on average) than Version A.</p>
<p><span>l rrrrr</span> Version</p>
<p>&amp; <span class="math inline">\(n\)</span> &amp; <span class="math inline">\(\bar{x}\)</span> &amp; <span class="math inline">\(s\)</span> &amp; min &amp; max<br />
A &amp; 30 &amp; 79.4 &amp; 14 &amp; 45 &amp; 100<br />
B &amp; 27 &amp; 74.1 &amp; 20 &amp; 32 &amp; 100<br />
[summaryStatsForTwoVersionsOfExams]</p>
<p>[htSetupForEvaluatingTwoExamVersions] Construct a hypotheses to evaluate whether the observed difference in sample means, <span class="math inline">\(\bar{x}_A - \bar{x}_B=5.3\)</span>, is due to chance.<a href="#fn141" class="footnoteRef" id="fnref141"><sup>141</sup></a></p>
<p>[conditionsForTDistForEvaluatingTwoExamVersions] To evaluate the hypotheses in Guided Practice [htSetupForEvaluatingTwoExamVersions] using the <span class="math inline">\(t\)</span> distribution, we must first verify assumptions. (a) Does it seem reasonable that the scores are independent within each group? (b) What about the normality / skew condition for observations in each group? (c) Do you think scores from the two groups would be independent of each other, i.e. the two samples are independent?<a href="#fn142" class="footnoteRef" id="fnref142"><sup>142</sup></a></p>
<p>After verifying the conditions for each sample and confirming the samples are independent of each other, we are ready to conduct the test using the <span class="math inline">\(t\)</span> distribution. In this case, we are estimating the true difference in average test scores using the sample data, so the point estimate is <span class="math inline">\(\bar{x}_A - \bar{x}_B = 5.3\)</span>. The standard error of the estimate can be calculated as</p>
<p><span class="math display">\[\begin{aligned}
SE = \sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}} = \sqrt{\frac{14^2}{30} + \frac{20^2}{27}} = 4.62\end{aligned}\]</span></p>
<p>Finally, we construct the test statistic:</p>
<p><span class="math display">\[\begin{aligned}
T = \frac{\text{point estimate} - \text{null value}}{SE} = \frac{(79.4-74.1) - 0}{4.62} = 1.15\end{aligned}\]</span></p>
<p>If we have a computer handy, we can identify the degrees of freedom as 45.97. Otherwise we use the smaller of <span class="math inline">\(n_1-1\)</span> and <span class="math inline">\(n_2-1\)</span>: <span class="math inline">\(df=26\)</span>.</p>
<div class="figure">
<img src="04/figures/pValueOfTwoTailAreaOfExamVersionsWhereDFIs26/pValueOfTwoTailAreaOfExamVersionsWhereDFIs26" alt="The t distribution with 26 degrees of freedom. The shaded right tail represents values with T \geq 1.15. Because it is a two-sided test, we also shade the corresponding lower tail." />
<p class="caption">The <span class="math inline">\(t\)</span> distribution with 26 degrees of freedom. The shaded right tail represents values with <span class="math inline">\(T \geq 1.15\)</span>. Because it is a two-sided test, we also shade the corresponding lower tail.</p>
</div>
<p>[pValueOfTwoTailAreaOfExamVersionsWhereDFIs26]</p>
<p><span>Identify the p-value using <span class="math inline">\(df = 26\)</span> and provide a conclusion in the context of the case study.</span> We examine row <span class="math inline">\(df=26\)</span> in the <span class="math inline">\(t\)</span> table. Because this value is smaller than the value in the left column, the p-value is larger than 0.200 (two tails!). Because the p-value is so large, we do not reject the null hypothesis. That is, the data do not convincingly show that one exam version is more difficult than the other, and the teacher should not be convinced that she should add points to the Version B exam scores.</p>
</div>
<div id="summary-for-inference-using-the-t-distribution" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Summary for inference using the <span class="math inline">\(t\)</span> distribution</h3>
<p><strong>Hypothesis tests.</strong> When applying the <span class="math inline">\(t\)</span> distribution for a hypothesis test, we proceed as follows:</p>
<ul>
<li><p>Write appropriate hypotheses.</p></li>
<li><p>Verify conditions for using the <span class="math inline">\(t\)</span> distribution.</p>
<ul>
<li><p>One-sample or differences from paired data: the observations (or differences) must be independent and nearly normal. For larger sample sizes, we can relax the nearly normal requirement, e.g. slight skew is okay or sample sizes of 15, moderate skew for sample sizes of 30, and strong skew for sample sizes of 60.</p></li>
<li><p>For a difference of means when the data are not paired: each sample mean must separately satisfy the one-sample conditions for the <span class="math inline">\(t\)</span> distribution, and the data in the groups must also be independent.</p></li>
</ul></li>
<li><p>Compute the point estimate of interest, the standard error, and the degrees of freedom. For <span class="math inline">\(df\)</span>, use <span class="math inline">\(n-1\)</span> for one sample, and for two samples use either statistical software or the smaller of <span class="math inline">\(n_1 - 1\)</span> and <span class="math inline">\(n_2 - 1\)</span>.</p></li>
<li><p>Compute the T score and p-value.</p></li>
<li><p>Make a conclusion based on the p-value, and write a conclusion in context and in plain language so anyone can understand the result.</p></li>
</ul>
<p><strong>Confidence intervals.</strong> Similarly, the following is how we generally computed a confidence interval using a <span class="math inline">\(t\)</span> distribution:</p>
<ul>
<li><p>Verify conditions for using the <span class="math inline">\(t\)</span> distribution. (See above.)</p></li>
<li><p>Compute the point estimate of interest, the standard error, the degrees of freedom, and <span class="math inline">\(t^{\star}_df\)</span>.</p></li>
<li><p>Calculate the confidence interval using the general formula, point estimate <span class="math inline">\(\pm t_{df}^{\star} SE\)</span>.</p></li>
<li><p>Put the conclusions in context and in plain language so even non-statisticians can understand the results.</p></li>
</ul>
</div>
<div id="pooledStandardDeviations" class="section level3">
<h3><span class="header-section-number">4.3.5</span> Pooled standard deviation estimate (special topic)</h3>
<p>Occasionally, two populations will have standard deviations that are so similar that they can be treated as identical. For example, historical data or a well-understood biological mechanism may justify this strong assumption. In such cases, we can make the <span class="math inline">\(t\)</span> distribution approach slightly more precise by using a pooled standard deviation.</p>
<p>The <strong>pooled standard deviation</strong> of two groups is a way to use data from both samples to better estimate the standard deviation and standard error. If <span class="math inline">\(s_1^{}\)</span> and <span class="math inline">\(s_2^{}\)</span> are the standard deviations of groups 1 and 2 and there are good reasons to believe that the population standard deviations are equal, then we can obtain an improved estimate of the group variances by pooling their data:</p>
<p><span class="math display">\[\begin{aligned}
s_{pooled}^2 = \frac{s_1^2\times (n_1-1) + s_2^2\times (n_2-1)}{n_1 + n_2 - 2}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are the sample sizes, as before. To use this new statistic, we substitute <span class="math inline">\(s_{pooled}^2\)</span> in place of <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span> in the standard error formula, and we use an updated formula for the degrees of freedom:</p>
<p><span class="math display">\[\begin{aligned}
df = n_1 + n_2 - 2\end{aligned}\]</span></p>
<p>The benefits of pooling the standard deviation are realized through obtaining a better estimate of the standard deviation for each group and using a larger degrees of freedom parameter for the <span class="math inline">\(t\)</span> distribution. Both of these changes may permit a more accurate model of the sampling distribution of <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>.</p>
<p><span>Pooling standard deviations should be done only after careful research</span> <span>A pooled standard deviation is only appropriate when background research indicates the population standard deviations are nearly equal. When the sample size is large and the condition may be adequately checked with data, the benefits of pooling the standard deviations greatly diminishes.</span></p>
</div>
</div>
<div id="anovaAndRegrWithCategoricalVariables" class="section level2">
<h2><span class="header-section-number">4.4</span> Comparing many means with ANOVA<br />
(special topic)</h2>
<p>Sometimes we want to compare means across many groups. We might initially think to do pairwise comparisons; for example, if there were three groups, we might be tempted to compare the first mean with the second, then with the third, and then finally compare the second and third means for a total of three comparisons. However, this strategy can be treacherous. If we have many groups and do many comparisons, it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations.</p>
<p>In this section, we will learn a new method called <strong>analysis of variance (ANOVA)</strong> and a new test statistic called <span class="math inline">\(F\)</span>. ANOVA uses a single hypothesis test to check whether the means across many groups are equal:</p>
<ul>
<li><p>The mean outcome is the same across all groups. In statistical notation, <span class="math inline">\(\mu_1 = \mu_2 = \cdots = \mu_k\)</span> where <span class="math inline">\(\mu_i\)</span> represents the mean of the outcome for observations in category <span class="math inline">\(i\)</span>.</p></li>
<li><p>At least one mean is different.</p></li>
</ul>
<p>Generally we must check three conditions on the data before performing ANOVA:</p>
<ul>
<li><p>the observations are independent within and across groups,</p></li>
<li><p>the data within each group are nearly normal, and</p></li>
<li><p>the variability across the groups is about equal.</p></li>
</ul>
<p>When these three conditions are met, we may perform an ANOVA to determine whether the data provide strong evidence against the null hypothesis that all the <span class="math inline">\(\mu_i\)</span> are equal.</p>
<p><span>College departments commonly run multiple lectures of the same introductory course each semester because of high demand. Consider a statistics department that runs three lectures of an introductory statistics course. We might like to determine whether there are statistically significant differences in first exam scores in these three classes (<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>). Describe appropriate hypotheses to determine whether there are any differences between the three classes.</span> [firstExampleForThreeStatisticsClassesAndANOVA] The hypotheses may be written in the following form:</p>
<ul>
<li><p>The average score is identical in all lectures. Any observed difference is due to chance. Notationally, we write <span class="math inline">\(\mu_A=\mu_B=\mu_C\)</span>.</p></li>
<li><p>The average score varies by class. We would reject the null hypothesis in favor of the alternative hypothesis if there were larger differences among the class averages than what we might expect from chance alone.</p></li>
</ul>
<p>Strong evidence favoring the alternative hypothesis in ANOVA is described by unusually large differences among the group means. We will soon learn that assessing the variability of the group means relative to the variability among individual observations within each group is key to ANOVA’s success.</p>
<p><span>Examine Figure [toyANOVA]. Compare groups I, II, and III. Can you visually determine if the differences in the group centers is due to chance or not? Now compare groups IV, V, and VI. Do these differences appear to be due to chance?</span></p>
<div class="figure">
<img src="04/figures/toyANOVA/toyANOVA" alt="Side-by-side dot plot for the outcomes for six groups." />
<p class="caption">Side-by-side dot plot for the outcomes for six groups.</p>
</div>
<p>[toyANOVA]</p>
<p>Any real difference in the means of groups I, II, and III is difficult to discern, because the data within each group are very volatile relative to any differences in the average outcome. On the other hand, it appears there are differences in the centers of groups IV, V, and VI. For instance, group V appears to have a higher mean than that of the other two groups. Investigating groups IV, V, and VI, we see the differences in the groups’ centers are noticeable because those differences are large <em>relative to the variability in the individual observations within each group</em>.</p>
<div id="is-batting-performance-related-to-player-position-in-mlb" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Is batting performance related to player position in MLB?</h3>
<p>We would like to discern whether there are real differences between the batting performance of baseball players according to their position: outfielder (), infielder (), designated hitter (), and catcher (). We will use a data set called <strong>bat10</strong>, which includes batting records of 327 Major League Baseball (MLB) players from the 2010 season. Six of the 327 cases represented in <strong>bat10</strong> are shown in Table [mlbBat10DataMatrix], and descriptions for each variable are provided in Table [mlbBat10Variables]. The measure we will use for the player batting performance (the outcome variable) is on-base percentage (<strong>OBP</strong>). The on-base percentage roughly represents the fraction of the time a player successfully gets on base or hits a home run.</p>
<table>
<caption>Six cases from the <strong>bat10</strong> data matrix.</caption>
<thead>
<tr class="header">
<th></th>
<th align="left">name</th>
<th align="left">team</th>
<th align="left">position</th>
<th align="right">AB</th>
<th align="right">H</th>
<th align="right">HR</th>
<th align="right">RBI</th>
<th align="right">AVG</th>
<th align="right">OBP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="left">I Suzuki</td>
<td align="left">SEA</td>
<td align="left">OF</td>
<td align="right">680</td>
<td align="right">214</td>
<td align="right">6</td>
<td align="right">43</td>
<td align="right">0.315</td>
<td align="right">0.359</td>
</tr>
<tr class="even">
<td>2</td>
<td align="left">D Jeter</td>
<td align="left">NYY</td>
<td align="left">IF</td>
<td align="right">663</td>
<td align="right">179</td>
<td align="right">10</td>
<td align="right">67</td>
<td align="right">0.270</td>
<td align="right">0.340</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="left">M Young</td>
<td align="left">TEX</td>
<td align="left">IF</td>
<td align="right">656</td>
<td align="right">186</td>
<td align="right">21</td>
<td align="right">91</td>
<td align="right">0.284</td>
<td align="right">0.330</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\vdots\)</span></td>
<td align="left"><span class="math inline">\(\vdots\)</span></td>
<td align="left"><span class="math inline">\(\vdots\)</span></td>
<td align="left"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"><span class="math inline">\(\vdots\)</span></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td>325</td>
<td align="left">B Molina</td>
<td align="left">SF</td>
<td align="left">C</td>
<td align="right">202</td>
<td align="right">52</td>
<td align="right">3</td>
<td align="right">17</td>
<td align="right">0.257</td>
<td align="right">0.312</td>
</tr>
<tr class="even">
<td>326</td>
<td align="left">J Thole</td>
<td align="left">NYM</td>
<td align="left">C</td>
<td align="right">202</td>
<td align="right">56</td>
<td align="right">3</td>
<td align="right">17</td>
<td align="right">0.277</td>
<td align="right">0.357</td>
</tr>
<tr class="odd">
<td>327</td>
<td align="left">C Heisey</td>
<td align="left">CIN</td>
<td align="left">OF</td>
<td align="right">201</td>
<td align="right">51</td>
<td align="right">8</td>
<td align="right">21</td>
<td align="right">0.254</td>
<td align="right">0.324</td>
</tr>
</tbody>
</table>
<p>[mlbBat10DataMatrix]</p>
<table>
<caption>Variables and their descriptions for the <strong>bat10</strong> data set.</caption>
<thead>
<tr class="header">
<th align="left"><span><strong>variable</strong></span></th>
<th align="left"><span><strong>description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>name</strong></td>
<td align="left">Player name</td>
</tr>
<tr class="even">
<td align="left"><strong>team</strong></td>
<td align="left">The abbreviated name of the player’s team</td>
</tr>
<tr class="odd">
<td align="left"><strong>position</strong></td>
<td align="left">The player’s primary field position (, , , )</td>
</tr>
<tr class="even">
<td align="left"><strong>AB</strong></td>
<td align="left">Number of opportunities at bat</td>
</tr>
<tr class="odd">
<td align="left"><strong>H</strong></td>
<td align="left">Number of hits</td>
</tr>
<tr class="even">
<td align="left"><strong>HR</strong></td>
<td align="left">Number of home runs</td>
</tr>
<tr class="odd">
<td align="left"><strong>RBI</strong></td>
<td align="left">Number of runs batted in</td>
</tr>
<tr class="even">
<td align="left"><strong>AVG</strong></td>
<td align="left">Batting average, which is equal to <span class="math inline">\(\resp{H}/\resp{AB}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><strong>OBP</strong></td>
<td align="left">On-base percentage, which is roughly equal to the fraction of times a player gets on base or hits a home run</td>
</tr>
</tbody>
</table>
<p>[mlbBat10Variables]</p>
<p>[nullHypForOBPAgainstPosition] The null hypothesis under consideration is the following: <span class="math inline">\(\mu_{\resp{OF}} = \mu_{\resp{IF}} = \mu_{\resp{DH}} = \mu_{\resp{C}}\)</span>. Write the null and corresponding alternative hypotheses in plain language.<a href="#fn143" class="footnoteRef" id="fnref143"><sup>143</sup></a></p>
<p><span>The player positions have been divided into four groups: outfield (), infield (), designated hitter (), and catcher (). What would be an appropriate point estimate of the on-base percentage by outfielders, <span class="math inline">\(\mu_{\resp{OF}}\)</span>?</span> A good estimate of the on-base percentage by outfielders would be the sample average of <strong>AVG</strong> for just those players whose position is outfield: <span class="math inline">\(\bar{x}_{OF} = 0.334\)</span>.</p>
<p>Table [mlbHRPerABSummaryTable] provides summary statistics for each group. A side-by-side box plot for the on-base percentage is shown in Figure [mlbANOVABoxPlot]. Notice that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied before we consider the ANOVA approach.</p>
<table>
<caption>Summary statistics of on-base percentage, split by player position.</caption>
<tbody>
<tr class="odd">
<td align="left">Sample size (<span class="math inline">\(n_i\)</span>)</td>
<td align="right">120</td>
<td align="right">154</td>
<td align="right">14</td>
<td align="right">39</td>
</tr>
<tr class="even">
<td align="left">Sample mean (<span class="math inline">\(\bar{x}_i\)</span>)</td>
<td align="right">0.334</td>
<td align="right">0.332</td>
<td align="right">0.348</td>
<td align="right">0.323</td>
</tr>
<tr class="odd">
<td align="left">Sample SD (<span class="math inline">\(s_i\)</span>)</td>
<td align="right">0.029</td>
<td align="right">0.037</td>
<td align="right">0.036</td>
<td align="right">0.045</td>
</tr>
</tbody>
</table>
<p>[mlbHRPerABSummaryTable]</p>
<div class="figure">
<img src="04/figures/mlbANOVA/mlbANOVABoxPlot" alt="Side-by-side box plot of the on-base percentage for 327 players across four groups. There is one prominent outlier visible in the infield group, but with 154 observations in the infield group, this outlier is not a concern." />
<p class="caption">Side-by-side box plot of the on-base percentage for 327 players across four groups. There is one prominent outlier visible in the infield group, but with 154 observations in the infield group, this outlier is not a concern.</p>
</div>
<p>[mlbANOVABoxPlot]</p>
<p>The largest difference between the sample means is between the designated hitter and the catcher positions. Consider again the original hypotheses:</p>
<ul>
<li><p><span class="math inline">\(\mu_{\resp{OF}} = \mu_{\resp{IF}} = \mu_{\resp{DH}} = \mu_{\resp{C}}\)</span></p></li>
<li><p>The average on-base percentage (<span class="math inline">\(\mu_i\)</span>) varies across some (or all) groups.</p></li>
</ul>
<p>Why might it be inappropriate to run the test by simply estimating whether the difference of <span class="math inline">\(\mu_{\textbf{DH}}\)</span> and <span class="math inline">\(\mu_{\resp{C}}\)</span> is statistically significant at a 0.05 significance level?</p>
<p>[multipleComparisonExampleThatIncludesDiscussionOfClassrooms] The primary issue here is that we are inspecting the data before picking the groups that will be compared. It is inappropriate to examine all data by eye (informal testing) and only afterwards decide which parts to formally test. This is called <strong>data snooping</strong> or <strong>data fishing</strong>. Naturally we would pick the groups with the large differences for the formal test, leading to an inflation in the Type 1 Error rate. To understand this better, let’s consider a slightly different problem.</p>
<p>Suppose we are to measure the aptitude for students in 20 classes in a large elementary school at the beginning of the year. In this school, all students are randomly assigned to classrooms, so any differences we observe between the classes at the start of the year are completely due to chance. However, with so many groups, we will probably observe a few groups that look rather different from each other. If we select only these classes that look so different, we will probably make the wrong conclusion that the assignment wasn’t random. While we might only formally test differences for a few pairs of classes, we informally evaluated the other classes by eye before choosing the most extreme cases for a comparison.</p>
<p>For additional information on the ideas expressed in Example [multipleComparisonExampleThatIncludesDiscussionOfClassrooms], we recommend reading about the <strong>prosecutor’s fallacy</strong>.<a href="#fn144" class="footnoteRef" id="fnref144"><sup>144</sup></a></p>
<p>In the next section we will learn how to use the <span class="math inline">\(F\)</span> statistic and ANOVA to test whether observed differences in means could have happened just by chance even if there was no difference in the respective population means.</p>
</div>
<div id="analysis-of-variance-anova-and-the-f-test" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Analysis of variance (ANOVA) and the F test</h3>
<p>The method of analysis of variance in this context focuses on answering one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? This question is different from earlier testing procedures since we will <em>simultaneously</em> consider many groups, and evaluate whether their sample means differ more than we would expect from natural variation. We call this variability the <strong>mean square between groups (<span class="math inline">\(MSG\)</span>)</strong>, and it has an associated degrees of freedom, <span class="math inline">\(df_{G}=k-1\)</span> when there are <span class="math inline">\(k\)</span> groups. The <span class="math inline">\(MSG\)</span> can be thought of as a scaled variance formula for means. If the null hypothesis is true, any variation in the sample means is due to chance and shouldn’t be too large. Details of <span class="math inline">\(MSG\)</span> calculations are provided in the footnote,<a href="#fn145" class="footnoteRef" id="fnref145"><sup>145</sup></a> however, we typically use software for these computations.</p>
<p>The mean square between the groups is, on its own, quite useless in a hypothesis test. We need a benchmark value for how much variability should be expected among the sample means if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the <strong>mean square error (<span class="math inline">\(MSE\)</span>)</strong>, which has an associated degrees of freedom value <span class="math inline">\(df_E=n-k\)</span>. It is helpful to think of <span class="math inline">\(MSE\)</span> as a measure of the variability within the groups. Details of the computations of the <span class="math inline">\(MSE\)</span> are provided in the footnote<a href="#fn146" class="footnoteRef" id="fnref146"><sup>146</sup></a> for interested readers.</p>
<p>When the null hypothesis is true, any differences among the sample means are only due to chance, and the <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span> should be about equal. As a test statistic for ANOVA, we examine the fraction of <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
 \label{formulaForTheFStatistic}
F = \frac{MSG}{MSE}\end{aligned}\]</span></p>
<p>The <span class="math inline">\(MSG\)</span> represents a measure of the between-group variability, and <span class="math inline">\(MSE\)</span> measures the variability within each of the groups.</p>
<p>For the baseball data, <span class="math inline">\(MSG = 0.00252\)</span> and <span class="math inline">\(MSE=0.00127\)</span>. Identify the degrees of freedom associated with MSG and MSE and verify the <span class="math inline">\(F\)</span> statistic is approximately 1.994.<a href="#fn147" class="footnoteRef" id="fnref147"><sup>147</sup></a></p>
<p>We can use the <span class="math inline">\(F\)</span> statistic to evaluate the hypotheses in what is called an <strong>F test</strong>. A p-value can be computed from the <span class="math inline">\(F\)</span> statistic using an <span class="math inline">\(F\)</span> distribution, which has two associated parameters: <span class="math inline">\(df_{1}\)</span> and <span class="math inline">\(df_{2}\)</span>. For the <span class="math inline">\(F\)</span> statistic in ANOVA, <span class="math inline">\(df_{1} = df_{G}\)</span> and <span class="math inline">\(df_{2}= df_{E}\)</span>. An <span class="math inline">\(F\)</span> distribution with 3 and 323 degrees of freedom, corresponding to the <span class="math inline">\(F\)</span> statistic for the baseball hypothesis test, is shown in Figure [fDist3And323].</p>
<div class="figure">
<img src="04/figures/fDist3And323/fDist3And323" alt="An F distribution with df_1=3 and df_2=323." />
<p class="caption">An <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(df_1=3\)</span> and <span class="math inline">\(df_2=323\)</span>.</p>
</div>
<p>[fDist3And323]</p>
<p>The larger the observed variability in the sample means (<span class="math inline">\(MSG\)</span>) relative to the within-group observations (<span class="math inline">\(MSE\)</span>), the larger <span class="math inline">\(F\)</span> will be and the stronger the evidence against the null hypothesis. Because larger values of <span class="math inline">\(F\)</span> represent stronger evidence against the null hypothesis, we use the upper tail of the distribution to compute a p-value.</p>
<p><span> Analysis of variance (ANOVA) is used to test whether the mean outcome differs across 2 or more groups. ANOVA uses a test statistic <span class="math inline">\(F\)</span>, which represents a standardized ratio of variability in the sample means relative to the variability within the groups. If <span class="math inline">\(H_0\)</span> is true and the model assumptions are satisfied, the statistic <span class="math inline">\(F\)</span> follows an <span class="math inline">\(F\)</span> distribution with parameters <span class="math inline">\(df_{1}=k-1\)</span> and <span class="math inline">\(df_{2}=n-k\)</span>. The upper tail of the <span class="math inline">\(F\)</span> distribution is used to represent the p-value.</span></p>
<p>[describePValueAreaForFDistributionInMLBOBPExample] The test statistic for the baseball example is <span class="math inline">\(F=1.994\)</span>. Shade the area corresponding to the p-value in Figure [fDist3And323].<a href="#fn148" class="footnoteRef" id="fnref148"><sup>148</sup></a></p>
<p><span>The p-value corresponding to the shaded area in the solution of Guided Practice [describePValueAreaForFDistributionInMLBOBPExample] is equal to about 0.115. Does this provide strong evidence against the null hypothesis?</span> The p-value is larger than 0.05, indicating the evidence is not strong enough to reject the null hypothesis at a significance level of 0.05. That is, the data do not provide strong evidence that the average on-base percentage varies by player’s primary field position.</p>
</div>
<div id="reading-an-anova-table-from-software" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Reading an ANOVA table from software</h3>
<p>The calculations required to perform an ANOVA by hand are tedious and prone to human error. For these reasons, it is common to use statistical software to calculate the <span class="math inline">\(F\)</span> statistic and p-value.</p>
<p>An ANOVA can be summarized in a table very similar to that of a regression summary, which we will see in . Table [anovaSummaryTableForOBPAgainstPosition] shows an ANOVA summary to test whether the mean of on-base percentage varies by player positions in the MLB. Many of these values should look familiar; in particular, the <span class="math inline">\(F\)</span> test statistic and p-value can be retrieved from the last columns.</p>
<p><span>lrrrrr</span> &amp; Df &amp; Sum Sq &amp; Mean Sq &amp; F value &amp; Pr(<span class="math inline">\(&gt;\)</span>F)<br />
position &amp; 3 &amp; 0.0076 &amp; 0.0025 &amp; 1.9943 &amp; 0.1147<br />
Residuals &amp; 323 &amp; 0.4080 &amp; 0.0013 &amp; &amp;<br />
[anovaSummaryTableForOBPAgainstPosition]</p>
</div>
<div id="graphical-diagnostics-for-an-anova-analysis" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Graphical diagnostics for an ANOVA analysis</h3>
<p>There are three conditions we must check for an ANOVA analysis: all observations must be independent, the data in each group must be nearly normal, and the variance within each group must be approximately equal.</p>
<dl>
<dt>Independence.</dt>
<dd><p>If the data are a simple random sample from less than 10% of the population, this condition is satisfied. For processes and experiments, carefully consider whether the data may be independent (e.g. no pairing). For example, in the MLB data, the data were not sampled. However, there are not obvious reasons why independence would not hold for most or all observations.</p>
</dd>
<dt>Approximately normal.</dt>
<dd><p>As with one- and two-sample testing for means, the normality assumption is especially important when the sample size is quite small. The normal probability plots for each group of the MLB data are shown in Figure [mlbANOVADiagNormalityGroups]; there is some deviation from normality for infielders, but this isn’t a substantial concern since there are about 150 observations in that group and the outliers are not extreme. Sometimes in ANOVA there are so many groups or so few observations per group that checking normality for each group isn’t reasonable. See the footnote<a href="#fn149" class="footnoteRef" id="fnref149"><sup>149</sup></a> for guidance on how to handle such instances.</p>
<div class="figure">
<img src="04/figures/mlbANOVA/mlbANOVADiagNormalityGroups" alt="Normal probability plot of OBP for each field position." />
<p class="caption">Normal probability plot of OBP for each field position.</p>
</div>
<p>[mlbANOVADiagNormalityGroups]</p>
</dd>
<dt>Constant variance.</dt>
<dd><p>The last assumption is that the variance in the groups is about equal from one group to the next. This assumption can be checked by examining a side-by-side box plot of the outcomes across the groups, as in Figure . In this case, the variability is similar in the four groups but not identical. We see in Table  that the standard deviation varies a bit from one group to the next. Whether these differences are from natural variation is unclear, so we should report this uncertainty with the final results.</p>
</dd>
</dl>
<p><span>Diagnostics for an ANOVA analysis</span> <span>Independence is always important to an ANOVA analysis. The normality condition is very important when the sample sizes for each group are relatively small. The constant variance condition is especially important when the sample sizes differ between groups.</span></p>
</div>
<div id="multipleComparisonsAndControllingTheType1ErrorRate" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Multiple comparisons and controlling Type 1 Error rate</h3>
<p>When we reject the null hypothesis in an ANOVA analysis, we might wonder, which of these groups have different means? To answer this question, we compare the means of each possible pair of groups. For instance, if there are three groups and there is strong evidence that there are some differences in the group means, there are three comparisons to make: group 1 to group 2, group 1 to group 3, and group 2 to group 3. These comparisons can be accomplished using a two-sample <span class="math inline">\(t\)</span> test, but we use a modified significance level and a pooled estimate of the standard deviation across groups. Usually this pooled standard deviation can be found in the ANOVA table, e.g. along the bottom of Table [anovaSummaryTableForOBPAgainstPosition].</p>
<p><span>Example  discussed three statistics lectures, all taught during the same semester. Table [summaryStatisticsForClassTestData] shows summary statistics for these three courses, and a side-by-side box plot of the data is shown in Figure [classDataSBSBoxPlot]. We would like to conduct an ANOVA for these data. Do you see any deviations from the three conditions for ANOVA?</span> In this case (like many others) it is difficult to check independence in a rigorous way. Instead, the best we can do is use common sense to consider reasons the assumption of independence may not hold. For instance, the independence assumption may not be reasonable if there is a star teaching assistant that only half of the students may access; such a scenario would divide a class into two subgroups. No such situations were evident for these particular data, and we believe that independence is acceptable.</p>
<p>The distributions in the side-by-side box plot appear to be roughly symmetric and show no noticeable outliers.</p>
<p>The box plots show approximately equal variability, which can be verified in Table [summaryStatisticsForClassTestData], supporting the constant variance assumption.</p>
<table>
<caption>Summary statistics for the first midterm scores in three different lectures of the same course.</caption>
<thead>
<tr class="header">
<th align="left">Class <span class="math inline">\(i\)</span></th>
<th align="right">A</th>
<th align="right">B</th>
<th align="right">C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(n_i\)</span></td>
<td align="right">58</td>
<td align="right">55</td>
<td align="right">51</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\bar{x}_i\)</span></td>
<td align="right">75.1</td>
<td align="right">72.0</td>
<td align="right">78.9</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(s_i\)</span></td>
<td align="right">13.9</td>
<td align="right">13.8</td>
<td align="right">13.1</td>
</tr>
</tbody>
</table>
<p>[summaryStatisticsForClassTestData]</p>
<div class="figure">
<img src="04/figures/classData/classDataSBSBoxPlot" alt="Side-by-side box plot for the first midterm scores in three different lectures of the same course." />
<p class="caption">Side-by-side box plot for the first midterm scores in three different lectures of the same course.</p>
</div>
<p>[classDataSBSBoxPlot]</p>
<p>[exerExaminingAnovaSummaryTableForMidtermData] An ANOVA was conducted for the midterm data, and summary results are shown in Table [anovaSummaryTableForMidtermData]. What should we conclude?<a href="#fn150" class="footnoteRef" id="fnref150"><sup>150</sup></a></p>
<p><span>lrrrrr</span> &amp; Df &amp; Sum Sq &amp; Mean Sq &amp; F value &amp; Pr(<span class="math inline">\(&gt;\)</span>F)<br />
lecture &amp; 2 &amp; 1290.11 &amp; 645.06 &amp; 3.48 &amp; 0.0330<br />
Residuals &amp; 161 &amp; 29810.13 &amp; 185.16 &amp; &amp;<br />
[anovaSummaryTableForMidtermData]</p>
<p>There is strong evidence that the different means in each of the three classes is not simply due to chance. We might wonder, which of the classes are actually different? As discussed in earlier chapters, a two-sample <span class="math inline">\(t\)</span> test could be used to test for differences in each possible pair of groups. However, one pitfall was discussed in Example : when we run so many tests, the Type 1 Error rate increases. This issue is resolved by using a modified significance level.</p>
<p>The scenario of testing many pairs of groups is called <strong>multiple comparisons</strong>. The <strong>Bonferroni correction</strong> suggests that a more stringent significance level is more appropriate for these tests:</p>
<p><span class="math display">\[\begin{aligned}
\alpha^* = \alpha / K\end{aligned}\]</span></p>
<p>where <span class="math inline">\(K\)</span> is the number of comparisons being considered (formally or informally). If there are <span class="math inline">\(k\)</span> groups, then usually all possible pairs are compared and <span class="math inline">\(K=\frac{k(k-1)}{2}\)</span>.</p>
<p><span>In Guided Practice [exerExaminingAnovaSummaryTableForMidtermData], you found strong evidence of differences in the average midterm grades between the three lectures. Complete the three possible pairwise comparisons using the Bonferroni correction and report any differences.</span> [multipleComparisonsOfThreeStatClasses] We use a modified significance level of <span class="math inline">\(\alpha^* = 0.05/3 = 0.0167\)</span>. Additionally, we use the pooled estimate of the standard deviation: <span class="math inline">\(s_{pooled}=13.61\)</span> on <span class="math inline">\(df=161\)</span>, which is provided in the ANOVA summary table.</p>
<p>Lecture A versus Lecture B: The estimated difference and standard error are, respectively,</p>
<p><span class="math display">\[\begin{aligned}
\bar{x}_A - \bar{x}_{B} &amp;= 75.1 - 72 = 3.1
    &amp;SE = \sqrt{\frac{13.61^2}{58} + \frac{13.61^2}{55}} &amp;= 2.56\end{aligned}\]</span></p>
<p>(See Section  for additional details.) This results in a <span class="math inline">\(T\)</span> score of 1.21 on <span class="math inline">\(df = 161\)</span> (we use the <span class="math inline">\(df\)</span> associated with <span class="math inline">\(s_{pooled}\)</span>). Statistical software was used to precisely identify the two-tailed p-value since the modified significance of 0.0167 is not found in the <span class="math inline">\(t\)</span> table. The p-value (0.228) is larger than <span class="math inline">\(\alpha^*=0.0167\)</span>, so there is not strong evidence of a difference in the means of lectures A and B.</p>
<p>Lecture A versus Lecture C: The estimated difference and standard error are 3.8 and 2.61, respectively. This results in a <span class="math inline">\(T\)</span> score of 1.46 on <span class="math inline">\(df = 161\)</span> and a two-tailed p-value of 0.1462. This p-value is larger than <span class="math inline">\(\alpha^*\)</span>, so there is not strong evidence of a difference in the means of lectures A and C.</p>
<p>Lecture B versus Lecture C: The estimated difference and standard error are 6.9 and 2.65, respectively. This results in a <span class="math inline">\(T\)</span> score of 2.60 on <span class="math inline">\(df = 161\)</span> and a two-tailed p-value of 0.0102. This p-value is smaller than <span class="math inline">\(\alpha^*\)</span>. Here we find strong evidence of a difference in the means of lectures B and C.</p>
<p>We might summarize the findings of the analysis from Example [multipleComparisonsOfThreeStatClasses] using the following notation:</p>
<p><span class="math display">\[\begin{aligned}
\mu_A &amp;\stackrel{?}{=} \mu_B
    &amp;\mu_A &amp;\stackrel{?}{=} \mu_C
    &amp;\mu_B &amp;\neq \mu_C\end{aligned}\]</span></p>
<p>The midterm mean in lecture A is not statistically distinguishable from those of lectures B or C. However, there is strong evidence that lectures B and C are different. In the first two pairwise comparisons, we did not have sufficient evidence to reject the null hypothesis. Recall that failing to reject <span class="math inline">\(H_0\)</span> does not imply <span class="math inline">\(H_0\)</span> is true.</p>
<p><span>Sometimes an ANOVA will reject the null but no groups will have statistically significant differences</span> <span>It is possible to reject the null hypothesis using ANOVA and then to not subsequently identify differences in the pairwise comparisons. However, <em>this does not invalidate the ANOVA conclusion</em>. It only means we have not been able to successfully identify which groups differ in their means.</span></p>
<p>The ANOVA procedure examines the big picture: it considers all groups simultaneously to decipher whether there is evidence that some difference exists. Even if the test indicates that there is strong evidence of differences in group means, identifying with high confidence a specific difference as statistically significant is more difficult.</p>
<p>Consider the following analogy: we observe a Wall Street firm that makes large quantities of money based on predicting mergers. Mergers are generally difficult to predict, and if the prediction success rate is extremely high, that may be considered sufficiently strong evidence to warrant investigation by the Securities and Exchange Commission (SEC). While the SEC may be quite certain that there is insider trading taking place at the firm, the evidence against any single trader may not be very strong. It is only when the SEC considers all the data that they identify the pattern. This is effectively the strategy of ANOVA: stand back and consider all the groups simultaneously.</p>
</div>
</div>
<div id="bootstrapping-to-study-the-standard-deviation" class="section level2">
<h2><span class="header-section-number">4.5</span> Bootstrapping to study the standard deviation</h2>
<p>We analyzed textbook pricing data in Section [pairedData] and found that prices on Amazon were statistically significantly cheaper on average. We might also want to better understand the variability of the price difference from one book to another, which we quantified using the standard deviation: <span class="math inline">\(s = \$14.26\)</span>. The sample standard deviation is a point estimate for the population standard deviation. Just as we care about the precision of a sample mean, we may care about the precise of the sample standard deviation.</p>
<div id="bootstrap-samples-and-distributions" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Bootstrap samples and distributions</h3>
<p>The theory required to quantify the uncertainty of the sample standard deviation is complex. In an ideal world, we would sample data from the population again and recompute the standard deviation with this new sample. Then we could do it again. And again. And so on until we get enough standard deviation estimates that we have a good sense of the precision of our original estimate. This is an ideal world where sampling data is free or extremely cheap. That is rarely the case, which poses a challenge to this “resample from the population” approach.</p>
<p>However, we can sample from the sample. In the textbook pricing example, there are 73 price differences. This sample can serve as a proxy for the population: we sample from this data set to get a sense for what it would be like if we took new samples.</p>
<p>A <strong>bootstrap sample</strong> is a sample of the original sample. In the case of the textbook data, we proceed as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Randomly sample one observation from the 73 price differences.</p></li>
<li><p>Randomly sample a second observation from the 73 price differences. There is a chance that this second observation will be the same one sampled in the first step.<br />
<span class="math inline">\(\vdots\)</span></p></li>
<li><p>Randomly sample a 73<span class="math inline">\(^{rd}\)</span> observation from the 73 price differences.</p></li>
</ol>
<p>This type of sampling is called <strong>sampling with replacement</strong>. Table [textbookBootstrapSample] shows a bootstrap sample for the textbook pricing example. Some of the values, such as , are duplicated since occasionally we sample the same observation multiple times.</p>
<table>
<caption>A bootstrap sample of the textbook price differences, which represents a sample of 73 values from the original 73 observations, where we are sampling with replacement. In sampling with replacement, it is possible for a value to be sampled multiple times. For example, was sampled twice in this bootstrap sample.</caption>
<tbody>
<tr class="odd">
<td></td>
<td align="center">6.63</td>
<td align="center">5.39</td>
<td align="center">6.39</td>
<td align="center">14.05</td>
<td align="center">6.63</td>
<td align="center">-0.25</td>
<td align="center">12.45</td>
<td align="center">-0.22</td>
<td align="center">9.45</td>
<td align="center">9.45</td>
</tr>
<tr class="even">
<td>11.70</td>
<td align="center">39.08</td>
<td align="center">4.80</td>
<td align="center">28.72</td>
<td align="center">9.45</td>
<td align="center">-0.25</td>
<td align="center">-3.88</td>
<td align="center">2.82</td>
<td align="center">45.34</td>
<td align="center">28.72</td>
<td align="center">16.62</td>
</tr>
<tr class="odd">
<td>38.35</td>
<td align="center">4.74</td>
<td align="center">44.40</td>
<td align="center">3.74</td>
<td align="center">1.75</td>
<td align="center">2.84</td>
<td align="center">30.25</td>
<td align="center">3.35</td>
<td align="center">6.63</td>
<td align="center">30.50</td>
<td align="center">0.00</td>
</tr>
<tr class="even">
<td>4.96</td>
<td align="center">6.39</td>
<td align="center">9.48</td>
<td align="center"></td>
<td align="center">66.00</td>
<td align="center">44.40</td>
<td align="center">-0.25</td>
<td align="center">-2.55</td>
<td align="center">17.98</td>
<td align="center">2.82</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>29.29</td>
<td align="center">9.22</td>
<td align="center">11.70</td>
<td align="center">9.31</td>
<td align="center">4.80</td>
<td align="center">13.63</td>
<td align="center">9.45</td>
<td align="center">38.23</td>
<td align="center">4.96</td>
<td align="center">19.69</td>
<td align="center"></td>
</tr>
<tr class="even">
<td>14.26</td>
<td align="center">12.45</td>
<td align="center">5.39</td>
<td align="center">-0.28</td>
<td align="center">8.23</td>
<td align="center">0.42</td>
<td align="center">2.82</td>
<td align="center">4.78</td>
<td align="center">7.01</td>
<td align="center">4.64</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td>9.12</td>
<td align="center">9.31</td>
<td align="center">9.12</td>
<td align="center">11.70</td>
<td align="center">27.15</td>
<td align="center">28.72</td>
<td align="center">30.71</td>
<td align="center">2.84</td>
<td align="center">-9.53</td>
<td align="center">14.05</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>[textbookBootstrapSample]</p>
<p>A bootstrap sample behaves similarly to how an actual sample would behave, and we compute the point estimate of interest. In the textbook price example, we compute the standard deviation of the bootstrap sample: $13.98.</p>
</div>
<div id="inference-using-the-bootstrap" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Inference using the bootstrap</h3>
<p>One bootstrap sample is not enough to understand the uncertainty of the standard deviation, so we need to collect another bootstrap sample and compute the standard deviation: $16.21. And another: $14.07. And so on. Using a computer, we took 10,000 bootstrap samples and computed the standard deviation for each, and these are summarized in . This is called the <strong>bootstrap distribution</strong> of the standard deviation for the textbook price differences. To make use of this distribution, we make an important assumption: the bootstrap distribution shown in Figure [textbookBootstrapSamplingForSD] is similar to the sampling distribution of the standard deviation. This assumption is reasonable when doing an informal exploration of the uncertainty of an estimate, and under certain conditions, we can rely on it for more formal inference methods.</p>
<div class="figure">
<img src="04/figures/textbooksS10/textbookBootstrapSamplingForSD" alt="Bootstrap distribution for the standard deviation of textbook price differences. The distribution is approximately centered at the original sample’s standard deviation, $14.26." />
<p class="caption">Bootstrap distribution for the standard deviation of textbook price differences. The distribution is approximately centered at the original sample’s standard deviation, $14.26.</p>
</div>
<p>[textbookBootstrapSamplingForSD]</p>
<p><span>Describe the bootstrap distribution for the standard deviation shown in Figure [textbookBootstrapSamplingForSD].</span> The distribution is symmetric, bell-shaped, and centered near $14.26, which is the point estimate from the original data. The standard deviation of the bootstrap distribution is $1.60, and most observations in this distribution lie between $11 and $17.</p>
<p>In this example, the bootstrap distribution’s standard deviation, $1.60, quantifies the uncertainty of the point estimate. This is an estimate of the standard error based on the bootstrap. We might be tempted to use it for a 95% confidence interval, but first we must perform some due diligence. As with every statistical method, we must check certain conditions before performing formal inference using the bootstrap.</p>
<p>The bootstrap distribution for the standard deviation will be a good approximation of the sampling distribution for the standard deviation when</p>
<ol style="list-style-type: decimal">
<li><p>observations in the original sample are independent,</p></li>
<li><p>the original sample size is at least 30, and</p></li>
<li><p>the bootstrap distribution is nearly normal.</p></li>
</ol>
<p>We’re already familiar with checking independence of observations, which we previously checked for this data set, and the second condition is easy to check. The last condition can be checked by examining the bootstrap distribution using a normal probability plot, as shown in Figure [textbookBootstrapSamplingQQPlotForSD]. In this example, we see a very straight line, which indicates the bootstrap distribution is nearly normal, and we can move forward with constructing a confidence interval.</p>
<div class="figure">
<img src="04/figures/textbooksS10/textbookBootstrapSamplingQQPlotForSD" alt="Normal probability plot for the bootstrap distribution." />
<p class="caption">Normal probability plot for the bootstrap distribution.</p>
</div>
<p>[textbookBootstrapSamplingQQPlotForSD]</p>
<p>As with many other point estimates, we will use the familiar formula</p>
<p><span class="math display">\[\begin{aligned}
\text{point estimate} \pm t_{df}^{\star} \times SE\end{aligned}\]</span></p>
<p>In the textbook example, using <span class="math inline">\(df = 73 - 1 = 72\)</span> leads to <span class="math inline">\(t_{72}^{\star} = 1.99\)</span> for a 95% confidence level. For bootstrapping, the standard error is computed as the standard deviation of the bootstrap distribution.</p>
<p><span>Compute the 95% confidence interval for the standard deviation of the textbook price difference.</span> We use the general formula for a 95% confidence interval with the <span class="math inline">\(t\)</span> distribution:</p>
<p><span class="math display">\[\begin{aligned}
\text{point estimate} &amp;\pm t_{df}^{\star} \times SE \\
14.26 &amp;\pm 1.99 \times 1.60 \\
(\$11.08, &amp;\$17.44)\end{aligned}\]</span></p>
<p>We are 95% confident that the standard deviation of the textbook price differences is between $11.08 and $17.44.</p>
<p>Had we wanted to conduct a hypothesis test, we could have used the point estimate and standard error for a t test as we have in previous sections.</p>
<p><span> The bootstrap may be used with any parameters using the same conditions as were provided for the standard deviation. However, in other situations, it may be more important to examine the validity of the third condition: that the bootstrap distribution is nearly normal.</span></p>
</div>
<div id="frequently-asked-questions" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Frequently asked questions</h3>
<dl>
<dt>There are more types of bootstrap techniques, right?</dt>
<dd><p>Yes! There are many excellent bootstrap techniques. We have only chosen to present one bootstrap technique that could be explained in a single section and is also reasonably reliable.</p>
</dd>
<dt>Can we use the bootstrap for the mean or difference of means?</dt>
<dd><p>Technically, yes. However, the methods introduced earlier tend to be more reliable than this particular bootstrapping method and other simple bootstrapping techniques. See the following page for details on an investigation into the accuracy of several bootstrapping methods as well as the <span class="math inline">\(t\)</span> distribution method introduced earlier in this chapter:</p>
<p><a href="http://www.openintro.org/stat/bootstrap">www.openintro.org/stat/bootstrap</a></p>
</dd>
<dt>I’ve heard a technique called the percentile bootstrap that is very robust.</dt>
<dd><p> <br />
It is a held belief that the percentile bootstrap is a robust bootstrap method. That is false. The percentile method is one of the least reliable bootstrap methods. Instead, use the method described in this section, which is more reliable, or learn about more advanced techniques.</p>
</dd>
</dl>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="123">
<li id="fn123"><p>Diez DM, Barr CD, and Çetinkaya-Rundel M. 2012. <em>openintro</em>: OpenIntro data sets and supplemental functions. <a href="http://cran.r-project.org/web/packages/openintro">cran.r-project.org/web/packages/openintro</a>.<a href="inferenceForNumericalData.html#fnref123">↩</a></p></li>
<li id="fn124"><p>Yes. Constant variability, nearly normal residuals, and linearity all appear reasonable.<a href="inferenceForNumericalData.html#fnref124">↩</a></p></li>
<li id="fn125"><p><span class="math inline">\(\hat{y} = 36.21 + 5.13x_1 + 1.08x_2 - 0.03x_3 + 7.29x_4\)</span>, and there are <span class="math inline">\(k=4\)</span> predictor variables.<a href="inferenceForNumericalData.html#fnref125">↩</a></p></li>
<li id="fn126"><p>It is the average difference in auction price for each additional Wii wheel included when holding the other variables constant. The point estimate is <span class="math inline">\(b_4 = 7.29\)</span>.<a href="inferenceForNumericalData.html#fnref126">↩</a></p></li>
<li id="fn127"><p><span class="math inline">\(e_i = y_i - \hat{y_i} = 51.55 - 49.62 = 1.93\)</span>, where 49.62 was computed using the variables values from the observation and the equation identified in Guided Practice [eqForMultipleRegrOfTotalPrForAllPredictorsWithCoefficients].<a href="inferenceForNumericalData.html#fnref127">↩</a></p></li>
<li id="fn128"><p>Three of the variables (, , and <strong>wheels</strong>) do take value 0, but the auction duration is always one or more days. If the auction is not up for any days, then no one can bid on it! That means the total auction price would always be zero for such an auction; the interpretation of the intercept in this setting is not insightful.<a href="inferenceForNumericalData.html#fnref128">↩</a></p></li>
<li id="fn129"><p><span class="math inline">\(R^2 = 1 - \frac{23.34}{83.06} = 0.719\)</span>.<a href="inferenceForNumericalData.html#fnref129">↩</a></p></li>
<li id="fn130"><p>In multiple regression, the degrees of freedom associated with the variance of the estimate of the residuals is <span class="math inline">\(n-k-1\)</span>, not <span class="math inline">\(n-1\)</span>. For instance, if we were to make predictions for new data using our current model, we would find that the unadjusted <span class="math inline">\(R^2\)</span> is an overly optimistic estimate of the reduction in variance in the response, and using the degrees of freedom in the adjusted <span class="math inline">\(R^2\)</span> formula helps correct this bias.<a href="inferenceForNumericalData.html#fnref130">↩</a></p></li>
<li id="fn131"><p><span class="math inline">\(R_{adj}^2 = 1 - \frac{23.34}{83.06}\times \frac{141-1}{141-4-1} = 0.711\)</span>.<a href="inferenceForNumericalData.html#fnref131">↩</a></p></li>
<li id="fn132"><p>The unadjusted <span class="math inline">\(R^2\)</span> would stay the same and the adjusted <span class="math inline">\(R^2\)</span> would go down.<a href="inferenceForNumericalData.html#fnref132">↩</a></p></li>
<li id="fn133"><p>The p-value for the auction duration is 0.8882, which indicates that there is not statistically significant evidence that the duration is related to the total auction price when accounting for the other variables. The p-value for the Wii wheels variable is about zero, indicating that this variable is associated with the total auction price.<a href="inferenceForNumericalData.html#fnref133">↩</a></p></li>
<li id="fn134"><p>An especially rigorous check would use <strong>time series</strong> methods. For instance, we could check whether consecutive residuals are correlated. Doing so with these residuals yields no statistically significant correlations.<a href="inferenceForNumericalData.html#fnref134">↩</a></p></li>
<li id="fn135"><p>Recall from Chapter [linRegrForTwoVar] that if outliers are present in predictor variables, the corresponding observations may be especially influential on the resulting model. This is the motivation for omitting the numerical variables, such as the number of characters and line breaks in emails, that we saw in Chapter [introductionToData]. These variables exhibited extreme skew. We could resolve this issue by transforming these variables (e.g. using a log-transformation), but we will omit this further investigation for brevity.<a href="inferenceForNumericalData.html#fnref135">↩</a></p></li>
<li id="fn136"><p>The new estimate is different: -2.87. This new value represents the estimated coefficient when we are also accounting for other variables in the logistic regression model.<a href="inferenceForNumericalData.html#fnref136">↩</a></p></li>
<li id="fn137"><p>In this particular application, we should err on the side of sending more mail to the inbox rather than mistakenly putting good messages in the spambox. So, in summary: emails in the first and last categories go to the regular inbox, and those in the second scenario go to the spambox.<a href="inferenceForNumericalData.html#fnref137">↩</a></p></li>
<li id="fn138"><p>First, note that we proposed a cutoff for the predicted probability of 0.95 for spam. In a worst case scenario, all the messages in the spambox had the minimum probability equal to about 0.95. Thus, we should expect to find about 5 or fewer legitimate messages among the 100 messages placed in the spambox.<a href="inferenceForNumericalData.html#fnref138">↩</a></p></li>
<li id="fn139"><p>Absolutely not. It is possible that there is some difference but we did not detect it. If there is a difference, we made a Type 2 Error. Notice: we also don’t have enough information to, if there is an actual difference difference, confidently say which direction that difference would be in.<a href="inferenceForNumericalData.html#fnref139">↩</a></p></li>
<li id="fn140"><p>We could have collected more data. If the sample sizes are larger, we tend to have a better shot at finding a difference if one exists.<a href="inferenceForNumericalData.html#fnref140">↩</a></p></li>
<li id="fn141"><p>Because the teacher did not expect one exam to be more difficult prior to examining the test results, she should use a two-sided hypothesis test. <span class="math inline">\(H_0\)</span>: the exams are equally difficult, on average. <span class="math inline">\(\mu_A - \mu_B = 0\)</span>. <span class="math inline">\(H_A\)</span>: one exam was more difficult than the other, on average. <span class="math inline">\(\mu_A - \mu_B \neq 0\)</span>.<a href="inferenceForNumericalData.html#fnref141">↩</a></p></li>
<li id="fn142"><p>(a) It is probably reasonable to conclude the scores are independent, provided there was no cheating. (b) The summary statistics suggest the data are roughly symmetric about the mean, and it doesn’t seem unreasonable to suggest the data might be normal. Note that since these samples are each nearing 30, moderate skew in the data would be acceptable. (c) It seems reasonable to suppose that the samples are independent since the exams were handed out randomly.<a href="inferenceForNumericalData.html#fnref142">↩</a></p></li>
<li id="fn143"><p><span class="math inline">\(H_0\)</span>: The average on-base percentage is equal across the four positions. <span class="math inline">\(H_A\)</span>: The average on-base percentage varies across some (or all) groups.<a href="inferenceForNumericalData.html#fnref143">↩</a></p></li>
<li id="fn144"><p>See, for example, <a href="http://www.stat.columbia.edu/~cook/movabletype/archives/2007/05/the_prosecutors.html">www.stat.columbia.edu/<span class="math inline">\(\sim\)</span>cook/movabletype/archives/2007/05/the_prosecutors.html</a>.<a href="inferenceForNumericalData.html#fnref144">↩</a></p></li>
<li id="fn145"><p>Let <span class="math inline">\(\bar{x}\)</span> represent the mean of outcomes across all groups. Then the mean square between groups is computed as</p>
<p><span class="math display">\[\begin{aligned}
MSG = \frac{1}{df_{G}}SSG = \frac{1}{k-1}\sum_{i=1}^{k} n_{i}\left(\bar{x}_{i} - \bar{x}\right)^2\end{aligned}\]</span></p>
<p>where <span class="math inline">\(SSG\)</span> is called the <strong>sum of squares between groups</strong> and <span class="math inline">\(n_{i}\)</span> is the sample size of group <span class="math inline">\(i\)</span>.<a href="inferenceForNumericalData.html#fnref145">↩</a></p></li>
<li id="fn146"><p>Let <span class="math inline">\(\bar{x}\)</span> represent the mean of outcomes across all groups. Then the <strong>sum of squares total (<span class="math inline">\(SST\)</span>)</strong> is computed as <span class="math inline">\(SST = \sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^2\)</span>, where the sum is over all observations in the data set. Then we compute the <strong>sum of squared errors (<span class="math inline">\(SSE\)</span>)</strong> in one of two equivalent ways:</p>
<p><span class="math display">\[\begin{aligned}
SSE = SST - SSG = (n_1-1)s_1^2 + (n_2-1)s_2^2 + \cdots + (n_k-1)s_k^2\end{aligned}\]</span></p>
<p>where <span class="math inline">\(s_i^2\)</span> is the sample variance (square of the standard deviation) of the residuals in group <span class="math inline">\(i\)</span>. Then the <span class="math inline">\(MSE\)</span> is the standardized form of <span class="math inline">\(SSE\)</span>: <span class="math inline">\(MSE = \frac{1}{df_{E}}SSE\)</span>.<a href="inferenceForNumericalData.html#fnref146">↩</a></p></li>
<li id="fn147"><p>There are <span class="math inline">\(k=4\)</span> groups, so <span class="math inline">\(df_{G} = k-1 = 3\)</span>. There are <span class="math inline">\(n = n_1 + n_2 + n_3 + n_4 = 327\)</span> total observations, so <span class="math inline">\(df_{E} = n - k = 323\)</span>. Then the <span class="math inline">\(F\)</span> statistic is computed as the ratio of <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span>: <span class="math inline">\(F = \frac{MSG}{MSE} = \frac{0.00252}{0.00127} = 1.984 \approx 1.994\)</span>. (<span class="math inline">\(F=1.994\)</span> was computed by using values for <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span> that were not rounded.)<a href="inferenceForNumericalData.html#fnref147">↩</a></p></li>
<li id="fn148"><p> </p>
<p><br />
<img src="04/figures/fDist3And323/fDist3And323Shaded" alt="image" /><a href="inferenceForNumericalData.html#fnref148">↩</a></p></li>
<li id="fn149"><p>First calculate the of the baseball data, which are calculated by taking the observed values and subtracting the corresponding group means. For example, an outfielder with OBP of 0.435 would have a residual of <span class="math inline">\(0.405 - \bar{x}_{OF} = 0.071\)</span>. Then to check the normality condition, create a normal probability plot using all the residuals simultaneously.<a href="inferenceForNumericalData.html#fnref149">↩</a></p></li>
<li id="fn150"><p>The p-value of the test is 0.0330, less than the default significance level of 0.05. Therefore, we reject the null hypothesis and conclude that the difference in the average midterm scores are not due to chance.<a href="inferenceForNumericalData.html#fnref150">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferenceForCategoricalData.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linRegrForTwoVar.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/programmingforpsych/blob/master/04-V01.Rmd",
"text": "Edit"
},
"download": ["Programming_Crump.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
