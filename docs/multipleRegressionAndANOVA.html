<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Answering questions with data</title>
  <meta name="description" content="An introductory statistics textbook for psychology students">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Answering questions with data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introductory statistics textbook for psychology students" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Answering questions with data" />
  
  <meta name="twitter:description" content="An introductory statistics textbook for psychology students" />
  

<meta name="author" content="Author List, TBD">
<meta name="author" content="Current Contributions, Matthew J. C. Crump">
<meta name="author" content="Adapted work so far from Navarro, D., Diaz, Barr, &amp; Cetinkaya-Rundel">
<meta name="author" content="In Draft subject to change, we will get all attributions and licenses done correctly">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linRegrForTwoVar.html">
<link rel="next" href="inferenceForCategoricalData.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script type="text/javascript">
mattcrump=1;
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="tufte.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#important-notes"><i class="fa fa-check"></i><b>0.1</b> Important notes</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="why-statistics.html"><a href="why-statistics.html"><i class="fa fa-check"></i><b>1</b> Why Statistics?</a><ul>
<li class="chapter" data-level="1.1" data-path="why-statistics.html"><a href="why-statistics.html#on-the-psychology-of-statistics"><i class="fa fa-check"></i><b>1.1</b> On the psychology of statistics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="why-statistics.html"><a href="why-statistics.html#the-curse-of-belief-bias"><i class="fa fa-check"></i><b>1.1.1</b> The curse of belief bias</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="why-statistics.html"><a href="why-statistics.html#the-cautionary-tale-of-simpsons-paradox"><i class="fa fa-check"></i><b>1.2</b> The cautionary tale of Simpson’s paradox</a></li>
<li class="chapter" data-level="1.3" data-path="why-statistics.html"><a href="why-statistics.html#statistics-in-psychology"><i class="fa fa-check"></i><b>1.3</b> Statistics in psychology</a></li>
<li class="chapter" data-level="1.4" data-path="why-statistics.html"><a href="why-statistics.html#statistics-in-everyday-life"><i class="fa fa-check"></i><b>1.4</b> Statistics in everyday life</a></li>
<li class="chapter" data-level="1.5" data-path="why-statistics.html"><a href="why-statistics.html#theres-more-to-research-methods-than-statistics"><i class="fa fa-check"></i><b>1.5</b> There’s more to research methods than statistics</a></li>
<li class="chapter" data-level="1.6" data-path="why-statistics.html"><a href="why-statistics.html#a-brief-introduction-to-research-designchstudydesign"><i class="fa fa-check"></i><b>1.6</b> A brief introduction to research design[ch:studydesign]</a><ul>
<li class="chapter" data-level="1.6.1" data-path="why-statistics.html"><a href="why-statistics.html#introduction-to-psychological-measurementsecmeasurement"><i class="fa fa-check"></i><b>1.6.1</b> Introduction to psychological measurement [sec:measurement]</a></li>
<li class="chapter" data-level="1.6.2" data-path="why-statistics.html"><a href="why-statistics.html#scales-of-measurementsecscales"><i class="fa fa-check"></i><b>1.6.2</b> Scales of measurement[sec:scales]</a></li>
<li class="chapter" data-level="1.6.3" data-path="why-statistics.html"><a href="why-statistics.html#assessing-the-reliability-of-a-measurementsecreliability"><i class="fa fa-check"></i><b>1.6.3</b> Assessing the reliability of a measurement [sec:reliability]</a></li>
<li class="chapter" data-level="1.6.4" data-path="why-statistics.html"><a href="why-statistics.html#the-role-of-variables-predictors-and-outcomes-secivdv"><i class="fa fa-check"></i><b>1.6.4</b> The “role” of variables: predictors and outcomes [sec:ivdv]</a></li>
<li class="chapter" data-level="1.6.5" data-path="why-statistics.html"><a href="why-statistics.html#experimental-and-non-experimental-researchsecresearchdesigns"><i class="fa fa-check"></i><b>1.6.5</b> Experimental and non-experimental research [sec:researchdesigns]</a></li>
<li class="chapter" data-level="1.6.6" data-path="why-statistics.html"><a href="why-statistics.html#assessing-the-validity-of-a-studysecvalidity"><i class="fa fa-check"></i><b>1.6.6</b> Assessing the validity of a study [sec:validity]</a></li>
<li class="chapter" data-level="1.6.7" data-path="why-statistics.html"><a href="why-statistics.html#confounds-artifacts-and-other-threats-to-validity"><i class="fa fa-check"></i><b>1.6.7</b> Confounds, artifacts and other threats to validity</a></li>
<li class="chapter" data-level="1.6.8" data-path="why-statistics.html"><a href="why-statistics.html#summary"><i class="fa fa-check"></i><b>1.6.8</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="DescribingData.html"><a href="DescribingData.html"><i class="fa fa-check"></i><b>2</b> Describing Data</a><ul>
<li class="chapter" data-level="2.1" data-path="DescribingData.html"><a href="DescribingData.html#this-is-what-too-many-numbers-looks-like"><i class="fa fa-check"></i><b>2.1</b> This is what too many numbers looks like</a></li>
<li class="chapter" data-level="2.2" data-path="DescribingData.html"><a href="DescribingData.html#look-at-the-data"><i class="fa fa-check"></i><b>2.2</b> Look at the data</a><ul>
<li class="chapter" data-level="2.2.1" data-path="DescribingData.html"><a href="DescribingData.html#stop-plotting-time-o-o-oh-u-can-plot-this"><i class="fa fa-check"></i><b>2.2.1</b> Stop, plotting time (o o oh) U can plot this</a></li>
<li class="chapter" data-level="2.2.2" data-path="DescribingData.html"><a href="DescribingData.html#histograms"><i class="fa fa-check"></i><b>2.2.2</b> Histograms</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="DescribingData.html"><a href="DescribingData.html#important-ideas-distribution-central-tendency-and-variance"><i class="fa fa-check"></i><b>2.3</b> Important Ideas: Distribution, Central Tendency, and Variance</a></li>
<li class="chapter" data-level="2.4" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-central-tendency-sameness"><i class="fa fa-check"></i><b>2.4</b> Measures of Central Tendency (Sameness)</a><ul>
<li class="chapter" data-level="2.4.1" data-path="DescribingData.html"><a href="DescribingData.html#from-many-numbers-to-one"><i class="fa fa-check"></i><b>2.4.1</b> From many numbers to one</a></li>
<li class="chapter" data-level="2.4.2" data-path="DescribingData.html"><a href="DescribingData.html#mode"><i class="fa fa-check"></i><b>2.4.2</b> Mode</a></li>
<li class="chapter" data-level="2.4.3" data-path="DescribingData.html"><a href="DescribingData.html#median"><i class="fa fa-check"></i><b>2.4.3</b> Median</a></li>
<li class="chapter" data-level="2.4.4" data-path="DescribingData.html"><a href="DescribingData.html#mean"><i class="fa fa-check"></i><b>2.4.4</b> Mean</a></li>
<li class="chapter" data-level="2.4.5" data-path="DescribingData.html"><a href="DescribingData.html#what-does-the-mean-mean"><i class="fa fa-check"></i><b>2.4.5</b> What does the mean mean?</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="DescribingData.html"><a href="DescribingData.html#measures-of-variation-differentness"><i class="fa fa-check"></i><b>2.5</b> Measures of Variation (Differentness)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="DescribingData.html"><a href="DescribingData.html#the-range"><i class="fa fa-check"></i><b>2.5.1</b> The Range</a></li>
<li class="chapter" data-level="2.5.2" data-path="DescribingData.html"><a href="DescribingData.html#the-difference-scores"><i class="fa fa-check"></i><b>2.5.2</b> The Difference Scores</a></li>
<li class="chapter" data-level="2.5.3" data-path="DescribingData.html"><a href="DescribingData.html#the-variance"><i class="fa fa-check"></i><b>2.5.3</b> The Variance</a></li>
<li class="chapter" data-level="2.5.4" data-path="DescribingData.html"><a href="DescribingData.html#the-standard-deviation"><i class="fa fa-check"></i><b>2.5.4</b> The Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="DescribingData.html"><a href="DescribingData.html#using-descriptive-statistics-with-data"><i class="fa fa-check"></i><b>2.6</b> Using Descriptive Statistics with data</a></li>
<li class="chapter" data-level="2.7" data-path="DescribingData.html"><a href="DescribingData.html#rolling-your-own-descriptive-statistics"><i class="fa fa-check"></i><b>2.7</b> Rolling your own descriptive statistics</a><ul>
<li class="chapter" data-level="2.7.1" data-path="DescribingData.html"><a href="DescribingData.html#absolute-deviations"><i class="fa fa-check"></i><b>2.7.1</b> Absolute deviations</a></li>
<li class="chapter" data-level="2.7.2" data-path="DescribingData.html"><a href="DescribingData.html#other-sign-inverting-operations"><i class="fa fa-check"></i><b>2.7.2</b> Other sign-inverting operations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Correlation.html"><a href="Correlation.html"><i class="fa fa-check"></i><b>3</b> Correlation</a><ul>
<li class="chapter" data-level="3.1" data-path="Correlation.html"><a href="Correlation.html#if-something-caused-something-else-to-change-what-would-that-look-like"><i class="fa fa-check"></i><b>3.1</b> If something caused something else to change, what would that look like?</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Correlation.html"><a href="Correlation.html#charlie-and-the-chocolate-factory"><i class="fa fa-check"></i><b>3.1.1</b> Charlie and the Chocolate factory</a></li>
<li class="chapter" data-level="3.1.2" data-path="Correlation.html"><a href="Correlation.html#scatterplots"><i class="fa fa-check"></i><b>3.1.2</b> Scatterplots</a></li>
<li class="chapter" data-level="3.1.3" data-path="Correlation.html"><a href="Correlation.html#positive-negative-and-no-correlation"><i class="fa fa-check"></i><b>3.1.3</b> Positive, Negative, and No-Correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Correlation.html"><a href="Correlation.html#pearsons-r"><i class="fa fa-check"></i><b>3.2</b> Pearson’s r</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Correlation.html"><a href="Correlation.html#the-idea-of-co-variance"><i class="fa fa-check"></i><b>3.2.1</b> The idea of co-variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Correlation.html"><a href="Correlation.html#turning-the-numbers-into-a-measure-of-co-variance"><i class="fa fa-check"></i><b>3.3</b> Turning the numbers into a measure of co-variance</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Correlation.html"><a href="Correlation.html#co-variance-the-measure"><i class="fa fa-check"></i><b>3.3.1</b> Co-variance, the measure</a></li>
<li class="chapter" data-level="3.3.2" data-path="Correlation.html"><a href="Correlation.html#pearsons-r-we-there-yet"><i class="fa fa-check"></i><b>3.3.2</b> Pearson’s r we there yet</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Correlation.html"><a href="Correlation.html#examples-with-data"><i class="fa fa-check"></i><b>3.4</b> Examples with Data</a></li>
<li class="chapter" data-level="3.5" data-path="Correlation.html"><a href="Correlation.html#regression-a-mini-intro"><i class="fa fa-check"></i><b>3.5</b> Regression: A mini intro</a><ul>
<li class="chapter" data-level="3.5.1" data-path="Correlation.html"><a href="Correlation.html#the-best-fit-line"><i class="fa fa-check"></i><b>3.5.1</b> The best fit line</a></li>
<li class="chapter" data-level="3.5.2" data-path="Correlation.html"><a href="Correlation.html#lines"><i class="fa fa-check"></i><b>3.5.2</b> Lines</a></li>
<li class="chapter" data-level="3.5.3" data-path="Correlation.html"><a href="Correlation.html#computing-the-best-fit-line"><i class="fa fa-check"></i><b>3.5.3</b> Computing the best fit line</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="Correlation.html"><a href="Correlation.html#interpreting-correlations"><i class="fa fa-check"></i><b>3.6</b> Interpreting Correlations</a><ul>
<li class="chapter" data-level="3.6.1" data-path="Correlation.html"><a href="Correlation.html#correlation-does-not-equal-causation"><i class="fa fa-check"></i><b>3.6.1</b> Correlation does not equal causation</a></li>
<li class="chapter" data-level="3.6.2" data-path="Correlation.html"><a href="Correlation.html#correlation-and-random-chance"><i class="fa fa-check"></i><b>3.6.2</b> Correlation and Random chance</a></li>
<li class="chapter" data-level="3.6.3" data-path="Correlation.html"><a href="Correlation.html#some-more-movies"><i class="fa fa-check"></i><b>3.6.3</b> Some more movies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html"><i class="fa fa-check"></i><b>4</b> Sampling and estimation</a><ul>
<li class="chapter" data-level="4.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#samples-populations-and-sampling"><i class="fa fa-check"></i><b>4.1</b> Samples, populations and sampling</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#defining-a-population"><i class="fa fa-check"></i><b>4.1.1</b> Defining a population</a></li>
<li class="chapter" data-level="4.1.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#simple-random-samples"><i class="fa fa-check"></i><b>4.1.2</b> Simple random samples</a></li>
<li class="chapter" data-level="4.1.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#most-samples-are-not-simple-random-samples"><i class="fa fa-check"></i><b>4.1.3</b> Most samples are not simple random samples</a></li>
<li class="chapter" data-level="4.1.4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#how-much-does-it-matter-if-you-dont-have-a-simple-random-sample"><i class="fa fa-check"></i><b>4.1.4</b> How much does it matter if you don’t have a simple random sample?</a></li>
<li class="chapter" data-level="4.1.5" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#population-parameters-and-sample-statistics"><i class="fa fa-check"></i><b>4.1.5</b> Population parameters and sample statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#the-law-of-large-numbers"><i class="fa fa-check"></i><b>4.2</b> The law of large numbers</a></li>
<li class="chapter" data-level="4.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#sampling-distributions-and-the-central-limit-theorem"><i class="fa fa-check"></i><b>4.3</b> Sampling distributions and the central limit theorem</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#sampling-distribution-of-the-sample-means"><i class="fa fa-check"></i><b>4.3.1</b> Sampling distribution of the sample means</a></li>
<li class="chapter" data-level="4.3.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#chance-can-produce-differences"><i class="fa fa-check"></i><b>4.3.2</b> Chance can produce differences</a></li>
<li class="chapter" data-level="4.3.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#differences-due-to-chance-can-be-simulated"><i class="fa fa-check"></i><b>4.3.3</b> Differences due to chance can be simulated</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#chance-makes-some-differences-more-likely-than-others"><i class="fa fa-check"></i><b>4.4</b> Chance makes some differences more likely than others</a></li>
<li class="chapter" data-level="4.5" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#the-crump-test"><i class="fa fa-check"></i><b>4.5</b> The Crump Test</a><ul>
<li class="chapter" data-level="4.5.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#intuitive-methods"><i class="fa fa-check"></i><b>4.5.1</b> Intuitive methods</a></li>
<li class="chapter" data-level="4.5.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#part-1-frequency-based-intuition-about-occurence"><i class="fa fa-check"></i><b>4.5.2</b> Part 1: Frequency based intuition about occurence</a></li>
<li class="chapter" data-level="4.5.3" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#part-2-simulating-chance"><i class="fa fa-check"></i><b>4.5.3</b> Part 2: Simulating chance</a></li>
<li class="chapter" data-level="4.5.4" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#part-3-judgment-and-decision-making"><i class="fa fa-check"></i><b>4.5.4</b> Part 3: Judgment and Decision-making</a></li>
<li class="chapter" data-level="4.5.5" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#part-4-experiment-design"><i class="fa fa-check"></i><b>4.5.5</b> Part 4: Experiment Design</a></li>
<li class="chapter" data-level="4.5.6" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#part-5-i-have-the-power"><i class="fa fa-check"></i><b>4.5.6</b> Part 5: I have the power</a></li>
<li class="chapter" data-level="4.5.7" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#summary-of-crump-test"><i class="fa fa-check"></i><b>4.5.7</b> Summary of Crump Test</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#the-randomization-test-permutation-test"><i class="fa fa-check"></i><b>4.6</b> The randomization test (permutation test)</a><ul>
<li class="chapter" data-level="4.6.1" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#pretend-example-does-chewing-gum-improve-your-grades"><i class="fa fa-check"></i><b>4.6.1</b> Pretend example does chewing gum improve your grades?</a></li>
<li class="chapter" data-level="4.6.2" data-path="sampling-and-estimation.html"><a href="sampling-and-estimation.html#take-homes-so-far"><i class="fa fa-check"></i><b>4.6.2</b> Take homes so far</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inferenceForNumericalDatattest.html"><a href="inferenceForNumericalDatattest.html"><i class="fa fa-check"></i><b>5</b> t-Tests</a></li>
<li class="chapter" data-level="6" data-path="inferenceForNumericalDataANOVA.html"><a href="inferenceForNumericalDataANOVA.html"><i class="fa fa-check"></i><b>6</b> ANOVA</a></li>
<li class="chapter" data-level="7" data-path="repeated-measures-anova.html"><a href="repeated-measures-anova.html"><i class="fa fa-check"></i><b>7</b> Repeated Measures ANOVA</a></li>
<li class="chapter" data-level="8" data-path="factorial-anova.html"><a href="factorial-anova.html"><i class="fa fa-check"></i><b>8</b> Factorial ANOVA</a></li>
<li class="chapter" data-level="9" data-path="mixed-design-anova.html"><a href="mixed-design-anova.html"><i class="fa fa-check"></i><b>9</b> Mixed Design ANOVA</a></li>
<li class="chapter" data-level="10" data-path="linRegrForTwoVar.html"><a href="linRegrForTwoVar.html"><i class="fa fa-check"></i><b>10</b> Introduction to linear regression</a></li>
<li class="chapter" data-level="11" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html"><i class="fa fa-check"></i><b>11</b> Multiple and logistic regression</a><ul>
<li class="chapter" data-level="11.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#introductionToMultipleRegression"><i class="fa fa-check"></i><b>11.1</b> Introduction to multiple regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#twoSingleVariableModelsForMarioKartData"><i class="fa fa-check"></i><b>11.1.1</b> A single-variable model for the Mario Kart data</a></li>
<li class="chapter" data-level="11.1.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#includingAndAssessingManyVariablesInAModel"><i class="fa fa-check"></i><b>11.1.2</b> Including and assessing many variables in a model</a></li>
<li class="chapter" data-level="11.1.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#adjusted-r2-as-a-better-estimate-of-explained-variance"><i class="fa fa-check"></i><b>11.1.3</b> Adjusted <span class="math inline">\(R^2\)</span> as a better estimate of explained variance</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelSelection"><i class="fa fa-check"></i><b>11.2</b> Model selection</a><ul>
<li class="chapter" data-level="11.2.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#identifying-variables-in-the-model-that-may-not-be-helpful"><i class="fa fa-check"></i><b>11.2.1</b> Identifying variables in the model that may not be helpful</a></li>
<li class="chapter" data-level="11.2.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#two-model-selection-strategies"><i class="fa fa-check"></i><b>11.2.2</b> Two model selection strategies</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#multipleRegressionModelAssumptions"><i class="fa fa-check"></i><b>11.3</b> Checking model assumptions using graphs</a></li>
<li class="chapter" data-level="11.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#logisticRegression"><i class="fa fa-check"></i><b>11.4</b> Logistic regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#email-data"><i class="fa fa-check"></i><b>11.4.1</b> Email data</a></li>
<li class="chapter" data-level="11.4.2" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#modelingTheProbabilityOfAnEvent"><i class="fa fa-check"></i><b>11.4.2</b> Modeling the probability of an event</a></li>
<li class="chapter" data-level="11.4.3" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#practical-decisions-in-the-email-application"><i class="fa fa-check"></i><b>11.4.3</b> Practical decisions in the email application</a></li>
<li class="chapter" data-level="11.4.4" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#diagnostics-for-the-email-classifier"><i class="fa fa-check"></i><b>11.4.4</b> Diagnostics for the email classifier</a></li>
<li class="chapter" data-level="11.4.5" data-path="multipleRegressionAndANOVA.html"><a href="multipleRegressionAndANOVA.html#improvingTheSetOfVariablesForASpamFilter"><i class="fa fa-check"></i><b>11.4.5</b> Improving the set of variables for a spam filter</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html"><i class="fa fa-check"></i><b>12</b> Inference for categorical data</a><ul>
<li class="chapter" data-level="12.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#singleProportion"><i class="fa fa-check"></i><b>12.1</b> Inference for a single proportion</a><ul>
<li class="chapter" data-level="12.1.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#when-the-sample-proportion-is-nearly-normal"><i class="fa fa-check"></i><b>12.1.1</b> When the sample proportion is nearly normal</a></li>
<li class="chapter" data-level="12.1.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#confIntForPropSection"><i class="fa fa-check"></i><b>12.1.2</b> Confidence intervals for a proportion</a></li>
<li class="chapter" data-level="12.1.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#htForPropSection"><i class="fa fa-check"></i><b>12.1.3</b> Hypothesis testing for a proportion</a></li>
<li class="chapter" data-level="12.1.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#choosing-a-sample-size-when-estimating-a-proportion"><i class="fa fa-check"></i><b>12.1.4</b> Choosing a sample size when estimating a proportion</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#differenceOfTwoProportions"><i class="fa fa-check"></i><b>12.2</b> Difference of two proportions</a><ul>
<li class="chapter" data-level="12.2.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#SampleDistributionOfTheDiffOfTwoProportions"><i class="fa fa-check"></i><b>12.2.1</b> Sample distribution of the difference of two proportions</a></li>
<li class="chapter" data-level="12.2.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#intervals-and-tests-for-p_1--p_2"><i class="fa fa-check"></i><b>12.2.2</b> Intervals and tests for <span class="math inline">\(p_1 -p_2\)</span></a></li>
<li class="chapter" data-level="12.2.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pooledHTForProportionsSection"><i class="fa fa-check"></i><b>12.2.3</b> Hypothesis testing when <span class="math inline">\(H_0: p_1=p_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#oneWayChiSquare"><i class="fa fa-check"></i><b>12.3</b> Testing for goodness of fit using chi-square (special topic)</a><ul>
<li class="chapter" data-level="12.3.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#creating-a-test-statistic-for-one-way-tables"><i class="fa fa-check"></i><b>12.3.1</b> Creating a test statistic for one-way tables</a></li>
<li class="chapter" data-level="12.3.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#chiSquareTestStatistic"><i class="fa fa-check"></i><b>12.3.2</b> The chi-square test statistic</a></li>
<li class="chapter" data-level="12.3.3" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-distribution-and-finding-areas"><i class="fa fa-check"></i><b>12.3.3</b> The chi-square distribution and finding areas</a></li>
<li class="chapter" data-level="12.3.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#pValueForAChiSquareTest"><i class="fa fa-check"></i><b>12.3.4</b> Finding a p-value for a chi-square distribution</a></li>
<li class="chapter" data-level="12.3.5" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#evaluating-goodness-of-fit-for-a-distribution"><i class="fa fa-check"></i><b>12.3.5</b> Evaluating goodness of fit for a distribution</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#twoWayTablesAndChiSquare"><i class="fa fa-check"></i><b>12.4</b> Testing for independence in two-way tables (special topic)</a><ul>
<li class="chapter" data-level="12.4.1" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#expected-counts-in-two-way-tables"><i class="fa fa-check"></i><b>12.4.1</b> Expected counts in two-way tables</a></li>
<li class="chapter" data-level="12.4.2" data-path="inferenceForCategoricalData.html"><a href="inferenceForCategoricalData.html#the-chi-square-test-for-two-way-tables"><i class="fa fa-check"></i><b>12.4.2</b> The chi-square test for two-way tables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="gifs.html"><a href="gifs.html"><i class="fa fa-check"></i><b>13</b> GIFs</a><ul>
<li class="chapter" data-level="13.1" data-path="gifs.html"><a href="gifs.html#correlation-gifs"><i class="fa fa-check"></i><b>13.1</b> Correlation GIFs</a><ul>
<li class="chapter" data-level="13.1.1" data-path="gifs.html"><a href="gifs.html#n10-both-variables-drawn-from-a-uniform-distribution"><i class="fa fa-check"></i><b>13.1.1</b> N=10, both variables drawn from a uniform distribution</a></li>
<li class="chapter" data-level="13.1.2" data-path="gifs.html"><a href="gifs.html#correlation-between-random-deviates-from-uniform-distribution-across-four-sample-sizes"><i class="fa fa-check"></i><b>13.1.2</b> Correlation between random deviates from uniform distribution across four sample sizes</a></li>
<li class="chapter" data-level="13.1.3" data-path="gifs.html"><a href="gifs.html#correlation-between-random-deviates-from-normal-distribution-across-four-sample-sizes"><i class="fa fa-check"></i><b>13.1.3</b> Correlation between random deviates from normal distribution across four sample sizes</a></li>
<li class="chapter" data-level="13.1.4" data-path="gifs.html"><a href="gifs.html#type-i-errors-sampling-random-deviates-from-normal-distribution-with-regression-lines"><i class="fa fa-check"></i><b>13.1.4</b> Type I errors, sampling random deviates from normal distribution with regression lines</a></li>
<li class="chapter" data-level="13.1.5" data-path="gifs.html"><a href="gifs.html#cell-size-and-correlation"><i class="fa fa-check"></i><b>13.1.5</b> Cell-size and correlation</a></li>
<li class="chapter" data-level="13.1.6" data-path="gifs.html"><a href="gifs.html#regression"><i class="fa fa-check"></i><b>13.1.6</b> Regression</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="gifs.html"><a href="gifs.html#sampling-distributions"><i class="fa fa-check"></i><b>13.2</b> Sampling distributions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="gifs.html"><a href="gifs.html#sampling-from-a-uniform-distribution"><i class="fa fa-check"></i><b>13.2.1</b> Sampling from a uniform distribution</a></li>
<li class="chapter" data-level="13.2.2" data-path="gifs.html"><a href="gifs.html#sampling-distribution-of-the-mean-normal-population-distribution-and-sample-histograms"><i class="fa fa-check"></i><b>13.2.2</b> Sampling distribution of the mean, Normal population distribution and sample histograms</a></li>
<li class="chapter" data-level="13.2.3" data-path="gifs.html"><a href="gifs.html#null-and-true-effect-samples-and-sampling-means"><i class="fa fa-check"></i><b>13.2.3</b> Null and True effect samples and sampling means</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="gifs.html"><a href="gifs.html#statistical-inference"><i class="fa fa-check"></i><b>13.3</b> Statistical Inference</a><ul>
<li class="chapter" data-level="13.3.1" data-path="gifs.html"><a href="gifs.html#randomization-test"><i class="fa fa-check"></i><b>13.3.1</b> Randomization Test</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Answering questions with data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multipleRegressionAndANOVA" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Multiple and logistic regression</h1>
<p>[multipleAndLogisticRegression]</p>
<p>The principles of simple linear regression lay the foundation for more sophisticated regression methods used in a wide range of challenging settings. In Chapter [multipleAndLogisticRegression], we explore multiple regression, which introduces the possibility of more than one predictor, and logistic regression, a technique for predicting categorical outcomes with two possible categories.</p>
<div id="introductionToMultipleRegression" class="section level2">
<h2><span class="header-section-number">11.1</span> Introduction to multiple regression</h2>
<p>Multiple regression extends simple two-variable regression to the case that still has one response but many predictors (denoted <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, <span class="math inline">\(x_3\)</span>, …). The method is motivated by scenarios where many variables may be simultaneously connected to an output.</p>
<p>We will consider Ebay auctions of a video game called <em>Mario Kart</em> for the Nintendo Wii. The outcome variable of interest is the total price of an auction, which is the highest bid plus the shipping cost. We will try to determine how total price is related to each characteristic in an auction while simultaneously controlling for other variables. For instance, all other characteristics held constant, are longer auctions associated with higher or lower prices? And, on average, how much more do buyers tend to pay for additional Wii wheels (plastic steering wheels that attach to the Wii controller) in auctions? Multiple regression will help us answer these and other questions.</p>
<p>The data set includes results from 141 auctions.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> Four observations from this data set are shown in Table [marioKartDataMatrix], and descriptions for each variable are shown in Table [marioKartVariables]. Notice that the condition and stock photo variables are indicator variables. For instance, the variable takes value 1 if the game up for auction is new and 0 if it is used. Using indicator variables in place of category names allows for these variables to be directly used in regression. See Section [categoricalPredictorsWithTwoLevels] for additional details. Multiple regression also allows for categorical variables with many levels, though we do not have any such variables in this analysis, and we save these details for a second or third course.</p>
<p><span>rrrrlr</span> &amp; price &amp; cond_</p>
<p>new &amp; stock_</p>
<p>photo &amp; duration &amp; wheels<br />
1 &amp; 51.55 &amp; 1 &amp; 1 &amp; 3 &amp; 1<br />
2 &amp; 37.04 &amp; 0 &amp; 1 &amp; 7 &amp; 1<br />
<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span> &amp;<span class="math inline">\(\vdots\)</span><br />
140 &amp; 38.76 &amp; 0 &amp; 0 &amp; 7 &amp; 0<br />
141 &amp; 54.51 &amp; 1 &amp; 1 &amp; 1 &amp; 2<br />
[marioKartDataMatrix]</p>
<table>
<caption>Variables and their descriptions for the data set.</caption>
<thead>
<tr class="header">
<th align="left"><span><strong>variable</strong></span></th>
<th align="left"><span><strong>description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>price</strong></td>
<td align="left">final auction price plus shipping costs, in US dollars</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">a coded two-level categorical variable, which takes value when the game is new and if the game is used</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">a coded two-level categorical variable, which takes value if the primary photo used in the auction was a stock photo and if the photo was unique to that auction</td>
</tr>
<tr class="even">
<td align="left"><strong>duration</strong></td>
<td align="left">the length of the auction, in days, taking values from 1 to 10</td>
</tr>
<tr class="odd">
<td align="left"><strong>wheels</strong></td>
<td align="left">the number of Wii wheels included with the auction (a <em>Wii wheel</em> is a plastic racing wheel that holds the Wii controller and is an optional but helpful accessory for playing Mario Kart)</td>
</tr>
</tbody>
</table>
<p>[marioKartVariables]</p>
<div id="twoSingleVariableModelsForMarioKartData" class="section level3">
<h3><span class="header-section-number">11.1.1</span> A single-variable model for the Mario Kart data</h3>
<p>Let’s fit a linear regression model with the game’s condition as a predictor of auction price. The model may be written as</p>
<p><span class="math display">\[\begin{aligned}
\widehat{price} &amp;= 42.87 + 10.90\times cond\_\hspace{0.3mm}new\end{aligned}\]</span></p>
<p>Results of this model are shown in Table [singleVarModelsForPriceUsingCond] and a scatterplot for price versus game condition is shown in Figure [marioKartSingle].</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 42.8711 &amp; 0.8140 &amp; 52.67 &amp; 0.0000<br />
cond_</p>
<p>new &amp; 10.8996 &amp; 1.2583 &amp; 8.66 &amp; 0.0000<br />
&amp;&amp;&amp;</p>
<p>[singleVarModelsForPriceUsingCond]</p>
<div class="figure">
<img src="06/figures/marioKartSingle/marioKartSingle" alt="Scatterplot of the total auction price against the game’s condition. The least squares line is also shown." />
<p class="caption">Scatterplot of the total auction price against the game’s condition. The least squares line is also shown.</p>
</div>
<p>[marioKartSingle]</p>
<p>Examine Figure [marioKartSingle]. Does the linear model seem ?<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<p><span>Interpret the coefficient for the game’s condition in the model. Is this coefficient significantly different from 0?</span> Note that is a two-level categorical variable that takes value 1 when the game is new and value 0 when the game is used. So 10.90 means that the model predicts an extra $10.90 for those games that are new versus those that are used. (See Section [categoricalPredictorsWithTwoLevels] for a review of the interpretation for two-level categorical predictor variables.) Examining the regression output in Table [singleVarModelsForPriceUsingCond], we can see that the p-value for is very close to zero, indicating there is strong evidence that the coefficient is different from zero when using this simple one-variable model.</p>
</div>
<div id="includingAndAssessingManyVariablesInAModel" class="section level3">
<h3><span class="header-section-number">11.1.2</span> Including and assessing many variables in a model</h3>
<p>Sometimes there are underlying structures or relationships between predictor variables. For instance, new games sold on Ebay tend to come with more Wii wheels, which may have led to higher prices for those auctions. We would like to fit a model that includes all potentially important variables simultaneously. This would help us evaluate the relationship between a predictor variable and the outcome while controlling for the potential influence of other variables. This is the strategy used in <strong>multiple regression</strong>. While we remain cautious about making any causal interpretations using multiple regression, such models are a common first step in providing evidence of a causal connection.</p>
<p>We want to construct a model that accounts for not only the game condition, as in Section [twoSingleVariableModelsForMarioKartData], but simultaneously accounts for three other variables: , <strong>duration</strong>, and <strong>wheels</strong>.</p>
<p><span class="math display">\[\begin{aligned}
\widehat{\textbf{price}}
    &amp;= \beta_0 + \beta_1\times \textbf{cond\_\hspace{0.3mm}new} +
        \beta_2\times \textbf{stock\_\hspace{0.3mm}photo} \notag \\
    &amp;\qquad\  + \beta_3 \times  \textbf{duration} +
        \beta_4 \times  \textbf{wheels} \notag \\
\hat{y}
    &amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 +
        \beta_3 x_3 + \beta_4 x_4
\label{eqForMultipleRegrOfTotalPrForAllPredictors}\end{aligned}\]</span></p>
<p>In this equation, <span class="math inline">\(y\)</span> represents the total price, <span class="math inline">\(x_1\)</span> indicates whether the game is new, <span class="math inline">\(x_2\)</span> indicates whether a stock photo was used, <span class="math inline">\(x_3\)</span> is the duration of the auction, and <span class="math inline">\(x_4\)</span> is the number of Wii wheels included with the game. Just as with the single predictor case, a multiple regression model may be missing important components or it might not precisely represent the relationship between the outcome and the available explanatory variables. While no model is perfect, we wish to explore the possibility that this one may fit the data reasonably well.</p>
<p>We estimate the parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, …, <span class="math inline">\(\beta_4\)</span> in the same way as we did in the case of a single predictor. We select <span class="math inline">\(b_0\)</span>, <span class="math inline">\(b_1\)</span>, …, <span class="math inline">\(b_4\)</span> that minimize the sum of the squared residuals:</p>
<p><span class="math display">\[\begin{aligned}
\label{sumOfSqResInMultRegr}
SSE = e_1^2 + e_2^2 + \dots + e_{141}^2
    = \sum_{i=1}^{141} e_i^2
     = \sum_{i=1}^{141} \left(y_i - \hat{y}_i\right)^2\end{aligned}\]</span></p>
<p>Here there are 141 residuals, one for each observation. We typically use a computer to minimize the sum in Equation  and compute point estimates, as shown in the sample output in Table [outputForMultipleRegrOutputForAllPredictors]. Using this output, we identify the point estimates <span class="math inline">\(b_i\)</span> of each <span class="math inline">\(\beta_i\)</span>, just as we did in the one-predictor case.</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 36.2110 &amp; 1.5140 &amp; 23.92 &amp; 0.0000<br />
cond_</p>
<p>new &amp; 5.1306 &amp; 1.0511 &amp; 4.88 &amp; 0.0000<br />
stock_</p>
<p>photo &amp; 1.0803 &amp; 1.0568 &amp; 1.02 &amp; 0.3085<br />
duration &amp; -0.0268 &amp; 0.1904 &amp; -0.14 &amp; 0.8882<br />
wheels &amp; 7.2852 &amp; 0.5547 &amp; 13.13 &amp; 0.0000<br />
&amp;&amp;&amp;</p>
<p>[outputForMultipleRegrOutputForAllPredictors]</p>
<p>A multiple regression model is a linear model with many predictors. In general, we write the model as</p>
<p><span class="math display">\[\begin{aligned}
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k %+ \epsilon\end{aligned}\]</span></p>
<p>when there are <span class="math inline">\(k\)</span> predictors. We often estimate the <span class="math inline">\(\beta_i\)</span> parameters using a computer.</p>
<p>[eqForMultipleRegrOfTotalPrForAllPredictorsWithCoefficients] Write out the model in Equation  using the point estimates from Table [outputForMultipleRegrOutputForAllPredictors]. How many predictors are there in this model?<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></p>
<p>What does <span class="math inline">\(\beta_4\)</span>, the coefficient of variable <span class="math inline">\(x_4\)</span> (Wii wheels), represent? What is the point estimate of <span class="math inline">\(\beta_4\)</span>?<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<p>[computeMultipleRegressionResidualForMarioKart] Compute the residual of the first observation in Table  using the equation identified in Guided Practice [eqForMultipleRegrOfTotalPrForAllPredictorsWithCoefficients].<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p><span>We estimated a coefficient for in Section [twoSingleVariableModelsForMarioKartData] of <span class="math inline">\(b_1 = 10.90\)</span> with a standard error of <span class="math inline">\(SE_{b_1} = 1.26\)</span> when using simple linear regression. Why might there be a difference between that estimate and the one in the multiple regression setting?</span> [colinearityOfCondNewAndStockPhoto] If we examined the data carefully, we would see that some predictors are correlated. For instance, when we estimated the connection of the outcome <strong>price</strong> and predictor using simple linear regression, we were unable to control for other variables like the number of Wii wheels included in the auction. That model was biased by the confounding variable <strong>wheels</strong>. When we use both variables, this particular underlying and unintentional bias is reduced or eliminated (though bias from other confounding variables may still remain).</p>
<p>Example [colinearityOfCondNewAndStockPhoto] describes a common issue in multiple regression: correlation among predictor variables. We say the two predictor variables are <strong>collinear</strong> (pronounced as <em>co-linear</em>) when they are correlated, and this collinearity complicates model estimation. While it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being collinear.</p>
<p>The estimated value of the intercept is 36.21, and one might be tempted to make some interpretation of this coefficient, such as, it is the model’s predicted price when each of the variables take value zero: the game is used, the primary image is not a stock photo, the auction duration is zero days, and there are no wheels included. Is there any value gained by making this interpretation?<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
</div>
<div id="adjusted-r2-as-a-better-estimate-of-explained-variance" class="section level3">
<h3><span class="header-section-number">11.1.3</span> Adjusted <span class="math inline">\(R^2\)</span> as a better estimate of explained variance</h3>
<p>We first used <span class="math inline">\(R^2\)</span> in Section [fittingALineByLSR] to determine the amount of variability in the response that was explained by the model:</p>
<p><span class="math display">\[\begin{aligned}
R^2 = 1 - \frac{\text{variability in residuals}}{\text{variability in the outcome}}
    = 1 - \frac{Var(e_i)}{Var(y_i)}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(e_i\)</span> represents the residuals of the model and <span class="math inline">\(y_i\)</span> the outcomes. This equation remains valid in the multiple regression framework, but a small enhancement can often be even more informative.</p>
<p>[computeUnadjustedR2ForAllPredictorsInMarioKart] The variance of the residuals for the model given in Guided Practice [computeMultipleRegressionResidualForMarioKart] is 23.34, and the variance of the total price in all the auctions is 83.06. Calculate <span class="math inline">\(R^2\)</span> for this model.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
<p>This strategy for estimating <span class="math inline">\(R^2\)</span> is acceptable when there is just a single variable. However, it becomes less helpful when there are many variables. The regular <span class="math inline">\(R^2\)</span> is actually a biased estimate of the amount of variability explained by the model. To get a better estimate, we use the adjusted <span class="math inline">\(R^2\)</span>.</p>
<p>The is computed as</p>
<p><span class="math display">\[\begin{aligned}
R_{adj}^{2} = 1-\frac{Var(e_i) / (n-k-1)}{Var(y_i) / (n-1)}
    = 1-\frac{Var(e_i)}{Var(y_i)} \times \frac{n-1}{n-k-1}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of cases used to fit the model and <span class="math inline">\(k\)</span> is the number of predictor variables in the model.</p>
<p>Because <span class="math inline">\(k\)</span> is never negative, the adjusted <span class="math inline">\(R^2\)</span> will be smaller – often times just a little smaller – than the unadjusted <span class="math inline">\(R^2\)</span>. The reasoning behind the adjusted <span class="math inline">\(R^2\)</span> lies in the associated with each variance.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></p>
<p>There were <span class="math inline">\(n=141\)</span> auctions in the data set and <span class="math inline">\(k=4\)</span> predictor variables in the model. Use <span class="math inline">\(n\)</span>, <span class="math inline">\(k\)</span>, and the variances from Guided Practice [computeUnadjustedR2ForAllPredictorsInMarioKart] to calculate <span class="math inline">\(R_{adj}^2\)</span> for the Mario Kart model.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></p>
<p>Suppose you added another predictor to the model, but the variance of the errors <span class="math inline">\(Var(e_i)\)</span> didn’t go down. What would happen to the <span class="math inline">\(R^2\)</span>? What would happen to the adjusted <span class="math inline">\(R^2\)</span>?</p>
<p><a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></p>
</div>
</div>
<div id="modelSelection" class="section level2">
<h2><span class="header-section-number">11.2</span> Model selection</h2>
<p>The best model is not always the most complicated. Sometimes including variables that are not evidently important can actually reduce the accuracy of predictions. In this section we discuss model selection strategies, which will help us eliminate from the model variables that are less important.</p>
<p>In this section, and in practice, the model that includes all available explanatory variables is often referred to as the <strong>full model</strong>. Our goal is to assess whether the full model is the best model. If it isn’t, we want to identify a smaller model that is preferable.</p>
<div id="identifying-variables-in-the-model-that-may-not-be-helpful" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Identifying variables in the model that may not be helpful</h3>
<p>Table [outputForMultipleRegrOutputForAllPredictors2] provides a summary of the regression output for the full model for the auction data. The last column of the table lists p-values that can be used to assess hypotheses of the following form:</p>
<ul>
<li><p><span class="math inline">\(\beta_i = 0\)</span> when the other explanatory variables are included in the model.</p></li>
<li><p><span class="math inline">\(\beta_i \neq 0\)</span> when the other explanatory variables are included in the model.</p></li>
</ul>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 36.2110 &amp; 1.5140 &amp; 23.92 &amp; 0.0000<br />
cond_</p>
<p>new &amp; 5.1306 &amp; 1.0511 &amp; 4.88 &amp; 0.0000<br />
stock_</p>
<p>photo &amp; 1.0803 &amp; 1.0568 &amp; 1.02 &amp; 0.3085<br />
duration &amp; -0.0268 &amp; 0.1904 &amp; -0.14 &amp; 0.8882<br />
wheels &amp; 7.2852 &amp; 0.5547 &amp; 13.13 &amp; 0.0000<br />
&amp; &amp; &amp; &amp;<br />
&amp;</p>
<p>[outputForMultipleRegrOutputForAllPredictors2]</p>
<p><span>The coefficient of has a <span class="math inline">\(t\)</span> test statistic of <span class="math inline">\(T=4.88\)</span> and a p-value for its corresponding hypotheses (<span class="math inline">\(H_0: \beta_1 = 0\)</span>, <span class="math inline">\(H_A: \beta_1 \neq 0\)</span>) of about zero. How can this be interpreted?</span> If we keep all the other variables in the model and add no others, then there is strong evidence that a game’s condition (new or used) has a real relationship with the total auction price.</p>
<p><span>Is there strong evidence that using a stock photo is related to the total auction price?</span> The <span class="math inline">\(t\)</span> test statistic for is <span class="math inline">\(T=1.02\)</span> and the p-value is about 0.31. After accounting for the other predictors, there is not strong evidence that using a stock photo in an auction is related to the total price of the auction. We might consider removing the variable from the model.</p>
<p>Identify the p-values for both the <strong>duration</strong> and <strong>wheels</strong> variables in the model. Is there strong evidence supporting the connection of these variables with the total price in the model?<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<p>There is not statistically significant evidence that either the stock photo or duration variables contribute meaningfully to the model. Next we consider common strategies for pruning such variables from a model.</p>
<p><span> The adjusted <span class="math inline">\(R^2\)</span> may be used as an alternative to p-values for model selection, where a higher adjusted <span class="math inline">\(R^2\)</span> represents a better model fit. For instance, we could compare two models using their adjusted <span class="math inline">\(R^2\)</span>, and the model with the higher adjusted <span class="math inline">\(R^2\)</span> would be preferred. This approach tends to include more variables in the final model when compared to the p-value approach.</span></p>
</div>
<div id="two-model-selection-strategies" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Two model selection strategies</h3>
<p>Two common strategies for adding or removing variables in a multiple regression model are called <em>backward-selection</em> and <em>forward-selection</em>. These techniques are often referred to as <strong>stepwise</strong> model selection strategies, because they add or delete one variable at a time as they “step” through the candidate predictors. We will discuss these strategies in the context of the p-value approach. Alternatively, we could have employed an <span class="math inline">\(R_{adj}^2\)</span> approach.</p>
<p>The <strong>backward-elimination</strong> strategy starts with the model that includes all potential predictor variables. Variables are eliminated one-at-a-time from the model until only variables with statistically significant p-values remain. The strategy within each elimination step is to drop the variable with the largest p-value, refit the model, and reassess the inclusion of all variables.</p>
<p><span>Results corresponding to the <em>full model</em> for the data are shown in Table [outputForMultipleRegrOutputForAllPredictors2]. How should we proceed under the backward-elimination strategy?</span> [backwardEliminationExampleWMarioKartData] There are two variables with coefficients that are not statistically different from zero: and <strong>duration</strong>. We first drop the <strong>duration</strong> variable since it has a larger corresponding p-value, <em>then we refit the model</em>. A regression summary for the new model is shown in Table [outputForMultipleRegrOutputForAllPredictorsButDuration].</p>
<p>In the new model, there is not strong evidence that the coefficient for is different from zero, even though the p-value decreased slightly, and the other p-values remain very small. Next, we again eliminate the variable with the largest non-significant p-value, , and refit the model. The updated regression summary is shown in Table [outputForMultipleRegrOutputForAllPredictorsButDurationAndStockPhoto].</p>
<p>In the latest model, we see that the two remaining predictors have statistically significant coefficients with p-values of about zero. Since there are no variables remaining that could be eliminated from the model, we stop. The final model includes only the and <strong>wheels</strong> variables in predicting the total auction price:</p>
<p><span class="math display">\[\begin{aligned}
\hat{y} \ &amp;= \ b_0 + b_1x_1 + b_4x_4 \\
    &amp;= \ 36.78 + 5.58x_1 + 7.23x_4\end{aligned}\]</span></p>
<p>where <span class="math inline">\(x_1\)</span> represents and <span class="math inline">\(x_4\)</span> represents <strong>wheels</strong>.</p>
<p>An alternative to using p-values in model selection is to use the adjusted <span class="math inline">\(R^2\)</span>. At each elimination step, we refit the model without each of the variables up for potential elimination. For example, in the first step, we would fit four models, where each would be missing a different predictor. If one of these smaller models has a higher adjusted <span class="math inline">\(R^2\)</span> than our current model, we pick the smaller model with the largest adjusted <span class="math inline">\(R^2\)</span>. We continue in this way until removing variables does not increase <span class="math inline">\(R_{adj}^2\)</span>. Had we used the adjusted <span class="math inline">\(R^2\)</span> criteria, we would have kept the variable along with the and <strong>wheels</strong> variables.</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 36.0483 &amp; 0.9745 &amp; 36.99 &amp; 0.0000<br />
cond_</p>
<p>new &amp; 5.1763 &amp; 0.9961 &amp; 5.20 &amp; 0.0000<br />
stock_</p>
<p>photo &amp; 1.1177 &amp; 1.0192 &amp; 1.10 &amp; 0.2747<br />
wheels &amp; 7.2984 &amp; 0.5448 &amp; 13.40 &amp; 0.0000<br />
&amp; &amp; &amp; &amp;<br />
&amp;&amp;<span class="math inline">\(df=137\)</span></p>
<p>[outputForMultipleRegrOutputForAllPredictorsButDuration]</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; t value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>t<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; 36.7849 &amp; 0.7066 &amp; 52.06 &amp; 0.0000<br />
cond_</p>
<p>new &amp; 5.5848 &amp; 0.9245 &amp; 6.04 &amp; 0.0000<br />
wheels &amp; 7.2328 &amp; 0.5419 &amp; 13.35 &amp; 0.0000<br />
&amp; &amp; &amp; &amp;<br />
&amp;&amp;<span class="math inline">\(df=138\)</span></p>
<p>[outputForMultipleRegrOutputForAllPredictorsButDurationAndStockPhoto]</p>
<p>Notice that the p-value for changed a little from the full model (0.309) to the model that did not include the <strong>duration</strong> variable (0.275). It is common for p-values of one variable to change, due to collinearity, after eliminating a different variable. This fluctuation emphasizes the importance of refitting a model after each variable elimination step. The p-values tend to change dramatically when the eliminated variable is highly correlated with another variable in the model.</p>
<p>The <strong>forward-selection</strong> strategy is the reverse of the backward-elimination technique. Instead of eliminating variables one-at-a-time, we add variables one-at-a-time until we cannot find any variables that present strong evidence of their importance in the model.</p>
<p><span>Construct a model for the data set using the forward-selection strategy.</span>[forwardEliminationExampleWMarioKartData] We start with the model that includes no variables. Then we fit each of the possible models with just one variable. That is, we fit the model including just the predictor, then the model including just the variable, then a model with just <strong>duration</strong>, and a model with just <strong>wheels</strong>. Each of the four models (yes, we fit four models!) provides a p-value for the coefficient of the predictor variable. Out of these four variables, the <strong>wheels</strong> variable had the smallest p-value. Since its p-value is less than 0.05 (the p-value was smaller than 2e-16), we add the Wii wheels variable to the model. Once a variable is added in forward-selection, it will be included in all models considered as well as the final model.</p>
<p>Since we successfully found a first variable to add, we consider adding another. We fit three new models: (1) the model including just the and <strong>wheels</strong> variables (output in Table [outputForMultipleRegrOutputForAllPredictorsButDurationAndStockPhoto]), (2) the model including just the and <strong>wheels</strong> variables, and (3) the model including only the <strong>duration</strong> and <strong>wheels</strong> variables. Of these models, the first had the lowest p-value for its new variable (the p-value corresponding to was 1.4e-08). Because this p-value is below 0.05, we add the variable to the model. Now the final model is guaranteed to include both the condition and wheels variables.</p>
<p>We must then repeat the process a third time, fitting two new models: (1) the model including the , , and <strong>wheels</strong> variables (output in Table [outputForMultipleRegrOutputForAllPredictorsButDuration]) and (2) the model including the <strong>duration</strong>, , and <strong>wheels</strong> variables. The p-value corresponding to in the first model (0.275) was smaller than the p-value corresponding to <strong>duration</strong> in the second model (0.682). However, since this smaller p-value was not below 0.05, there was not strong evidence that it should be included in the model. Therefore, neither variable is added and we are finished.</p>
<p>The final model is the same as that arrived at using the backward-selection strategy.</p>
<p><span>As before, we could have used the <span class="math inline">\(R_{adj}^2\)</span> criteria instead of examining p-values in selecting variables for the model. Rather than look for variables with the smallest p-value, we look for the model with the largest <span class="math inline">\(R_{adj}^2\)</span>. What would the result of forward-selection be using the adjusted <span class="math inline">\(R^2\)</span> approach?</span> Using the forward-selection strategy, we start with the model with no predictors. Next we look at each model with a single predictor. If one of these models has a larger <span class="math inline">\(R_{adj}^2\)</span> than the model with no variables, we use this new model. We repeat this procedure, adding one variable at a time, until we cannot find a model with a larger <span class="math inline">\(R_{adj}^2\)</span>. If we had done the forward-selection strategy using <span class="math inline">\(R_{adj}^2\)</span>, we would have arrived at the model including , , and <strong>wheels</strong>, which is a slightly larger model than we arrived at using the p-value approach and the same model we arrived at using the adjusted <span class="math inline">\(R^2\)</span> and backwards-elimination.</p>
<p><span> The backward-elimination strategy begins with the largest model and eliminates variables one-by-one until we are satisfied that all remaining variables are important to the model. The forward-selection strategy starts with no variables included in the model, then it adds in variables according to their importance until no other important variables are found.</span></p>
<p>There is no guarantee that the backward-elimination and forward-selection strategies will arrive at the same final model using the p-value or adjusted <span class="math inline">\(R^2\)</span> methods. If the backwards-elimination and forward-selection strategies are both tried and they arrive at different models, choose the model with the larger <span class="math inline">\(R_{adj}^2\)</span> as a tie-breaker; other tie-break options exist but are beyond the scope of this book.</p>
<p>It is generally acceptable to use just one strategy, usually backward-elimination with either the p-value or adjusted <span class="math inline">\(R^2\)</span> criteria. However, before reporting the model results, we must verify the model conditions are reasonable.</p>
</div>
</div>
<div id="multipleRegressionModelAssumptions" class="section level2">
<h2><span class="header-section-number">11.3</span> Checking model assumptions using graphs</h2>
<p>Multiple regression methods using the model</p>
<p><span class="math display">\[\begin{aligned}
\hat{y} &amp;= \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k\end{aligned}\]</span></p>
<p>generally depend on the following four assumptions:</p>
<ol style="list-style-type: decimal">
<li><p>the residuals of the model are nearly normal,</p></li>
<li><p>the variability of the residuals is nearly constant,</p></li>
<li><p>the residuals are independent, and</p></li>
<li><p>each variable is linearly related to the outcome.</p></li>
</ol>
<p>Simple and effective plots can be used to check each of these assumptions. We will consider the model for the auction data that uses the game condition and number of wheels as predictors. The plotting methods presented here may also be used to check the conditions for the models introduced in Chapter [linRegrForTwoVar].</p>
<dl>
<dt>Normal probability plot.</dt>
<dd><p>A normal probability plot of the residuals is shown in Figure [mkDiagnosticNormalQuantilePlot]. While the plot exhibits some minor irregularities, there are no outliers that might be cause for concern. In a normal probability plot for residuals, we tend to be most worried about residuals that appear to be outliers, since these indicate long tails in the distribution of residuals.</p>
<div class="figure">
<img src="06/figures/marioKartDiagnostics/mkDiagnosticNormalQuantilePlot" alt="A normal probability plot of the residuals is helpful in identifying observations that might be outliers." />
<p class="caption">A normal probability plot of the residuals is helpful in identifying observations that might be outliers.</p>
</div>
<p>[mkDiagnosticNormalQuantilePlot]</p>
</dd>
<dt>Absolute values of residuals against fitted values.</dt>
<dd><p>A plot of the absolute value of the residuals against their corresponding fitted values (<span class="math inline">\(\hat{y}_i\)</span>) is shown in Figure [mkDiagnosticEvsAbsF]. This plot is helpful to check the condition that the variance of the residuals is approximately constant. We don’t see any obvious deviations from constant variance in this example.</p>
<div class="figure">
<img src="06/figures/marioKartDiagnostics/mkDiagnosticEvsAbsF" alt="Comparing the absolute value of the residuals against the fitted values (\hat{y}_i) is helpful in identifying deviations from the constant variance assumption." />
<p class="caption">Comparing the absolute value of the residuals against the fitted values (<span class="math inline">\(\hat{y}_i\)</span>) is helpful in identifying deviations from the constant variance assumption.</p>
</div>
<p>[mkDiagnosticEvsAbsF]</p>
</dd>
<dt>Residuals in order of their data collection.</dt>
<dd><p>A plot of the residuals in the order their corresponding auctions were observed is shown in Figure [mkDiagnosticInOrder]. Such a plot is helpful in identifying any connection between cases that are close to one another, e.g. we could look for declining prices over time or if there was a time of the day when auctions tended to fetch a higher price. Here we see no structure that indicates a problem.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p>
<div class="figure">
<img src="06/figures/marioKartDiagnostics/mkDiagnosticInOrder" alt="Plotting residuals in the order that their corresponding observations were collected helps identify connections between successive observations. If it seems that consecutive observations tend to be close to each other, this indicates the independence assumption of the observations would fail." />
<p class="caption">Plotting residuals in the order that their corresponding observations were collected helps identify connections between successive observations. If it seems that consecutive observations tend to be close to each other, this indicates the independence assumption of the observations would fail.</p>
</div>
<p>[mkDiagnosticInOrder]</p>
</dd>
<dt>Residuals against each predictor variable.</dt>
<dd><p>We consider a plot of the residuals against the variable and the residuals against the <strong>wheels</strong> variable. These plots are shown in Figure [mkDiagnosticEvsVariables]. For the two-level condition variable, we are guaranteed not to see any remaining trend, and instead we are checking that the variability doesn’t fluctuate across groups. In this example, when we consider the residuals against the <strong>wheels</strong> variable, we see some possible structure. There appears to be curvature in the residuals, indicating the relationship is probably not linear.</p>
<div class="figure">
<img src="06/figures/marioKartDiagnostics/mkDiagnosticEvsVariables" alt="In the two-level variable for the game’s condition, we check for differences in distribution shape or variability. For numerical predictors, we also check for trends or other structure. We see some slight bowing in the residuals against the wheels variable." />
<p class="caption">In the two-level variable for the game’s condition, we check for differences in distribution shape or variability. For numerical predictors, we also check for trends or other structure. We see some slight bowing in the residuals against the <strong>wheels</strong> variable.</p>
</div>
<p>[mkDiagnosticEvsVariables]</p>
</dd>
</dl>
<p>It is necessary to summarize diagnostics for any model fit. If the diagnostics support the model assumptions, this would improve credibility in the findings. If the diagnostic assessment shows remaining underlying structure in the residuals, we should try to adjust the model to account for that structure. If we are unable to do so, we may still report the model but must also note its shortcomings. In the case of the auction data, we report that there may be a nonlinear relationship between the total price and the number of wheels included for an auction. This information would be important to buyers and sellers; omitting this information could be a setback to the very people who the model might assist.</p>
<p><span> The truth is that no model is perfect. However, even imperfect models can be useful. Reporting a flawed model can be reasonable so long as we are clear and report the model’s shortcomings.</span></p>
<p><span>Don’t report results when assumptions are grossly violated</span> <span>While there is a little leeway in model assumptions, don’t go too far. If model assumptions are very clearly violated, consider a new model, even if it means learning more statistical methods or hiring someone who can help.</span></p>
<p>Confidence intervals for coefficients in multiple regression can be computed using the same formula as in the single predictor model:</p>
<p><span class="math display">\[\begin{aligned}
b_i \ \pm\ t_{df}^{\star}SE_{b_{i}}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(t_{df}^{\star}\)</span> is the appropriate <span class="math inline">\(t\)</span> value corresponding to the confidence level and model degrees of freedom, <span class="math inline">\(df=n-k-1\)</span>.</p>
</div>
<div id="logisticRegression" class="section level2">
<h2><span class="header-section-number">11.4</span> Logistic regression</h2>
<p>In this section we introduce <strong>logistic regression</strong> as a tool for building models when there is a categorical response variable with two levels. Logistic regression is a type of <strong>generalized linear model</strong> (GLM) for response variables where regular multiple regression does not work very well. In particular, the response variable in these settings often takes a form where residuals look completely different from the normal distribution.</p>
<p>GLMs can be thought of as a two-stage modeling approach. We first model the response variable using a probability distribution, such as the binomial or Poisson distribution. Second, we model the parameter of the distribution using a collection of predictors and a special form of multiple regression.</p>
<p>In Section [logisticRegression] we will revisit the <strong>email</strong> data set from Chapter [introductionToData]. These emails were collected from a single email account, and we will work on developing a basic spam filter using these data. The response variable, <strong>spam</strong>, has been encoded to take value 0 when a message is not spam and 1 when it is spam. Our task will be to build an appropriate model that classifies messages as spam or not spam using email characteristics coded as predictor variables. While this model will not be the same as those used in large-scale spam filters, it shares many of the same features.</p>
<div id="email-data" class="section level3">
<h3><span class="header-section-number">11.4.1</span> Email data</h3>
<p>The <strong>email</strong> data set was first presented in Chapter [introductionToData] with a relatively small number of variables. In fact, there are many more variables available that might be useful for classifying spam. Descriptions of these variables are presented in Table [emailVariables]. The <strong>spam</strong> variable will be the outcome, and the other 10 variables will be the model predictors. While we have limited the predictors used in this section to be categorical variables (where many are represented as indicator variables), numerical predictors may also be used in logistic regression. See the footnote for an additional discussion on this topic.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a></p>
<table>
<caption>Descriptions for 11 variables in the <strong>email</strong> data set. Notice that all of the variables are indicator variables, which take the value 1 if the specified characteristic is present and 0 otherwise.</caption>
<thead>
<tr class="header">
<th align="left"><span><strong>variable</strong></span></th>
<th align="left"><span><strong>description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>spam</strong></td>
<td align="left">Specifies whether the message was spam.</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">An indicator variable for if more than one person was listed in the <em>To</em> field of the email.</td>
</tr>
<tr class="odd">
<td align="left"><strong>cc</strong></td>
<td align="left">An indicator for if someone was CCed on the email.</td>
</tr>
<tr class="even">
<td align="left"><strong>attach</strong></td>
<td align="left">An indicator for if there was an attachment, such as a document or image.</td>
</tr>
<tr class="odd">
<td align="left"><strong>dollar</strong></td>
<td align="left">An indicator for if the word “dollar” or dollar symbol ($) appeared in the email.</td>
</tr>
<tr class="even">
<td align="left"><strong>winner</strong></td>
<td align="left">An indicator for if the word “winner” appeared in the email message.</td>
</tr>
<tr class="odd">
<td align="left"><strong>inherit</strong></td>
<td align="left">An indicator for if the word “inherit” (or a variation, like “inheritance”) appeared in the email.</td>
</tr>
<tr class="even">
<td align="left"><strong>password</strong></td>
<td align="left">An indicator for if the word “password” was present in the email.</td>
</tr>
<tr class="odd">
<td align="left"><strong>format</strong></td>
<td align="left">Indicates if the email contained special formatting, such as bolding, tables, or links</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left">Indicates whether “Re:” was included at the start of the email subject.</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left">Indicates whether any exclamation point was included in the email subject.</td>
</tr>
</tbody>
</table>
<p>[emailVariables]</p>
</div>
<div id="modelingTheProbabilityOfAnEvent" class="section level3">
<h3><span class="header-section-number">11.4.2</span> Modeling the probability of an event</h3>
<p>The outcome variable for a GLM is denoted by <span class="math inline">\(Y_i\)</span>, where the index <span class="math inline">\(i\)</span> is used to represent observation <span class="math inline">\(i\)</span>. In the email application, <span class="math inline">\(Y_i\)</span> will be used to represent whether email <span class="math inline">\(i\)</span> is spam (<span class="math inline">\(Y_i=1\)</span>) or not (<span class="math inline">\(Y_i=0\)</span>).</p>
<p>The predictor variables are represented as follows: <span class="math inline">\(x_{1,i}\)</span> is the value of variable 1 for observation <span class="math inline">\(i\)</span>, <span class="math inline">\(x_{2,i}\)</span> is the value of variable 2 for observation <span class="math inline">\(i\)</span>, and so on.</p>
<p>Logistic regression is a generalized linear model where the outcome is a two-level categorical variable. The outcome, <span class="math inline">\(Y_i\)</span>, takes the value 1 (in our application, this represents a spam message) with probability <span class="math inline">\(p_i\)</span> and the value 0 with probability <span class="math inline">\(1-p_i\)</span>. It is the probability <span class="math inline">\(p_i\)</span> that we model in relation to the predictor variables.</p>
<p>The logistic regression model relates the probability an email is spam (<span class="math inline">\(p_i\)</span>) to the predictors <span class="math inline">\(x_{1,i}\)</span>, <span class="math inline">\(x_{2,i}\)</span>, …, <span class="math inline">\(x_{k,i}\)</span> through a framework much like that of multiple regression:</p>
<p><span class="math display">\[\begin{aligned}
transformation(p_{i}) = \beta_0 + \beta_1x_{1,i} + \beta_2 x_{2,i} + \cdots \beta_k x_{k,i}
\label{linkTransformationEquation}\end{aligned}\]</span></p>
<p>We want to choose a transformation in Equation  that makes practical and mathematical sense. For example, we want a transformation that makes the range of possibilities on the left hand side of Equation  equal to the range of possibilities for the right hand side; if there was no transformation for this equation, the left hand side could only take values between 0 and 1, but the right hand side could take values outside of this range. A common transformation for <span class="math inline">\(p_i\)</span> is the <strong>logit transformation</strong>, which may be written as</p>
<p><span class="math display">\[\begin{aligned}
logit(p_i) = \log_{e}\left( \frac{p_i}{1-p_i} \right)\end{aligned}\]</span></p>
<p>The logit transformation is shown in Figure [logitTransformationFigureHoriz]. Below, we rewrite Equation  using the logit transformation of <span class="math inline">\(p_i\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\log_{e}\left( \frac{p_i}{1-p_i} \right)
    = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \cdots + \beta_k x_{k,i}\end{aligned}\]</span></p>
<p>In our spam example, there are 10 predictor variables, so <span class="math inline">\(k = 10\)</span>. This model isn’t very intuitive, but it still has some resemblance to multiple regression, and we can fit this model using software. In fact, once we look at results from software, it will start to feel like we’re back in multiple regression, even if the interpretation of the coefficients is more complex.</p>
<div class="figure">
<img src="06/figures/logitTransformationFigureHoriz/logitTransformationFigureHoriz" alt="Values of p_i against values of logit(p_i)." />
<p class="caption">Values of <span class="math inline">\(p_i\)</span> against values of <span class="math inline">\(logit(p_i)\)</span>.</p>
</div>
<p>[logitTransformationFigureHoriz]</p>
<p>Here we create a spam filter with a single predictor: . This variable indicates whether more than one email address was listed in the <em>To</em> field of the email. The following logistic regression model was fit using statistical software:</p>
<p><span class="math display">\[\begin{aligned}
\log\left( \frac{p_i}{1-p_i} \right) = -2.12 - 1.81\times\text{\textbf{to\_\hspace{0.3mm}multiple}}\end{aligned}\]</span></p>
<p>If an email is randomly selected and it has just one address in the <em>To</em> field, what is the probability it is spam? What if more than one address is listed in the <em>To</em> field?</p>
<p>[logisticExampleWithToMultiple] If there is only one email in the <em>To</em> field, then takes value 0 and the right side of the model equation equals -2.12. Solving for <span class="math inline">\(p_i\)</span>: <span class="math inline">\(\frac{e^{-2.12}}{1 + e^{-2.12}} = 0.11\)</span>. Just as we labeled a fitted value of <span class="math inline">\(y_i\)</span> with a “hat” in single-variable and multiple regression, we will do the same for this probability: <span class="math inline">\(\hat{p}_i = 0.11\)</span>.</p>
<p>If there is more than one address listed in the <em>To</em> field, then the right side of the model equation is <span class="math inline">\(-2.12 - 1.81\times1 = -3.93\)</span>, which corresponds to a probability <span class="math inline">\(\hat{p}_i = 0.02\)</span>.</p>
<p>Notice that we could examine -2.12 and -3.93 in Figure [logitTransformationFigureHoriz] to estimate the probability before formally calculating the value.</p>
<p>To convert from values on the regression-scale (e.g. -2.12 and -3.93 in Example [logisticExampleWithToMultiple]), use the following formula, which is the result of solving for <span class="math inline">\(p_i\)</span> in the regression model:</p>
<p><span class="math display">\[\begin{aligned}
p_i
    = \frac{e^{\beta_0 + \beta_1 x_{1,i}+\cdots+\beta_k x_{k,i}}}
        {\ 1\ \ +\ \ e^{\beta_0 + \beta_1 x_{1,i}+\cdots+\beta_k x_{k,i}}\ }\end{aligned}\]</span></p>
<p>As with most applied data problems, we substitute the point estimates for the parameters (the <span class="math inline">\(\beta_i\)</span>) so that we may make use of this formula. In Example [logisticExampleWithToMultiple], the probabilities were calculated as</p>
<p><span class="math display">\[\begin{aligned}
&amp;\frac{\ e^{-2.12}\ }{\ 1\ +\ e^{-2.12}\ } = 0.11 &amp;&amp; \frac{\ e^{-2.12 - 1.81}\ }{\ 1\ +\ e^{-2.12 - 1.81}\ } = 0.02\end{aligned}\]</span></p>
<p>While the information about whether the email is addressed to multiple people is a helpful start in classifying email as spam or not, the probabilities of 11% and 2% are not dramatically different, and neither provides very strong evidence about which particular email messages are spam. To get more precise estimates, we’ll need to include many more variables in the model.</p>
<p>We used statistical software to fit the logistic regression model with all ten predictors described in Table [emailVariables]. Like multiple regression, the result may be presented in a summary table, which is shown in Table [emailLogisticModelResults]. The structure of this table is almost identical to that of multiple regression; the only notable difference is that the p-values are calculated using the normal distribution rather than the <span class="math inline">\(t\)</span> distribution.</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; z value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>z<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; -0.8362 &amp; 0.0962 &amp; -8.69 &amp; 0.0000<br />
to_</p>
<p>multiple &amp; -2.8836 &amp; 0.3121 &amp; -9.24 &amp; 0.0000<br />
winner &amp; 1.7038 &amp; 0.3254 &amp; 5.24 &amp; 0.0000<br />
format &amp; -1.5902 &amp; 0.1239 &amp; -12.84 &amp; 0.0000<br />
re_</p>
<p>subj &amp; -2.9082 &amp; 0.3708 &amp; -7.84 &amp; 0.0000<br />
exclaim_</p>
<p>subj &amp; 0.1355 &amp; 0.2268 &amp; 0.60 &amp; 0.5503<br />
cc &amp; -0.4863 &amp; 0.3054 &amp; -1.59 &amp; 0.1113<br />
attach &amp; 0.9790 &amp; 0.2170 &amp; 4.51 &amp; 0.0000<br />
dollar &amp; -0.0582 &amp; 0.1589 &amp; -0.37 &amp; 0.7144<br />
inherit &amp; 0.2093 &amp; 0.3197 &amp; 0.65 &amp; 0.5127<br />
password &amp; -1.4929 &amp; 0.5295 &amp; -2.82 &amp; 0.0048<br />
[emailLogisticModelResults]</p>
<p>Just like multiple regression, we could trim some variables from the model using the p-value. Using backwards elimination with a p-value cutoff of 0.05 (start with the full model and trim the predictors with p-values greater than 0.05), we ultimately eliminate the , <strong>dollar</strong>, <strong>inherit</strong>, and <strong>cc</strong> predictors. The remainder of this section will rely on this smaller model, which is summarized in Table [emailLogisticReducedModel].</p>
<p><span>rrrrr</span></p>
<p>&amp; &amp; &amp; &amp;<br />
&amp; Estimate &amp; Std. Error &amp; z value &amp; Pr(<span class="math inline">\(&gt;\)</span><span class="math inline">\(|\)</span>z<span class="math inline">\(|\)</span>)<br />
&amp; &amp; &amp; &amp;<br />
(Intercept) &amp; -0.8595 &amp; 0.0910 &amp; -9.44 &amp; 0.0000<br />
to_</p>
<p>multiple &amp; -2.8372 &amp; 0.3092 &amp; -9.18 &amp; 0.0000<br />
winner &amp; 1.7370 &amp; 0.3218 &amp; 5.40 &amp; 0.0000<br />
format &amp; -1.5569 &amp; 0.1207 &amp; -12.90 &amp; 0.0000<br />
re_</p>
<p>subj &amp; -3.0482 &amp; 0.3630 &amp; -8.40 &amp; 0.0000<br />
attach &amp; 0.8643 &amp; 0.2042 &amp; 4.23 &amp; 0.0000<br />
password &amp; -1.4871 &amp; 0.5290 &amp; -2.81 &amp; 0.0049<br />
[emailLogisticReducedModel]</p>
<p>Examine the summary of the reduced model in Table [emailLogisticReducedModel], and in particular, examine the row. Is the point estimate the same as we found before, -1.81, or is it different? Explain why this might be.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p>
<p>Point estimates will generally change a little – and sometimes a lot – depending on which other variables are included in the model. This is usually due to colinearity in the predictor variables. We previously saw this in the Ebay auction example when we compared the coefficient of in a single-variable model and the corresponding coefficient in the multiple regression model that used three additional variables (see Sections [twoSingleVariableModelsForMarioKartData] and [includingAndAssessingManyVariablesInAModel]).</p>
<p><span>Spam filters are built to be automated, meaning a piece of software is written to collect information about emails as they arrive, and this information is put in the form of variables. These variables are then put into an algorithm that uses a statistical model, like the one we’ve fit, to classify the email. Suppose we write software for a spam filter using the reduced model shown in Table [emailLogisticReducedModel]. If an incoming email has the word “winner” in it, will this raise or lower the model’s calculated probability that the incoming email is spam?</span> [exampleForSpamAndWinner] The estimated coefficient of <strong>winner</strong> is positive (1.7370). A positive coefficient estimate in logistic regression, just like in multiple regression, corresponds to a positive association between the predictor and response variables when accounting for the other variables in the model. Since the response variable takes value 1 if an email is spam and 0 otherwise, the positive coefficient indicates that the presence of “winner” in an email raises the model probability that the message is spam.</p>
<p><span>Suppose the same email from Example [exampleForSpamAndWinner] was in HTML format, meaning the <strong>format</strong> variable took value 1. Does this characteristic increase or decrease the probability that the email is spam according to the model?</span>[exampleForSpamAndFormat] Since HTML corresponds to a value of 1 in the <strong>format</strong> variable and the coefficient of this variable is negative (-1.5569), this would lower the probability estimate returned from the model.</p>
</div>
<div id="practical-decisions-in-the-email-application" class="section level3">
<h3><span class="header-section-number">11.4.3</span> Practical decisions in the email application</h3>
<p>Examples [exampleForSpamAndWinner] and [exampleForSpamAndFormat] highlight a key feature of logistic and multiple regression. In the spam filter example, some email characteristics will push an email’s classification in the direction of spam while other characteristics will push it in the opposite direction.</p>
<p>If we were to implement a spam filter using the model we have fit, then each future email we analyze would fall into one of three categories based on the email’s characteristics:</p>
<ol style="list-style-type: decimal">
<li><p>The email characteristics generally indicate the email is not spam, and so the resulting probability that the email is spam is quite low, say, under 0.05.</p></li>
<li><p>The characteristics generally indicate the email is spam, and so the resulting probability that the email is spam is quite large, say, over 0.95.</p></li>
<li><p>The characteristics roughly balance each other out in terms of evidence for and against the message being classified as spam. Its probability falls in the remaining range, meaning the email cannot be adequately classified as spam or not spam.</p></li>
</ol>
<p>If we were managing an email service, we would have to think about what should be done in each of these three instances. In an email application, there are usually just two possibilities: filter the email out from the regular inbox and put it in a “spambox”, or let the email go to the regular inbox.</p>
<p>The first and second scenarios are intuitive. If the evidence strongly suggests a message is not spam, send it to the inbox. If the evidence strongly suggests the message is spam, send it to the spambox. How should we handle emails in the third category?<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p>
<p>Suppose we apply the logistic model we have built as a spam filter and that 100 messages are placed in the spambox over 3 months. If we used the guidelines above for putting messages into the spambox, about how many legitimate (non-spam) messages would you expect to find among the 100 messages?<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
<p>Almost any classifier will have some error. In the spam filter guidelines above, we have decided that it is okay to allow up to 5% of the messages in the spambox to be real messages. If we wanted to make it a little harder to classify messages as spam, we could use a cutoff of 0.99. This would have two effects. Because it raises the standard for what can be classified as spam, it reduces the number of good emails that are classified as spam. However, it will also fail to correctly classify an increased fraction of spam messages. No matter the complexity and the confidence we might have in our model, these practical considerations are absolutely crucial to making a helpful spam filter. Without them, we could actually do more harm than good by using our statistical model.</p>
</div>
<div id="diagnostics-for-the-email-classifier" class="section level3">
<h3><span class="header-section-number">11.4.4</span> Diagnostics for the email classifier</h3>
<p>There are two key conditions for fitting a logistic regression model:</p>
<ol style="list-style-type: decimal">
<li><p>Each predictor <span class="math inline">\(x_i\)</span> is linearly related to logit<span class="math inline">\((p_i)\)</span> if all other predictors are held constant.</p></li>
<li><p>Each outcome <span class="math inline">\(Y_i\)</span> is independent of the other outcomes.</p></li>
</ol>
<p>The first condition of the logistic regression model is not easily checked without a fairly sizable amount of data. Luckily, we have 3,921 emails in our data set! Let’s first visualize these data by plotting the true classification of the emails against the model’s fitted probabilities, as shown in Figure [logisticModelPredict]. The vast majority of emails (spam or not) still have fitted probabilities below 0.5.</p>
<div class="figure">
<img src="06/figures/logisticModel/logisticModelPredict" alt="The predicted probability that each of the 3,912 emails is spam is classified by their grouping, spam or not. Noise (small, random vertical shifts) have been added to each point so that points with nearly identical values aren’t plotted exactly on top of one another. This makes it possible to see more observations." />
<p class="caption">The predicted probability that each of the 3,912 emails is spam is classified by their grouping, spam or not. Noise (small, random vertical shifts) have been added to each point so that points with nearly identical values aren’t plotted exactly on top of one another. This makes it possible to see more observations.</p>
</div>
<p>[logisticModelPredict]</p>
<p>This may at first seem very discouraging: we have fit a logistic model to create a spam filter, but no emails have a fitted probability of being spam above 0.75. Don’t despair; we will discuss ways to improve the model through the use of better variables in Section [improvingTheSetOfVariablesForASpamFilter].</p>
<p>We’d like to assess the quality of our model. For example, we might ask: if we look at emails that we modeled as having a 10% chance of being spam, do we find about 10% of them actually are spam? To help us out, we’ll borrow an advanced statistical method called <strong>natural splines</strong> that estimates the local probability over the region 0.00 to 0.75 (the largest predicted probability was 0.73, so we avoid extrapolating). All you need to know about natural splines to understand what we are doing is that they are used to fit flexible lines rather than straight lines.</p>
<p>The curve fit using natural splines is shown in Figure [logisticModelSpline] as a solid black line. If the logistic model fits well, the curve should closely follow the dashed <span class="math inline">\(y=x\)</span> line. We have added shading to represent the confidence bound for the curved line to clarify what fluctuations might plausibly be due to chance. Even with this confidence bound, there are weaknesses in the first model assumption. The solid curve and its confidence bound dips below the dashed line from about 0.1 to 0.3, and then it drifts above the dashed line from about 0.35 to 0.55. These deviations indicate the model relating the parameter to the predictors does not closely resemble the true relationship.</p>
<div class="figure">
<img src="06/figures/logisticModel/logisticModelSpline" alt="The solid black line provides the empirical estimate of the probability for observations based on their predicted probabilities (confidence bounds are also shown for this line), which is fit using natural splines. A small amount of noise was added to the observations in the plot to allow more observations to be seen." />
<p class="caption">The solid black line provides the empirical estimate of the probability for observations based on their predicted probabilities (confidence bounds are also shown for this line), which is fit using natural splines. A small amount of noise was added to the observations in the plot to allow more observations to be seen.</p>
</div>
<p>[logisticModelSpline]</p>
<p>We could evaluate the second logistic regression model assumption – independence of the outcomes – using the model residuals. The residuals for a logistic regression model are calculated the same way as with multiple regression: the observed outcome minus the expected outcome. For logistic regression, the expected value of the outcome is the fitted probability for the observation, and the residual may be written as</p>
<p><span class="math display">\[\begin{aligned}
e_i = Y_i - \hat{p}_i\end{aligned}\]</span></p>
<p>We could plot these residuals against a variety of variables or in their order of collection, as we did with the residuals in multiple regression. However, since the model will need to be revised to effectively classify spam and you have already seen similar residual plots in Section [multipleRegressionModelAssumptions], we won’t investigate the residuals here.</p>
</div>
<div id="improvingTheSetOfVariablesForASpamFilter" class="section level3">
<h3><span class="header-section-number">11.4.5</span> Improving the set of variables for a spam filter</h3>
<p>If we were building a spam filter for an email service that managed many accounts (e.g. Gmail or Hotmail), we would spend much more time thinking about additional variables that could be useful in classifying emails as spam or not. We also would use transformations or other techniques that would help us include strongly skewed numerical variables as predictors.</p>
<p>Take a few minutes to think about additional variables that might be useful in identifying spam. Below is a list of variables we think might be useful:</p>
<ol style="list-style-type: decimal">
<li><p>An indicator variable could be used to represent whether there was prior two-way correspondence with a message’s sender. For instance, if you sent a message to <a href="mailto:john@example.com">john@example.com</a> and then John sent you an email, this variable would take value 1 for the email that John sent. If you had never sent John an email, then the variable would be set to 0.</p></li>
<li><p>A second indicator variable could utilize an account’s past spam flagging information. The variable could take value 1 if the sender of the message has previously sent messages flagged as spam.</p></li>
<li><p>A third indicator variable could flag emails that contain links included in previous spam messages. If such a link is found, then set the variable to 1 for the email. Otherwise, set it to 0.</p></li>
</ol>
<p>The variables described above take one of two approaches. Variable (1) is specially designed to capitalize on the fact that spam is rarely sent between individuals that have two-way communication. Variables (2) and (3) are specially designed to flag common spammers or spam messages. While we would have to verify using the data that each of the variables is effective, these seem like promising ideas.</p>
<p>Table [emailTableOfSpamAnd] shows a contingency table for spam and also for the new variable described in (1) above. If we look at the 1,090 emails where there was correspondence with the sender in the preceding 30 days, not one of these message was spam. This suggests variable (1) would be very effective at accurately classifying some messages as not spam. With this single variable, we would be able to send about 28% of messages through to the inbox with confidence that almost none are spam.</p>
<p><span>lrrrr</span> &amp; &amp;<br />
&amp;  </p>
<p>&amp;  </p>
<p>&amp;&amp;  </p>
<p>Total<br />
&amp; &amp; &amp; &amp;<br />
&amp; 367 &amp; 0 &amp;&amp; 367<br />
  &amp; 2464 &amp; 1090 &amp;&amp; 3554<br />
&amp; &amp; &amp; &amp;<br />
Total &amp; 2831 &amp; 1090 &amp;&amp; 3921<br />
[emailTableOfSpamAnd]</p>
<p>The variables described in (2) and (3) would provide an excellent foundation for distinguishing messages coming from known spammers or messages that take a known form of spam. To utilize these variables, we would need to build databases: one holding email addresses of known spammers, and one holding URLs found in known spam messages. Our access to such information is limited, so we cannot implement these two variables in this textbook. However, if we were hired by an email service to build a spam filter, these would be important next steps.</p>
<p>In addition to finding more and better predictors, we would need to create a customized logistic regression model for each email account. This may sound like an intimidating task, but its complexity is not as daunting as it may at first seem. We’ll save the details for a statistics course where computer programming plays a more central role.</p>
<p>For what is the extremely challenging task of classifying spam messages, we have made a lot of progress. We have seen that simple email variables, such as the format, inclusion of certain words, and other circumstantial characteristics, provide helpful information for spam classification. Many challenges remain, from better understanding logistic regression to carrying out the necessary computer programming, but completing such a task is very nearly within your reach.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>Diez DM, Barr CD, and Çetinkaya-Rundel M. 2012. <em>openintro</em>: OpenIntro data sets and supplemental functions. <a href="http://cran.r-project.org/web/packages/openintro">cran.r-project.org/web/packages/openintro</a>.<a href="multipleRegressionAndANOVA.html#fnref2">↩</a></p></li>
<li id="fn3"><p>Yes. Constant variability, nearly normal residuals, and linearity all appear reasonable.<a href="multipleRegressionAndANOVA.html#fnref3">↩</a></p></li>
<li id="fn4"><p><span class="math inline">\(\hat{y} = 36.21 + 5.13x_1 + 1.08x_2 - 0.03x_3 + 7.29x_4\)</span>, and there are <span class="math inline">\(k=4\)</span> predictor variables.<a href="multipleRegressionAndANOVA.html#fnref4">↩</a></p></li>
<li id="fn5"><p>It is the average difference in auction price for each additional Wii wheel included when holding the other variables constant. The point estimate is <span class="math inline">\(b_4 = 7.29\)</span>.<a href="multipleRegressionAndANOVA.html#fnref5">↩</a></p></li>
<li id="fn6"><p><span class="math inline">\(e_i = y_i - \hat{y_i} = 51.55 - 49.62 = 1.93\)</span>, where 49.62 was computed using the variables values from the observation and the equation identified in Guided Practice [eqForMultipleRegrOfTotalPrForAllPredictorsWithCoefficients].<a href="multipleRegressionAndANOVA.html#fnref6">↩</a></p></li>
<li id="fn7"><p>Three of the variables (, , and <strong>wheels</strong>) do take value 0, but the auction duration is always one or more days. If the auction is not up for any days, then no one can bid on it! That means the total auction price would always be zero for such an auction; the interpretation of the intercept in this setting is not insightful.<a href="multipleRegressionAndANOVA.html#fnref7">↩</a></p></li>
<li id="fn8"><p><span class="math inline">\(R^2 = 1 - \frac{23.34}{83.06} = 0.719\)</span>.<a href="multipleRegressionAndANOVA.html#fnref8">↩</a></p></li>
<li id="fn9"><p>In multiple regression, the degrees of freedom associated with the variance of the estimate of the residuals is <span class="math inline">\(n-k-1\)</span>, not <span class="math inline">\(n-1\)</span>. For instance, if we were to make predictions for new data using our current model, we would find that the unadjusted <span class="math inline">\(R^2\)</span> is an overly optimistic estimate of the reduction in variance in the response, and using the degrees of freedom in the adjusted <span class="math inline">\(R^2\)</span> formula helps correct this bias.<a href="multipleRegressionAndANOVA.html#fnref9">↩</a></p></li>
<li id="fn10"><p><span class="math inline">\(R_{adj}^2 = 1 - \frac{23.34}{83.06}\times \frac{141-1}{141-4-1} = 0.711\)</span>.<a href="multipleRegressionAndANOVA.html#fnref10">↩</a></p></li>
<li id="fn11"><p>The unadjusted <span class="math inline">\(R^2\)</span> would stay the same and the adjusted <span class="math inline">\(R^2\)</span> would go down.<a href="multipleRegressionAndANOVA.html#fnref11">↩</a></p></li>
<li id="fn12"><p>The p-value for the auction duration is 0.8882, which indicates that there is not statistically significant evidence that the duration is related to the total auction price when accounting for the other variables. The p-value for the Wii wheels variable is about zero, indicating that this variable is associated with the total auction price.<a href="multipleRegressionAndANOVA.html#fnref12">↩</a></p></li>
<li id="fn13"><p>An especially rigorous check would use <strong>time series</strong> methods. For instance, we could check whether consecutive residuals are correlated. Doing so with these residuals yields no statistically significant correlations.<a href="multipleRegressionAndANOVA.html#fnref13">↩</a></p></li>
<li id="fn14"><p>Recall from Chapter [linRegrForTwoVar] that if outliers are present in predictor variables, the corresponding observations may be especially influential on the resulting model. This is the motivation for omitting the numerical variables, such as the number of characters and line breaks in emails, that we saw in Chapter [introductionToData]. These variables exhibited extreme skew. We could resolve this issue by transforming these variables (e.g. using a log-transformation), but we will omit this further investigation for brevity.<a href="multipleRegressionAndANOVA.html#fnref14">↩</a></p></li>
<li id="fn15"><p>The new estimate is different: -2.87. This new value represents the estimated coefficient when we are also accounting for other variables in the logistic regression model.<a href="multipleRegressionAndANOVA.html#fnref15">↩</a></p></li>
<li id="fn16"><p>In this particular application, we should err on the side of sending more mail to the inbox rather than mistakenly putting good messages in the spambox. So, in summary: emails in the first and last categories go to the regular inbox, and those in the second scenario go to the spambox.<a href="multipleRegressionAndANOVA.html#fnref16">↩</a></p></li>
<li id="fn17"><p>First, note that we proposed a cutoff for the predicted probability of 0.95 for spam. In a worst case scenario, all the messages in the spambox had the minimum probability equal to about 0.95. Thus, we should expect to find about 5 or fewer legitimate messages among the 100 messages placed in the spambox.<a href="multipleRegressionAndANOVA.html#fnref17">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linRegrForTwoVar.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inferenceForCategoricalData.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/CrumpLab/statistics/blob/master/12-Multiple_Regression.Rmd",
"text": "Edit"
},
"download": ["statistics.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
